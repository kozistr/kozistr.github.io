{"componentChunkName":"component---src-templates-post-tsx","path":"/Cotatron/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>최근 mindslab 에서 VC (Voice Conversion)관련 논문이 나와서 오랜만에 요 쪽 domain 도 볼 겸 해서 논문을 읽게 됐습니다.</p>\n<p>간단하게 요약하면, 유명한 google 의 TTS model 인 <em>tacotron2</em> 기반으로 given transcription 와 mel alignment 를 활용해서 speaker-independent linguistic representation 을 뽑는 concept(?) 입니다.</p>\n<p>결론은 VCTK dataset 에서 최근 paper 인 <em>Blow</em> 보다 높은 MOS, DMOS 를 달성했습니다. 아래 링크에 들어가면 모델이 생성한 sample 들을 들어볼 수 있어요.</p>\n<p>paper : <a href=\"https://arxiv.org/pdf/2005.03295.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>demo : <a href=\"https://mindslab-ai.github.io/cotatron/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">link</a></p>\n<p>code : 아직 official code / pre-trained model은 없는데, 곧 나올 예정인 듯합니다</p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>이전 SOTA 였던 paper</p>\n<ul>\n<li>Blow : <a href=\"https://arxiv.org/pdf/1906.00794.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n</ul>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<p><em>Cotatron</em>의 전체적인 architecture 는 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/0ccd6e470b0bbb9a40438dd5fa01217b/e35ec/cotatron-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 88.51351351351352%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEaklEQVQ4y22Ta0ybVRjHj2ZTox/8YvxoYuJH5xITt7ksTFiBqVM3d3eaaKLLsjmzuOnuiybepk7cVaAIdDA2YBPYxcE6YGwMNii0XNpCKRRK7xdo+9KWXn+mLyMx0Sf55fmf58355Zy8OSIQjnKtz0KDdpTrg5PYfdOkU0niiRROhx6HrQ2zSc2YuZnxsVaZTLaO35Hnme5y3MftbMfvtyFcwTBnu6wUtOop1LnpHBhmyuvG6fIxab3HbLiRYUMFpqFKDIPnMRkr0faW4rLXY7f+hctRT2TmNolZNV7vCCKRTDIdlNANGPD6A/y7ouEQ0z4HEclHOOSVyeS+3k50PZ2YjDoMAxqGDTr8HhvxWBSR2WhyTPBtfTnaUSM+jweD0cikdQKjYwq1yc0tvQ31kEPm9pCTmo5Bqu/1caVTT0OPmQaNiZZRP65paU5Y39XGetVPlLfeYGLMwsPubixmEw09I5zptvNzo5Yfrnfx4w0Nx5v6ON1uRtnr5OQdI7+q+/mlSUfRgB+9zTMnjMVTaEYMROMxUok4sdmofOVYKsH0bBQpmSCUiBNKxJBSCboHBzBNWhmaGGcqNvd9OjhDIplGSCEfU34LpJOyxDBux2CxydlmcqD5W4v2Vh+9TX30Nurouamlqeo2ndcf0nyphc7aDnTNevqsI4QiXoTD1sWYuQJJchGJxrmo91Crd8vCiu8uc+ZIMXXnr1GrrONaVSNVpy7z9lMfkC3WkrtgM6te+JCVy7fz+ovbuFXTgnA7exgZriAa8ZJMg/K+kdK7elmo+qaGIxu/57c95yjYU8jpfcUc/6iA/Ge3kbVoOznZO1As/hjFM1vIEmtQV7UibNaHdD04RzBoJyhFULYbUbb2A2n+2FvOK+INlou3WCZyWSLeZOnzW8hZuYPslz5h1eMbUYj3yRUbyBLv0XypDeGwd6PVFOF0mhmzWCnpGKbs/hBTAQ9nvyxjhXiH1U9uJXvRTlYpdpO76FMUYgMKsQ6FWI/isQ3kPr6RLPEuLdV3EU6HBq2mmNmoT75mUdsAJXf65Vzy9QVWiDXkLd9Fzqs7yXt6K/liPXkLNpG3cDP5CzbJrF64mWyxbk7osD1gSK8kEnYTjsQo19mp6nfJwjNflfPyc2tY9sRalogcXhN5LBH5/2GpWM1ikc1NlRrh91twOh4QCQdIp+GeyUbHiF0W9g8ZKDpRTdn+KkqPXqL08EX+PFRF8f4KSg5eQHmgkpIMBy9QuFfF6MA4IiNJJJFlmUrMRvA47Xi8XqRgUH6fsVSUeGoW37QHq93C2IQZf8DL/5XI/M150o+syVSKeDxOSJLwBwJMBYMybq8Xi9WKRqvF7nIRkCSkcJjQzAyBkEQsHs+ccE40L8v01KNc293GZ6oT7FIV8HnlKb64eJZ9dSXszKwrTsqzXed/Z09NIbuvFNJlNsy95XnRvDSVSsm9oaedfdWFHLlcwtG6Mo7Wl3PsqopDtUoOVBfNzerKONag4vDVcrRjw/wDQjl69XH2WhIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/0ccd6e470b0bbb9a40438dd5fa01217b/fcda8/cotatron-architecture.png\"\n        srcset=\"/static/0ccd6e470b0bbb9a40438dd5fa01217b/12f09/cotatron-architecture.png 148w,\n/static/0ccd6e470b0bbb9a40438dd5fa01217b/e4a3f/cotatron-architecture.png 295w,\n/static/0ccd6e470b0bbb9a40438dd5fa01217b/fcda8/cotatron-architecture.png 590w,\n/static/0ccd6e470b0bbb9a40438dd5fa01217b/e35ec/cotatron-architecture.png 861w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"1-speaker-independent-linguistic-features-from-tts\" style=\"position:relative;\"><a href=\"#1-speaker-independent-linguistic-features-from-tts\" aria-label=\"1 speaker independent linguistic features from tts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. speaker-independent linguistic features from TTS</h3>\n<p>이번에 제안한 <em>cotatron</em> 은 google 의 <em>tacotron2</em> 를 기반으로 합니다.</p>\n<blockquote>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><msub><mi>M</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi></mrow></msub><mo>^</mo></mover><mo separator=\"true\">,</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>D</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>r</mi><mo stretchy=\"false\">(</mo><mi>E</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>r</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><msub><mi>M</mi><mrow><mn>0</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msup><mi>z</mi><mrow><mi>i</mi><mi>d</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{M_{1:i}} , A_i = Decoder (Encoder(T), M_{0:i-1}, z^{id})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1412em;vertical-align:-0.1944em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9468em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mrel mtight\">:</span><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.2523em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0991em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">Deco</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">er</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">co</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">er</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">0</span><span class=\"mrel mtight\">:</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\">d</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</blockquote>\n<p><em>T</em> 는 Transcription, <em>M</em> 은 log mel-spectogram, <em>z</em> 는 speaker representation.</p>\n<p>요거로부터 mel alignment + given transcription + speaker representation 으로 새로운 speech 를 생성합니다.</p>\n<p>이 이후가 중요(?)한데, TTS 훈련 후에,\nDecoder output 으로 transcription 과 mel-spectogram 사이의 <em>Alignment</em> 가 나오는데, 요 부분을 training 할 때 <em>teacher-forcing</em> 기술을 사용해서 훈련했다고 합니다.</p>\n<p>그래서 최종적으로 Speaker-Independent linguistic features 는 다음과 같습니다.</p>\n<blockquote>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi>m</mi><mi>u</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mo separator=\"true\">,</mo><mi>E</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mi>r</mi><mrow><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L = matmul(A, Encoder_{text}(T))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">ma</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">A</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">co</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">e</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">x</span><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mclose\">))</span></span></span></span></span></p>\n</blockquote>\n<p>그런데 한 가지 짚어야 할 점은,\n<em>T</em> 는 speaker 에 대한 정보가 없는 text 고,\n<em>A</em> 는 간단히 text 와 mel spectogram 과의 coef 라 할 수 있는데,\n즉, <em>L</em> 이 speaker 에 대한 정보를 담고 있지 않다는 점이다. 이 부분은 아래에</p>\n<p><em>Cotatron</em>은 이미 <em>Tacotron2</em> 기반의 모델이라 multi-speaker speech synthesis 에 well-optimized 됐을 거지만,\n조금 더 잘해 보려고(?) 기존의 embedding table 을 걷어내고, speaker representation encoder 를 하나 만들어 넣었다고 합니다.</p>\n<p>해당 encoder 구조는 2d cnn 6 layers + gru 구조로 구성.</p>\n<h4 id=\"speaker-disentanglement-issue-\" style=\"position:relative;\"><a href=\"#speaker-disentanglement-issue-\" aria-label=\"speaker disentanglement issue  permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>speaker disentanglement issue ?</h4>\n<p>그래서 이런 speaker disentanglement 에 대한 issue 를 해결하기 위해 speaker classifier 를 추가로 붙여 줬다고 캅니다.</p>\n<p>이 때 사용된 모델은 간단한 1d cnn 4 layers + temporal max-pooling + fc 로 구성.</p>\n<h3 id=\"2-voice-conversion\" style=\"position:relative;\"><a href=\"#2-voice-conversion\" aria-label=\"2 voice conversion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. voice conversion</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2e3ff1af06cf5a395aa66de760bdc7b0/6d2da/voice-conversion-system.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.972972972972975%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACBklEQVQoz4WSS2/TQBRG/YP5A2yR2AFrFogFFHVRKoSEkFoegkIp0IaGpokTu34mcWI7qZ+1PX7koLqopAKJka6uNHfuN9+dMxL/WbXIUUcj9JHC8KTHoC8ThGFbW61Wf52X4jgmikKiKCZJAnyvR5qEGPMlH2SbY8tDMU22vr5nY38XdWKtta/IspyLi4I0zRCiRFoul3iei+f5BIGLbb0hCX0+nig8+tTnRUfjWBlx/902d3Y26ds6cV7hJAVhJpjPbEzjFMuUSdMUad1uVZWUZUUcR4RBQFPX7ViXOQojppMJM2fK070ud19943XXYGodoY6eMR3v0DQ10mKxwJ3PcZw589kXbHOLqfWZvZ7Gg7c/2TgYcXDS5fbLx9x6/pCONkSICs2eUFQ1VVmQ5zF5nrSXS0IUCCFaZ9mFT5o61GVEVggM9xw3SIjTmFNbp2uqLOMrIEkS/xOipGsamqYx6A/w/QXBucMyiOgYMzqmy8HZFMsNODwbsNs7pMwLwijC8/1r0usheZ6HqijIAxlNU3AmR7iLBds/dDaPdJ58P+PYcNnY3+He7hYiLyiKgrIs/7Be+z5SnuctnSAIWod1DWUpKIuMqsipigyaBlEIxG+hy56rZyppmuaG6DVl13UxTbMdfzweo6gqqqqiaTq6YTAcDrEsC1mWsW27zZd7SZLcEPwFwbfqMC9D2CgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/2e3ff1af06cf5a395aa66de760bdc7b0/fcda8/voice-conversion-system.png\"\n        srcset=\"/static/2e3ff1af06cf5a395aa66de760bdc7b0/12f09/voice-conversion-system.png 148w,\n/static/2e3ff1af06cf5a395aa66de760bdc7b0/e4a3f/voice-conversion-system.png 295w,\n/static/2e3ff1af06cf5a395aa66de760bdc7b0/fcda8/voice-conversion-system.png 590w,\n/static/2e3ff1af06cf5a395aa66de760bdc7b0/efc66/voice-conversion-system.png 885w,\n/static/2e3ff1af06cf5a395aa66de760bdc7b0/6d2da/voice-conversion-system.png 936w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>위 이미지처럼 voice-conversion system 인데, 전반적인 pipeline 이 그려져 있습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d0d7975fc223ded182582768b92bd9fc/914c7/residual_encoder_vc_decoder.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.56756756756756%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADQUlEQVQ4y02P6U8iBxiH+XN305jN9st+6Pd+6JEe2rvi4lVAxbvr1qMaUA5lABkGRhgGZhCGGW4QGECfBmzi/pI375knv9fR75xSVLa4S7lJ366TTXsw9H16rX94aFxynIjw+cES7w5czK3PM7c2zxvPL8ytL/Du0MXbfSd7wiWLgQ/45QSOcj1LoZJGraRJymGUkkjRyKCbGYxaHt0yCKopwgWJcCHNuRTlIi0QVERCqkSoIJEpqXyx7eQwEcLxw4nIzxcyP51J/HgiMn8q8qv/joWLLB5Bo9PrU+uOMFsD6t0R+ZKFZjRntdUeYLaHjB+h02oxHo1w/B3PcnSr8FFU2Q6LHN3mZ/1hIk9EuSeuVVlKt/jjRuP7s1sWAncsBGTm/Rn+jOosinVipQbDhy6DoY3j/aWEWyjgTegsh7O4Y0X+iuZxhbIcSvf0BwMeBkP6Q5vheELVqlFrtrDHk9lsupuq2+1i2zaOg1SJjZCEN5hi3Z9gI5xmN55nTywRUAw0q8m5YnKczHOW1vgQz3GUUDgRVU5TBT6KBRSjTt0yse3py9kavpg6C89Nju1EkYOMwf5djUutiVQycSfLrIRkVsNZ1q4V1iI5loMZ1iJZXOEcglph2H/g8enpGbgZzeGLq6wH0/gSBbYEhX3Z4jxfY2IPYTSEyej5tWadQbc9q2ezsc2LnnAcSPd4gyKbYYmVC4GNSJrNSJrdpE4gV0EsmWzkOnhSVZxXMs5QlsWrDK5rBa9k4pYbJMuNGW7m0Bsv4b5RcUeLuGM6HqHIVtLAe1vhQutRqrUQjD7R+zYhxcQvlwnIZcJqjWipzU25R6Xdf/Y3Be7EkviuBXzXMdb8V7O8IyTYiYv8KysM+x0e7RqjvsXTuEXDVOk0dBg3ebTrTIYWk9HDC/Dtzu+8cn7Fa9c3vF7+lldLX/PZ6ne82f2NL4830LQYdXOPVHKFWNRJXFgiEXuPJK6iF30YZR9mNfk/8BFHrqyTr5ZnoVbL6HWTgmVQMCuY3Ta23Wc06mDbbbpdk1arQq9n0e1a9Ho1Rnab8Wjw4rBU1NCLGllZRs0pGPflWa5VTT7VYGBTLOpUqxaFgoYsZzE+uZnCpvoP0sLZoRm7vTcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d0d7975fc223ded182582768b92bd9fc/fcda8/residual_encoder_vc_decoder.png\"\n        srcset=\"/static/d0d7975fc223ded182582768b92bd9fc/12f09/residual_encoder_vc_decoder.png 148w,\n/static/d0d7975fc223ded182582768b92bd9fc/e4a3f/residual_encoder_vc_decoder.png 295w,\n/static/d0d7975fc223ded182582768b92bd9fc/fcda8/residual_encoder_vc_decoder.png 590w,\n/static/d0d7975fc223ded182582768b92bd9fc/efc66/residual_encoder_vc_decoder.png 885w,\n/static/d0d7975fc223ded182582768b92bd9fc/914c7/residual_encoder_vc_decoder.png 978w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"21-residual-encoder\" style=\"position:relative;\"><a href=\"#21-residual-encoder\" aria-label=\"21 residual encoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1 residual encoder</h4>\n<p>speech 를 decoding 하는 과정에서 아무리 transcription + speech 에 정보가 잘 있어도 speech 자체 만에 대한 정보도 다양하고 중요하여서,\n해당 정보를 따로 encoding 해서 decoder 에서 사용한다고 합니다.</p>\n<p>residual encoder 의 특징은</p>\n<ul>\n<li>위에 한 번 언급된 speaker encoder 와 비슷한 구조</li>\n<li>temporal information 보존을 위해 time-wise 하게는 stride 적용 x</li>\n<li>특정 speaker feature 에 overfit 을 막기 위해 작은 channel size 를 사용.</li>\n<li>결론적으로 single channel output 이 위 문제를 막으면서 잘 동작했다고 캅니다.</li>\n<li>plus) Hann 으로 smoothing 함 (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>21</mn></mrow><annotation encoding=\"application/x-tex\">k = 21</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">21</span></span></span></span></span>)</li>\n</ul>\n<h4 id=\"22-vc-decoder\" style=\"position:relative;\"><a href=\"#22-vc-decoder\" aria-label=\"22 vc decoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.2 VC decoder</h4>\n<p>위에 image 처럼, Cotatron feature 와 mel encoded feature 가 concat 돼서 들어가고 target speaker id 도 같이 들어갑니다.</p>\n<blockquote>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>M</mi><mrow><mi>s</mi><mo>→</mo><mo>∗</mo></mrow></msub><mo>=</mo><mi>D</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><msub><mi>r</mi><mrow><mi>v</mi><mi>c</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mi>s</mi></msub><mo separator=\"true\">,</mo><msub><mi>R</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><msup><mi>y</mi><mrow><mi>i</mi><mi>d</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">M_{s \\to *} = Decoder_{vc} (concat(L_s, R_s), y^{id})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1757em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">s</span><span class=\"mrel mtight\">→</span><span class=\"mord mtight\">∗</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0991em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">Deco</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">e</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathnormal mtight\">c</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">co</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">t</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\">d</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</blockquote>\n<p>VC decoder 구조는 <em>GAN-TTS</em> 란 paper 와 유사합니다. head, tail 에 1d conv 가 1 layer 씩 있고, 중간에 GBlock w/ CondBN 4 blocks 있는 형태 입니다.\n물론 CondBN 에 Condition 으로 target speaker feature 가 들어갑니다.</p>\n<p>요 decoder 에 대한 모델적인 여러 시도를 했는데 결론적으로 성능 향상은 없었다고 하면서 future works 로 남기며 턴을 종료했습니다.</p>\n<h3 id=\"3-training-recipe\" style=\"position:relative;\"><a href=\"#3-training-recipe\" aria-label=\"3 training recipe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. training recipe</h3>\n<p>은 논문 참고해 주세요 (<del>귀찮</del>)</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"vctk-benchmark-many-to-many\" style=\"position:relative;\"><a href=\"#vctk-benchmark-many-to-many\" aria-label=\"vctk benchmark many to many permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VCTK Benchmark (many-to-many)</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1ff9906a2a1bfb44626eff66a7423181/02cd5/vctk_benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABk0lEQVQoz3WTx67CMBBF/f+/AysQG5qEgNBE7733Xi868xR2L5LlxDO+zY6TpPP5rGKxqFarpdFopEqlona7rX6/r1qtpl6vp/F4bO/Mg8FAk8lEnU5H3W7Xeh+PB1Byp9NJl8tFzIfDQbvdTtvt9jfW67U2m429/zf743g8ysEWDAYVjUaVTCYN+Hq9mmoIwuGw4vG47ve71dgUCoUUCATM0ev1MpWxWEyz2UxuPp+bxeFwaHYYq9XKVLNGBMyLxUK3203L5dLWptOpAe33eyMnDmoGiLJyuax6va5IJGIAPKyREwMQHlSQKT2lUsnAsO55npEaYD6f/7ECSgPMzWbTNvoARMDMITCjil6+M5mMuXM0p1Ipk0tOZIK15/MpyAibGofzfr8tQ5QAhFr6uQ3ZbNbWHKcKYCKRUC6XM+mo/Xw+8m8AtiBhjX7AqUFGDXKI6HHkAnqhULDTxD4PRQiwgfVGo2Ebq9Wq2cMZd5cYIEin03+njEwU0oRKMiQ/gCBhI9eDm0DG2OPbXycW/2cgwy8yY97yfUd+1wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/1ff9906a2a1bfb44626eff66a7423181/fcda8/vctk_benchmark.png\"\n        srcset=\"/static/1ff9906a2a1bfb44626eff66a7423181/12f09/vctk_benchmark.png 148w,\n/static/1ff9906a2a1bfb44626eff66a7423181/e4a3f/vctk_benchmark.png 295w,\n/static/1ff9906a2a1bfb44626eff66a7423181/fcda8/vctk_benchmark.png 590w,\n/static/1ff9906a2a1bfb44626eff66a7423181/02cd5/vctk_benchmark.png 821w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>기존 SOTA 인 Blow 보다 훨 높은 MOS, DMOS 를 보여줍니다. SCA 는 Blow 를 넘진 못헀네요.</p>\n<h3 id=\"speaker-disentanglement\" style=\"position:relative;\"><a href=\"#speaker-disentanglement\" aria-label=\"speaker disentanglement permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Speaker Disentanglement</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/81ba0c3552364e82dc7d254d975d1804/715a3/degree_of_speaker_disentanglement.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25.675675675675674%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA10lEQVQY001QWQqFMBDr/W8jiBcQf/1z31BRwX0HRcgjA8orhJZkmsyMwt8JwxBBEGAYBriuizzPUZYlPM9DHMcoikJ03/cFrO/7/t8C6jxP7PuObduwrqu8j+PAsiwC8q9G8P1q8zx/f4n7vqGYqmkaTNOEYRjQdR1N0yBJEuEsy0JVVdJJlmViws6oO46DaZoQRRHSNMV1XVBt28K2bUl8nkdIgmkcj4HkedOIndCcK2EQJ6Q5VyKGFEnQmGnjOH6gCT+Rr+ta9sn6rutkv5yEdayhxvF/l+1y5B6RtxYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/81ba0c3552364e82dc7d254d975d1804/fcda8/degree_of_speaker_disentanglement.png\"\n        srcset=\"/static/81ba0c3552364e82dc7d254d975d1804/12f09/degree_of_speaker_disentanglement.png 148w,\n/static/81ba0c3552364e82dc7d254d975d1804/e4a3f/degree_of_speaker_disentanglement.png 295w,\n/static/81ba0c3552364e82dc7d254d975d1804/fcda8/degree_of_speaker_disentanglement.png 590w,\n/static/81ba0c3552364e82dc7d254d975d1804/715a3/degree_of_speaker_disentanglement.png 830w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>그냥 Cotatron feature 만 쓸 때와 mel spectogram 만 따로 encoding 해서 쓴 경우와 비교했을 때,\nSCA 가 훨씬 높은 걸 보여주네요.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>간단한 concept 으로 꽤괜 성능이 나오고,\ntranscript 를 주지 않아도 성능이 준 것과 comparable 하다는 점도 재밌고,\nCortatron encoder 를 다른 task 에 적용해 봐도 재밌는 결과 볼 수 있을 것 같네용</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 최근 mindslab 에서 VC (Voice Conversion)관련 논문이 나와서 오랜만에 요 쪽 domain 도 볼 겸 해서 논문을 읽게 됐습니다. 간단하게 요약하면, 유명한 google 의 TTS model 인 tacotron2 기반…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#architecture\">Architecture</a></p>\n<ul>\n<li>\n<p><a href=\"#1-speaker-independent-linguistic-features-from-tts\">1. speaker-independent linguistic features from TTS</a></p>\n<ul>\n<li><a href=\"#speaker-disentanglement-issue-\">speaker disentanglement issue ?</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-voice-conversion\">2. voice conversion</a></p>\n<ul>\n<li><a href=\"#21-residual-encoder\">2.1 residual encoder</a></li>\n<li><a href=\"#22-vc-decoder\">2.2 VC decoder</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#3-training-recipe\">3. training recipe</a></p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"#vctk-benchmark-many-to-many\">VCTK Benchmark (many-to-many)</a></li>\n<li><a href=\"#speaker-disentanglement\">Speaker Disentanglement</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/Cotatron/"},"frontmatter":{"title":"Cotatron Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data","date":"May 10, 2020","tags":["Deep-Learning"],"keywords":["NLP","ChatBot","Blender"],"update":"May 10, 2020"},"timeToRead":3}},"pageContext":{"slug":"/Cotatron/","series":[],"lastmod":"2020-05-10"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}