{"componentChunkName":"component---src-pages-search-tsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"rawMarkdownBody":"\nposted by [kozistr](http://kozistr.tech)\n\n## tl;dr\n\n최근에 NVLabs 에서 VAE 관련 논문이 하나 나왔는데, 매주 월요일이 회사 짬데이라고 개인 or 팀 끼리 공부하고 싶은 주제 공부하고 공유하는 문화가 있어서, 마침 잘 돼서 논문 리뷰를 해 봅니다.\n\npaper : [arXiv](https://arxiv.org/pdf/2007.03898.pdf)\n\ncode : [github](https://github.com/NVlabs/NVAE)\n\n## Related Work\n\nVAE 관련 연구들이 엄청 많아서, 요 논문과 직접 연관이 있는 것들만 적어보면\n\n1. IAF-VAEs (VAE w/ Invertible Autoregressive Flows) : [paper](https://arxiv.org/pdf/1606.04934.pdf)\n2. VQ-VAE-2 (Vector Quantized Variational AutoEncoder v2) : [paper](https://arxiv.org/pdf/1906.00446.pdf)\n3. BIVA (Bidirectional-Inference Variational AutoEncoder) : [paper](https://arxiv.org/pdf/1902.02102.pdf)\n\n## Difference\n\n논문에서 previous works 와 this work 의 차이를 *related work* 에 적힌 3 개의 연구와 비교를 합니다.\n\n요약하면 아래와 같습니다.\n\n### VQ-VAE-2 vs NVAE\n\n비슷한 점은 둘 다 high quality image 생성이 가능하다는 점\n\n| diff \\ work | VQ-VAE-2 | NVAE |\n| :---: | :---: | :---: |\n| objective | ~~VAE objective~~ | VAE objective |\n| latent variable | up to 128x128 (big) | small |\n\n### IAF-VAEs vs NVAE\n\nstatistical models (hierarchical prior, approximate posterior) 컨셉을 IAF-VAEs 에서 가져온 것은 맞는데,\n\n| diff \\ work | IAF-VAEs | NVAE |\n| :---: | :---: | :---: |\n| statistical models | ~~neural network~~ | neural network |\n| posterior | x | parameterized |\n| large-scale | x | o |\n\n### BN (Batch Normalization) in VAE?\n\n이전 연구들에선 BN 이 instability 를 cause 해서 사용을 지양했는데,\n이번 엔구에선 BN parameter 를 적절하게 사용하면 오히려 좋은 성능이 나온다 라고 하더군요.\n\n## Novelty\n\n위와 같은 차이점들에 대해서 이번 연구는 요약해서 크게 3 가지 novelties 를 가집니다.\n\n### Network Design\n\n이전 Hierarchical VAE 연구들은 hierarchical 한 요소들을 objective term 이나 level 에서 고려헀지만,\n\n이번 연구에서는 statistical models 을 **network design 자체에 녹여냈다**.\n\n### Stability\n\nhierarchical groups 수가 증가, input image size (high resolution) 가 커 지면서 stabilization 이 issue 가 되는데,\n이를 직접 디자인한 **(1) network modules**, **(2) approximate posterior 를 parameterize** 함으로 문제 해결.\n\n### Efficiency\n\n효율적은 operation 사용 (e.g. depth-wise separable convolution at decoder) 으로 memory 를 아끼고 성능도 잡아냈다.\n\n% 저자 왈 depth-wise separable conv 를 decoder 에 사용했을 땐 성능이 좋았는데, encoder 에 사용했을 땐 성능이 오히려 안좋았다고 캅니다.\n\n## Introduce\n\n간단하게 deep hierarchical VAE 를 review 하고 넘어가면,\n\n*approximate posterior* 와 *prior* 를 증가시키기 위해, *latent variables* 를 총 *$L$* 개의 disjoint groups 으로 나눕니다.\n\n> $z = {z_1, z_2, ..., z_L}$\n\n그리고 *prior* 와 *approximate posterior* 는 이렇게 써 볼 수 있을텐데, (물론 두 distributions 은 Normal 을 따른다 가정한다)\n\n> $p(z) = \\prod_{l} p(z_l\\|z_{<l})$\n\n> $q(z\\|x) = \\prod_{l} q(z_l\\|z_{<l}, x)$\n\n그럼 $log p(x)$ 에 대한 lower bound $L_{VAE} (x)$ 는 아래와 같이 쓸 수 있겠죠...?\n\n> $L_{VAE} (x) = \\mathop{\\mathbb{E}}_{q(z\\|x)} - KL(q(z_{1}\\|x) \\|\\| p (z_{1})) - \\sum_{l=2}^{L} \\mathbb{E}_{q (z_{l}\\|x)} KL (q (z_{l}\\|x, z_{<l}) \\|\\| p(z_{l}\\|z_{<l}))$\n\n한번 더 적어보면, 아래가 이제 $(l - 1)^{th}$ group 까지의 *approximate posterior* 가 되는 겁니다.\n\n> $q(z_{<l}\\|x) = \\prod_{i=1}^{l-1} q(z_{i}\\|x, z_{<i})$\n\n그럼 여기서 $p(x, z)$, $p(z\\|x)$ 를 어떻게 neural network 로 구현할 지가 이번 연구에서 포인트 입니다.\n\n## Architecture\n\n아래 이미지와 같이 Bidirectional Encoder ($p(z\\|x)$) 와 Generative Model ($p(x, z)$)이 있는데,\n\nlevel 에 맞게 각 group 에서 sample 해 와서 add 하는 연산을 합니다.\n\n또한 computation cost 를 줄이기 위해 Bidirectional Encoder 에 있는 top-down model 부분은 Generative Model 하고 weight-share 하네요.\n\n![img](architecture.png)\n\n### Residual Cells for VAE\n\n일반적으로 long-range correlations 를 잘 잡아내기 위한 방법으론 *hierarchical multi-scale model* 을 사용하는 건데,\n그냥 이걸 썼다는 정도 입니다.\n\n### Residual Cells for Generative Model\n\nnetwork 의 receptive field 크기를 늘릴 수록 long-range correlations 을 잡는데 유리하다고 설명하는데,\n일반적으로는 encoder / decoder 에 사용된 residual network 안에 convolution kernel size 를 늘리면 되겠지만,\n그냥 늘리면 computation 이 엄청 늘어나니까, 이걸 depth-wise separable convolution 을 사용해 해결합니다.\n\n![img](residual_blocks.png)\n\nMobileNet-V2 에서 언급되었던 것 처럼, depth-wise convolution 하나만 으로 대채만 하는 건,\n각 channel 따로따로 동작하는 연산특 때문에 표현성(?)에서 제한이 있어서, 위 그림처럼 conv 1x1 으로 채널을 뻥튀기 해 준 후에 depthwise conv 5x5 를 적용합니다.\n\n#### BatchNormalization\n\n위에서 언급했듯, BN 은 training instability 때문에 BN 대신 WN (Weight Normalization) 을 사용했는데,\n\n이 논문에서, 실제 BN 의 문제는 evaluation 시, slow-moving running statistics 값 때문에 shifted 돼서, output 이 dramatic 하게 바뀔수 있다는 점이라 말하면서,\n이 문제를 해결하기 위해 BN momentum 값을 batch statistics 을 빠르게 잘 잡도록 변경을 해 줬다고 합니다.\n\n또한, scaling parameter 에 norm regularization 도 해 줬다고 합니다.\n\n### Residual Cells for Bidirectional Encoder\n\nencoder 에서는 depth-wise convolution 이 효과가 없어서 그냥 regular convolution 사용했다고 합니다.\n\n### Taming the Unbounded KL Term\n\ndeep hierarchical VAE 를 훈련하는데 있어, unbounded KL 를 optimize 하기엔 어렵다는 말을 쭉 합니다.\n\n그래서 KL 를 잘 optimize 하고 stable 하게 훈련할 수있는 방법들을 제안합니다.\n\n#### Residual Normal Distributions\n\n> $p(z_{l}^{i}\\|z_{<l}) = \\mathcal{N} (\\mu_{i} (z_{<l}), \\sigma_{i} (z_{<l}))$ 가 $i^{th}$ variable in $z_{l}$ prior 가 normal  이라 하면,\n\n아래와 같이 정의해 볼 수 있습니다.\n\n> $q(z_{l}^{i}\\|z_{<l}, x) = \\mathcal{N} (\\mu_{i} (z_{<l}) + \\Delta \\mu_{i} (z_{<l}, x), \\sigma_{i} (z_{<l}) + \\Delta \\sigma_{i} (z_{<l}, x))$\n\n여기서 *delta* 들은 *prior* 와*approximate posterior* 의 relative location & scale 입니다.\n\n> $KL (q(z^{i}\\|x)\\|\\|p(z^{i})) = (\\frac{\\Delta \\mu_{I}^{2}}{\\sigma_{i}^{2}} + \\Delta \\sigma_{i}^{2} - log \\sigma_{i}^{2} - 1) / 2$\n\n만약 decoder output 인 $\\sigma_{i}$ 가 bounded below 면, 위 KL term 이 공식에 나온 것 처럼, encoder output 인 relative parameter 에 영향을 많이 받게됩니다.\n\n즉, $q(z_{l}^{i}\\|z_{<l}, x)$ 가 absolute location & scale 일 때, 요 KL term 으로 minimization 하기 쉬워지는걸 의도했네요.\n\n#### Spectral Regularization\n\n위에서 제안한 *Residual Normal Distributions* 만으로 stablize 하기 어렵다고 생각해 (아직 unbounded 기 때문),\nbound KL 을 만들기 위해, input changes 에 output 이 dramatic 하게 변하면 안된다는게 보장돼야 합니다 ~> **smoothness**.\n그래서 이 연구에선 Lipschitz constant 를 regularizing 함으로 bounded 를 ensure 함을 가정합니다.\n\n이어서, Lipschitz constant 를 측정하기는 힘드니, *Spectral Regularization* 을 각 layer 에 적용을 합니다. (lipschitz constant 를 minimize 해 주는 scheme 에서), loss term 에도 해당 regularization term 을 추가해서 minimize 합니다.\n\n> $L_{SR} = \\lambda \\sum_{i} s^{(i)}$, $s(i)$ 는 $i^{th}$ convolution 의 largest singular value\n\n#### More Expressive Approximate Posteriors with Normalizing Flow\n\n지금 구조는 *approximate posterior* 를 각 group 에서 병렬로 샘플하기 좋은 구조로 돼있는데 (상대적으로 작은 parameter 수, 등등), 조금 다르게 말하면, less expressive 하다고 말할 수 있다.\n\n더 expressive 하게 만들기 위해 normalizing flow 몇 개를 추가해서 더 expressive 하게 만들자가 목적이다.\n\nencoder 에만 해당 normalization flow 가 추가되면,\n\n1. IAF (Inverse Autoregressive Flows) 가능 (따른 명시적으로 inverse 해 주는 flow 필요 x)\n2. sampling time 도 flow 덕문에 증가하지 않을 거다.\n\n라는 장점을 듭니다.\n\n## Experiment Result\n\n###  Quantitative Performance Benchmark\n\nbits/dimension (bpd) metric 에서 SOTA 에 해당하는 성능을 보이네요.\n\n![img](performance_benchmark.png)\n\n### Generations\n\n![img](generated_images.png)\n\n## Conclusion\n\n개인적으로 NVIDIA 연구들은 보면 StyleGANv2 도 그렇고 network design 으로 문제를 해결해 나가는 모습을 많이 보이는데, 이번에도 어썸했다.\n\n또, 연구를 진행하고 결과를 내는 것들이 대단하다 생각하지만,\n최근 들어 VAE 쪽 논문들이 큰 novelty 없이 생산되는(?) 경향이 있었는데, 오랜만에 개인적으로 괜찮다 생각되는 AE 논문이 나온거 같아서 기분이가 좋았다.\n\n결론 : 굳굳굳\n","excerpt":"posted by kozistr tl;dr 최근에 NVLabs 에서 VAE 관련 논문이 하나 나왔는데, 매주 월요일이 회사 짬데이라고 개인 or 팀 끼리 공부하고 싶은 주제 공부하고 공유하는 문화가 있어서, 마침 잘 돼서 논문 리뷰를 해 봅니다. p…","fields":{"slug":"/NVAE/"},"frontmatter":{"date":"Sep 07, 2020","title":"NVAE A Deep Hierarchical Variational Autoencoder","tags":["Deep-Learning"],"update":"Sep 07, 2020"}}},{"node":{"rawMarkdownBody":"\r\n## TL;DR\r\n\r\n최근에 Clova AI 에서 unsupervised image 2 image translation 관련 논문이 나와서 한번 빠르게 봤습니다.\r\n\r\n일단 제목부터가 재밌는데 TUNIT, **Truly** Unsupervised Image to Image Translation 의 약자입니다. \r\n요즘 unsup, semi-sup 이라 하면서, 사실은 supervised 인 approach 들이 있어서 그런지, 이거는 **찐**이다 라는 걸 제목부터 보여주고 싶었나 보네요.\r\n\r\n~~헛소리였고.~~,  reproducible 가능한 코드도 논문 공개와 같이 돼서 좋네요. \r\n\r\npaper : [arXiv](https://arxiv.org/pdf/2006.06500.pdf)\r\n\r\ncode : [github](https://github.com/clovaai/tunit)\r\n\r\n## Related Work\r\n\r\nImage-to-Image Translation 쪽 papers 이 정도만\r\n\r\n* CyCleGAN : [paper](https://arxiv.org/pdf/1703.10593.pdf)\r\n* UNIT : [paper](https://arxiv.org/pdf/1703.00848.pdf)\r\n* MUNIT : [paper](https://arxiv.org/pdf/1804.04732.pdf)\r\n* FUNIT : [paper](https://arxiv.org/pdf/1905.01723.pdf)\r\n* StyleGAN-v2 : [post](http://kozistr.tech/deep-learning/2020/02/29/StyleGANv2.html)\r\n* StarGAN-v2 : [post](http://kozistr.tech/deep-learning/2020/02/10/StarGANv2.html)\r\n\r\n## Introduce\r\n\r\n이전에도 여러 i2i translation approaches 가 존재했지만, \r\n주로 연구들이 set-level 에서 multimodal translate 할 때, domain label 이 필요하다는 점이 있었고,\r\n이런 문제를 해결하기 위해 pre-trained classifier 를 adopt 해서 domain info 를 extract 하는 방식들의 접근이 있었어요.\r\n또 self-supervised 방식으로 mutual information maximization 를 통해 각 이미지들을 잘 cluster 하려는 시도도 있었습니다.\r\n\r\n하지만 real-world data 들은 label 들을 주로 구하기 힘든 문제들이 존재하니, \r\n그럼 어떻게 unlabelled data 로 (unsupervised) i2i translate 를 잘 할 수 있을지를 해결한 논문입니다.\r\n\r\n## Method\r\n\r\n![img](/assets/TUNIT/tunit-architecture.png)\r\n\r\n위 이미지가 전반적인 TUNIT architecture 인데요, 크게 3 종류의 network 로 구성돼있습니다.\r\n\r\n* Network E : Guiding Network\r\n* Network D : Discriminator\r\n* Network G : Generator\r\n\r\n### Guiding Network\r\n\r\n이 논문의 핵심(?)인 network 인 guiding network 인데요, \r\n해당 network 에서는 input image 를 입력받으면, *pseudo label* 과 *style code (embedding)* 을 줍니다.\r\n\r\n*pseudo label* 은 network 가 예측한 해당 domain 의 class 가 되겠고, *style code* 는 해당 image 의 style 을 담고 있는 embedding 일 겁니다.\r\n\r\n#### Unsupervised Domain Classification\r\n\r\n하지만, 여기선 image domain 에 대한 label 을 구할 수가 없는데요, 대략 임의의 class 수 ($K$, e.g) 5, 10, 20)를 잡습니다. (K 잡는 게 NetVLAD 비슷한 느낌 하네요)\r\n\r\n그리고 다른 unsupervised method 에서 주로 사용하는 방식인 augmentation (e.g) random cropping, horizontal flip, ....)을 통해서 각 domain 의 cluster 를 학습하는데, \r\n여기서 mutual information (MI) 를 maximize 하는 방식으로 진행하게 됩니다.\r\n\r\n> $I(p,p^+) = H(p) - H(p\\|p^+)$, $p = E_{class}(x)$\r\n\r\n여기서 image $x$ 라 하면, $x^+$ 는 augmented image $x$ 이고, $p$ 는 K domains 에 대한 확률값이라 볼 수 있겠습니다. (softmax 가 정확히는 확률값은 아니지만)\r\n\r\nentropy $H(p)$ 가 maximize, cond entropy $H(p\\|p^+)$ 가 minimize 되면서, \r\n결과적으로 해당 MI 가 maximize 되면, 모든 samples 들이 K domains 에 골고루 분포되면서,\r\naugmented 된 domain 들에 대해선 같은 domain 으로 묶겠죠?\r\n\r\n위 공식을 entropy loss scheme 으로 다시 써 보면 아래처럼 됩니다.\r\n\r\n> $L_{MI}$ = $I(p,p^+)$ = $I(P)$ = $\\sum_{i=1}^{K} \\sum_{j=1}^{K} P_{ij} \\log \\frac{P_{ij}}{P_{i}P_{j}}$, $P_{ij} = P(p = i, p^+ = j)$\r\n\r\n#### Improving Domain Classification\r\n\r\nimage 가 higher-resolution & complex 하고 diverse 하는 문제를 극복하기 위해 \r\ndomain classification 이외에 auxiliary branch 로 style code 를 뽑아 여기에 contrastive loss 를 붙였습니다.\r\n\r\n> $L_{style}^E = - log \\frac{exp(s \\cdot s^+ / \\Gamma)}{\\sum_{i=0}^N exp(s \\cdot s_{i}^- / \\Gamma)}$\r\n\r\n이런 식인데, contrastive loss 목적처럼 positive pair 는 가깝게, negative pair 는 멀리 보내는 역할을 합니다.\r\n\r\n이 method 적용으로 위 method 하나만 사용했을 때 보다 약 35% 정도 AnimalFaces 에서 classification accuracy 가 증가했다고 하네요.\r\n\r\n### Discriminator & Generator\r\n\r\n두 개 다 특별한 거 없이, discriminator 는 multi-task discriminator (K classes) 고, \r\ngenerator 도 guiding network E 에서 오는 style code 기반으로 이미지를 생성하는 network 입니다. \r\n\r\nadv loss 도 simple 한 vanilla gan loss 를 사용하네요.\r\n\r\n#### Style Contrastive Loss\r\n\r\n> $L_{style}^G = - log \\frac{exp(s^{'} \\cdot s^~ / \\Gamma)}{\\sum_{i=0}^N exp(s^{'} \\cdot s_{i}^- / \\Gamma)}$\r\n\r\n$s^{'} = E_{style}(G(x, s^~))$, $s^{'}$ 는 생성된 이미지에 대한 style 이고 (positive), - 는 negative style.\r\n\r\n위 loss function 을 사용하면 ref image x 에 대해 생성된 이미지가 유사해지니 (positive 는 유사, negative 는 안 유사),\r\nguiding network E 가 모든 이미지를 하나의 style code 로 뽑는 것도 막을 수 있겠죠. (여기서 only recon loss 만 사용할 때 문제)\r\n\r\n#### Reconstruction Loss\r\n\r\n여기도 특별한 건 없고, image x 와 해당 style s 에 대해 생성된 이미지와의 L1 loss 를 minimize 합니다.\r\n\r\n### Total Loss\r\n\r\n> $L_{D} = - L_{adv}$\r\n> $L_{G} = L_{adv} + \\lambda_{style}^G L_{style}^G + \\lambda_{rec} L_{rec}$\r\n> $L_{E} = L_{G} - \\lambda_{MI} L_{MI} + \\lambda_{style}^E L_{style}^E$\r\n\r\n## Experiment Result\r\n\r\n### Translation Loss on AnimalFaces-10\r\n\r\n$L_{style}$, $L_{rec}$ 없을 때 하고 풀버전(?) 하고 거의 유사하긴 하지만, 모두 적용했을 때가 제일 FID 가 좋네요.\r\n\r\n![img](translation-loss.png)\r\n\r\n### t-SNE visualization of the style space\r\n\r\n$K = 10$ 설정일 때, style code 들이 얼마나 cluster 별 domain style 을 뽑고 있나 봤을 때, \r\n잘 분리하고 있는 걸 보여줍니다.\r\n\r\n![img](t-SNE-vis.png)\r\n\r\n## Conclusion\r\n\r\n재밌는 approach 들을 사용했고 (arbitrary K classes, MI maximization, style contrastive learning), \r\n결과도 이전 연구 성능보다 outperform 해서 좋았습니다.\r\n\r\n결론 : 굳\r\n","excerpt":"TL;DR 최근에 Clova AI 에서 unsupervised image 2 image translation 관련 논문이 나와서 한번 빠르게 봤습니다. 일단 제목부터가 재밌는데 TUNIT, Truly Unsupervised Image to Image…","fields":{"slug":"/TUNIT/"},"frontmatter":{"date":"Jun 16, 2020","title":"TUNIT Rethinking the Truly Unsupervised Image-to-Image Translation","tags":["Deep-Learning"],"update":"Jun 16, 2020"}}}]}},"pageContext":{}},"staticQueryHashes":["2027115977","694178885"]}