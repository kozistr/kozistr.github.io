{"componentChunkName":"component---src-templates-post-tsx","path":"/SinGAN/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 ICCV 2019 에서 <a href=\"https://syncedreview.com/2019/10/29/iccv-2019-best-papers-announced/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Best Paper Awards</a> 에서 선정된 papers 중에 하나인 <strong>SinGAN</strong> 을 리뷰해 보겠습니다.</p>\n<p>개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서 저도 간략한 overview 와 technical review 를 해 보려고 합니다.</p>\n<p>소개 전에 간단하게 SinGAN 으로 뭘 할 수 있는지 보면, <strong>단 한 장의 이미지로 realistic 한 image manipulation 들을 생성</strong>할 수 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVR42gExAc7+AMfR2b/M177HzdPY2dDd58TW48rX3svV2uTt9szU2bO8wMfS2sjV4LfBxcHO1cra5tPh69Tc3sXR2MvV2ABddHRVcXdjbGiKiYJPb3FAYmhgamNKVlJohpN1gHxNVExdeHtden1dZVxHXl1IZGVad35laWE5TEdbX1YAn4tklHxQnoNXzLykm4tkkn1Qp4tin4dfkntQn4Nap45nmHtQnnxPqIhfm4ZelntQoIZaqYhgmoBUsJp3APazY/i0WP/HX/jYovKvYf6/XPvCZPu4YPmzVvi3Wfu/ZveoT/qsUvmyWfu6ZfOoTvm7XvazWf20U/7OfgCphWCtgFC8hk3Xu52ohF6jeEmqf1SrgVibdEuxgVHCkF6sfEywf0y1glHAkGGfdk2tf0+1hlS3hU7NoHKb5LA6lW1x6AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png\"\n        srcset=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/12f09/teaser.png 148w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/e4a3f/teaser.png 295w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png 590w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/efc66/teaser.png 885w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/c83ae/teaser.png 1180w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png 1493w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACbUlEQVR42gFiAp39APDz89Xf5LvN1Njg4ufo67bM/5au4Y+v47nU/eHh2tHOws/MwuHj4rK/y626xeXn6P///7exvqmdwNPK6ACnra+bqKawxMmaraV4hXh4jrR1mNtGjPFRi9aOm4u/uIvTyqLGxq93foJyg4/y9vr///98cJdYRIuBcqQAfYRwU2Ixcn5JXGsvbHlNxrmIpZB3mpyle2tpc4libH1MpKNytbqSjGlgg11b9/r5////Z1x5V0SFZliHAKe1pX2QcICXdHGMcXyQiIiMsHl5lH56l4yIoqaskZ2if5mad6erk1ppbmZxd8DN2MXS22ZchHBapFJGcwDEzM+psbWKmqWHnqmYrb14jsp0jtxJfv1Ng/mgppXCtY/d067Jyrg1ZIpYZG1XaXg7bJFIPmFuWJpeUYEAkp2EWWxEZXVRZXNNgI1turKAoIRpf4ecaG2Ab3pXYlk0srB/sLubUWZ4d2hcjn9zZYOaTElzXkqLRDpiAKm3n3mQZ3iOam6KZIeafZaVo3VteXhrc4t9hKOkh4uKZ42NZKKmjG13gXh2dIyGgoWXpEdBZmVNlG1dkADP09W6wsWptbyrvMKyvsh3itFwitpMffpVifqjq5zJwJzXza7HyLk7ZotTZHFZb4BHdJZkXYJhT49OQ2wAk5qHYnJFa3tRb31UiJR3sKyHl4B1dYWuXW2UbHxZZWk9vLWFtL6eR2N3amFafnNuV3qSPjhgWEGKYlKKAKu0ln2MUYmXX4CRWI+adK2nmZeFdZmCc5yHepaefoWLZ5iYbaisj3+EiZOIgaKTi5WirH18nW1hi3Rtif4hUnmd3PJgAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png\"\n        srcset=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/12f09/manipulation.png 148w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/e4a3f/manipulation.png 295w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png 590w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/efc66/manipulation.png 885w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png 964w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1905.01164.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>official implementation : <a href=\"https://github.com/tamarott/SinGAN\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>기존 GAN 들을 대부분의 연구들을 보면 얼굴, 침실, 풍경 등 한 가지 종류에 focus 한 게 대부분이고, 주로 많은 데이터를 요구했습니다.</p>\n<p>다양한 종류의 object 를 생성하는 것은 여전히 잘 못하고 있고, 이런 문제를 해결할려고 conditional 하게 생성을 하거나 (e.g. cGAN), task 를 특정하는 등의 방법으로 문제를 해결하려 했습니다.</p>\n<p>왜냐면 이전 방법들로는 적은 수의 데이터와 여러 종류의 데이터의 distribution 을 잘 학습하기엔 엄청 어려웠어요</p>\n<p>그럼 이런 문제들을 어떻게 하면 해결할 수 있을까에서,</p>\n<blockquote>\n<p>'단 1 장'의 이미지로 GAN 을 훈련할 수 있을까??</p>\n</blockquote>\n<p>이런 이번 논문인 <strong>SinGAN</strong> 이란 concept 이 나오게 됐습니다. (멋지죠?)</p>\n<p>물론 이전에 이런 노력을 안한건 아니에요. 정확히 논문 이름들은 기억이 안나는데, 대부분이 input 에 대해서 conditional 한 method 를 사용하고 있었습니다.</p>\n<p>또한 이전에 Unconditional Single Image GAN 이라고 하면 Texture Generation 이란 task 로 유일하게 문제를 풀고 있었는데, 이 task 의 한계는\ntexture image 에 대해선 결과가 reasonable 한데, non-texture image 에 대해서는 별로 였어요.</p>\n<p>하지만 이번에 소개할 논문에서는</p>\n<ul>\n<li>unconditional 하게, noise 로 부터 image 생성</li>\n<li>general purpose 로 natural image target (non-texture) 에도 적용 가능한 방법 제안</li>\n</ul>\n<p>합니다.</p>\n<p>물론 결과는 이전 method 들 보다 훨씬 general 하고 결과도 outperform 합니다!</p>\n<h2 id=\"technical-review\" style=\"position:relative;\"><a href=\"#technical-review\" aria-label=\"technical review permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Technical Review</h2>\n<p><strong>SinGAN</strong> 에 소개된 novelty 를 1 가지로 요약 해 보면 아래와 같아요</p>\n<p><strong>Multi-Scale Architecture (# 2.1)</strong></p>\n<p>완전 새로운 concept 는 아니고, multi-scale architecture 에 대해서는 이전에 LAPGAN 이란 GAN 에서 한 번 비슷하게 소개가 되었는데,\n궁금하시면 한번 봐도 좋을 것 같습니다.</p>\n<h3 id=\"multi-scale-architecture\" style=\"position:relative;\"><a href=\"#multi-scale-architecture\" aria-label=\"multi scale architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi Scale Architecture</h3>\n<p>SinGAN 의 ultimate goal 이라고 하면, single image 의 internal distribution 을 잘 배우는 unconditional generative model 를 만드는 겁니다.</p>\n<p>이런 것을 하려면 다음과 같은 것들을 잘 해야 할텐데,</p>\n<ul>\n<li>many different scales 로 복잡한 image structure 의 distribution 을 capture 하기\n<ul>\n<li>global properties : 이미지 내 큰 objects 들의 모양과 배열 e.g.) 하늘 위치, 땅 위치</li>\n<li>local properties : global properties 의 details</li>\n</ul>\n</li>\n</ul>\n<p>그래서 multi-scale architecture 를 선택했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB60lEQVR42jXQa2/SUAAG4P45/4l/wMQPukQ/mBiDiZosauYly+KFYJwwptskXKaI3GSzHegsUgbtoS0tbenlnPb09JwaUJ/vb943L7dfLG5uPXv17r1kuJGpe2edyNQdiJ/m8k/yJRjTyncxs7N7JilRGAn9mQqsrjh9uLt32Oa5XPb11duZy7fuZqvNeDaxSm8jZSwbi2v3N288em55wXa+cCXzoNzqeUu/UjsXfxlH9fr1ezezB4ccxhjMQKP5RQEyDKAsSe7SpZTK8mR6IYVhqGtA+i06toXjeDIFmm5C3/UcJ0KQQwilaZoQkjLmeT4AwPW8JEmiMErTlDGW/kcImamaaS6CEDseDhDmcBx32u03heLF3PZV2aztY0Od28sXxSMfRX9jpfZJkx+gIKge/1Qm9seusPF4Y/ugzOn6HEJY73xrnAqxt9RP26Fj+RA2Ol0UhmxN+DEYSlJCEl4Ya5rL94Vc4WWr1+NUTSOErBYmCQpDDQAUrriexyiNY0IIoYzFJGGMKUAN/OBkOC637nzmm5yu65RSjPFEVqy5Bmof/IWpGYvz0Xj1BaVpmpq2MxCHEAa1Yx4Ap/r109bOpb1KjvtXyxghxLbs8WhkmialFCGUJAlZi9aWjiOKQ1lWFqYx6A8d2/kDcI7XkWFbDSQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png\"\n        srcset=\"/static/7c27c9c01342508a98366734bfe86dd2/12f09/multi-scale-architecture.png 148w,\n/static/7c27c9c01342508a98366734bfe86dd2/e4a3f/multi-scale-architecture.png 295w,\n/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png 590w,\n/static/7c27c9c01342508a98366734bfe86dd2/efc66/multi-scale-architecture.png 885w,\n/static/7c27c9c01342508a98366734bfe86dd2/c83ae/multi-scale-architecture.png 1180w,\n/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png 1320w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>위 그림에서 x_0 가 original training image 이고, x_1 ~ x_N 가 x_0 에서 r 배 (r > 1) 씩 down-sampled image 입니다.</p>\n<p>각 scale 에서...</p>\n<h4 id=\"generator\" style=\"position:relative;\"><a href=\"#generator\" aria-label=\"generator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generator</h4>\n<p>noise (z_n) 와 이전 단계에서 생성된 image (~x_n-1) 를 받아서 image (~x_n) 을 만듭니다.</p>\n<h4 id=\"discriminator\" style=\"position:relative;\"><a href=\"#discriminator\" aria-label=\"discriminator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Discriminator</h4>\n<p>real image 와 (x_n) fake image (~x_n) 를 구분.</p>\n<p>하나 차이(?)점이 있다면, 맨 아래 scale stage 에서는 only noise (z_N) 를 사용해서 image 를 생성합니다.</p>\n<p>논문에서 coarse-to-fine fashion 이라고 소개를 하는데, 좀 쉽게 설명 해 보면,</p>\n<p>아래 단계에서는 down-sampled image 를 학습하니, 상대적으로 detail 보단 global 한 feature 에 집중을 하면서 학습을 하고,\n위 단계일 수록 fine feature 에 더욱 집중하게 됩니다. 동일한 receptive field 에 생성하는 image scale 이 다르니,\n위 그림에 <strong>Effective Patch Size</strong> 가 달라지면서 coarse-to-fine fashion 으로 학습이 된다 입니다.</p>\n<h4 id=\"single-scale-generation\" style=\"position:relative;\"><a href=\"#single-scale-generation\" aria-label=\"single scale generation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Scale Generation</h4>\n<p>각 G_N 부분에 해당되는 block 인데, 구조는 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABdklEQVR42n2Qy07CQABF+7cu3Kg7P4CEqjGANhEl4iOpD0wkaJSo7UIURQR5SCUMVoXa0g59SGEq0JkxsnThyV2d5G4OQ/8FITQajf5IjDGEkFLKCILA87wgXFFCKKUDt2erbUdTXF3zTKMN6gpoWKriaArq6n2oY4yHw+GbLBOMGY7j5mZnA4vLp2LmLFsEL3WvKbmgpj7lxh/g5GB/LRLiwqH04R41FCTX0QDdl6pHqbPqO2RarXa1XNpJpaeD4ZlI/O6h0Ac1rVIA9xm1lI+vR5fY4ALLJrbjxvPTa+6mAZrzXHwqEFpNXTMYY0ppoVJdicW2Eonmi2RJ5eE78BV51GrubW0usewCyyb5Xdr5kPNZ0zD4ZDKyEU2LIkMIwRg7tg113bIs+7PVzmf1yqNVK3Wfi8XL89vT5M3JcUW8sKWyUsyNvz3f900ICcHMpB/xPM91XYQQ6ru9jupon1+TIah7puGZxgB2HFXp6Rr2x7+HSd0frAJk+ZUepPoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png\"\n        srcset=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/12f09/single-scale-generation.png 148w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/e4a3f/single-scale-generation.png 295w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png 590w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png 733w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>% 이전 stage 에서 up-sampled image : x_n+1</p>\n<ol>\n<li>z_n + x_n+1 가 conv 연산을 통과</li>\n<li>x_n+1 가 residual 하게 마지막에 연결</li>\n</ol>\n<p>가 간단한 구조인데, conv block 부분을 더 자세하게 설명하면,</p>\n<p><code class=\"language-text\">Conv (3x3) - BatchNorm - LeakyReLU</code></p>\n<p>이 convention 으로 5 층을 쌓았네요.</p>\n<p>처음 (coarsest scale) 엔 32 kernels / block 으로 시작을 하고 4 scales 마다 kernel 을 2 배 늘려 주었다고 합니다.</p>\n<p>이렇게 해 준 이유는 (상대적으로 light 한 구조여서),</p>\n<p>주로 generator 의 capacity 가 커지면 training image 를 외어버리는 경우가 생기는데, 이를 방지하려고 light 하게 설계를 한 것 같습니다.</p>\n<p>또한 fully-convolutional 하게 설계를 한 이유는, arbitrary image size 에도 training / inference 가 가능하게끔 하려고 라고 설명을 합니다.</p>\n<h3 id=\"gan-training\" style=\"position:relative;\"><a href=\"#gan-training\" aria-label=\"gan training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN training</h3>\n<p>이 부분이 이제 GAN을 학습하는데 있어서 제일 중요한 부분인데, 딱히 특별한 부분은 없습니다.</p>\n<h4 id=\"loss-function\" style=\"position:relative;\"><a href=\"#loss-function\" aria-label=\"loss function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>loss function</h4>\n<p>loss 는 adversarial loss + reconstruction loss 로 이뤄졌고</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">total_loss <span class=\"token operator\">=</span> adv_loss <span class=\"token operator\">+</span> alpha <span class=\"token operator\">*</span> rec_loss</code></pre></div>\n<h5 id=\"adversarial-loss\" style=\"position:relative;\"><a href=\"#adversarial-loss\" aria-label=\"adversarial loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial loss</h5>\n<p>WGAN-GP loss 사용 했고. 논문에 보면, 다른 texture single image GAN 과 다르게, patch 별이 아닌 전체 이미지에 대한\nloss 를 사용했더니 네트워크가 boundary conditions (SM) 를 학습할 수 있었다고 합니다.</p>\n<h4 id=\"reconstruction-loss\" style=\"position:relative;\"><a href=\"#reconstruction-loss\" aria-label=\"reconstruction loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reconstruction loss</h4>\n<p>l2 loss 를 사용. 각 stage 에서의 rec loss 를 다음과 같이 정의가 가능한데,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">rec_loss_n <span class=\"token operator\">=</span> l2_loss<span class=\"token punctuation\">(</span>G_n<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">~</span>x_n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x_n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>또한 ~x_n 의 역할이 하나 더 있는데, stage n 에서의 noise z_n 에 대한 std 값을 결정하는데 쓰여요.\n~x_n+1 하고 x_n 의 RMSE 값을 구해서 각 scale 에 얼마만큼 더해야 하는지를 알려주는 정도로 사용된다고 합니다.</p>\n<p>또 중요한 부분은 noise 를 넣어줄 때, 첫 단계에만 fixed noise 로 넣어주고 다른 단계에서는 noise 를 따로 만들어 주지 않았는데,\nimage pixel difference 를 줄이려는 것에 focus 를 하려고 이렇게 했다고 캅니다.</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"정량적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\" aria-label=\"정량적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정량적인</h3>\n<p>총 2 가지의 정략적인 방법을 사용했는데,</p>\n<ol>\n<li>Amazon Mechanical Turk (AMT)</li>\n<li>Single Image Frechet Inception Distance (SIFID)</li>\n</ol>\n<p>요즘 GAN paper 들에서 자주 사용하는 metric 들입니다.</p>\n<h4 id=\"amt\" style=\"position:relative;\"><a href=\"#amt\" aria-label=\"amt permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AMT</h4>\n<p>AMT 는 사람들에게 직접 답을 하게 해서 결과를 매기는 서비스입니다.\n여기서는 해당 이미지가 Fake 인지 Real 인지를 구별하게 하는 투표 방식을 사용했습니다.</p>\n<p>여기선 2 가지 방식으로 조사를 하였는데,</p>\n<ol>\n<li>실제 이미지와 SinGAN 이 생성한 이미지를 보여주고 어느 쪽이 가짜인지 맞히는 Paired 실험</li>\n<li>둘 중 하나만 보여주고 얼마나 헷갈렸는지를 물어보는 Unpaired 실험</li>\n</ol>\n<p>실험 조건은, 각 실험 당 1 초의 시간에 1 명당 50 장의 image 를 보여주었답니다.</p>\n<p>생성한 이미지는 stage N, N - 1 에서 생성한 이미지들을 주었다는데, (논문에선 N -2 까지)</p>\n<ul>\n<li>stage N 은 noise 로 부터 생성한 이미지고</li>\n<li>stage N - 1 은 진짜 이미지를 축소해서 G_N-1 에 넣어주는 방식인</li>\n</ul>\n<p>이런 실험에선 노이즈로 부터 생성할 때와 진짜 이미지 기반으로 생성할 때의 차이를 볼 수 있는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 95.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEfUlEQVR42gXBCVNSCQAA4PeXdne22UZcLDQJDZUgjwcIIud7D/AEUeKSJFMMUR7H4/A9lSM5BRTTsHbLVsew1KbZptqjmd01W3c9UnR09/uA84uLT/vFT4f//fz73ubbjzt7R7v7x7v7xzuHF29++7z9fufdx8+7+8ef/v2ye3j+7o/9rfd/vf7w5997n49Pz4Cj4rlkJMXRWvgGa/NgBLTmRI6HkGOpqcfC0ei4BlvTQIhjy0HOZbbBzVZrOLp7wpEIX6sLZrPAUfFCjmaqGynMloqWQT84vixw5xXeH+oVslqQVN9azdMNCNyPFYEVUGNmC6/xpTS2ukvkWcLmV4CTky/hKdugpcM+bnJ67tgcOmfAOk1YCa/ZN9oWd0kzmDDuhcO4JeDpn3AbvS6T22UcH+teXIwAZycHr7zVG87SV54rr9xlBfTyVoC6jTM2XeTXPspLF+mls2QTK9vGazdR0raHvOUhbXpIL8a++rDQBxRPjoPEKO4bCRL2EDE2GRiOTFmjQfuk3xoiRqdwO+GzBAMDsdDolH8oTNiChC04YQ2OiV88iwKHJ+dca64G6WQqpEyEX6uxchxPBeNPauRdDKSV0W2s7Z9hjDwWoCsM1VCdhH1TibDvYFyjdSKVAd798lHpnGuAJI1iFihh8LS32/AnXf68QGvkypgiJbttcLgDS/bg+db+UWkHD+rmCbR9Es8itrAKFNZXky445oQiKBxGkRmXLOVHsnjnLCZ96G1Zwjh5981lD+MRDmUwWdqLJLyKhBcJ3Wc9z40Db99sraDMddf1NRd1Db3201h5wc/aCICrjsp1d9Wai/rcSVvD6jZwcN1RueG5XnBXrLsqCm7Sr8tmYGurEI8h6QSSTsgzSXkyJkrPyheyqtm4LJOQZ+LQbFySTkgfzqlTMVE6AWcScDYJo3db8mkc2Hi5asPrYCUdljQr5XwEYtxDWVOLIo0J5IKNcrhVCTf1D98IPpJZXKwWHghL+Ea9vObGNZ/fAez9s4MSLJB3qaamtFlQdoNOUsC3prM8Mfx9A0gWwZS6OnILlz6dEveYrjJZpVAHtbNbwOdXJbMjQPH0AJ1mqQxko7XSYCnv0V1t76zBU+wuQ4nacMWK0VS3yxBFFRZqNt0vU6hKhzGqeYiGKEuCD+4AxdOj9GJvdF4enVfG5ttjOUU8151Z0gcTEBGDQ2kkPAvNzLXN5fvDKWRiRjadRB6kFX5C/uNKGDg5PSSyQqtdpNOLNRqBXgeik42RR/CQo7lPIzGbIZ0etPtYkSXETnD6ekVmE2wfbatnXPd5nMDJ6YE/yWVzyqkUMptDEQqpWi0YnBdI4Up61dWGW5W0CjIkpofnZH39VDqtvKmByqyl1TMpmZwDKJ4dWH3VkvZvoM5L2nulAy6KvJ2ORZmw+us27WWzkyxXfScUV6BEY+/gJbHyW4unrENXwuZ9jQeNwOnZ8eLy3WiqNxLrS6T0cwumZMa8/GzkQbxnMqSOp2+n0rpYSp9/aktmtPi0KpbSprOGSFS9Wpj5H8QgfVucKSs6AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png\"\n        srcset=\"/static/f68470a6fe3e8debb47d08a2175cacf2/12f09/generate_from_different_scales.png 148w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/e4a3f/generate_from_different_scales.png 295w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png 590w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png 745w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>실험 결과를 보면, stage N, 노이즈로 부터 생성한 것은 원본과는 많이 다른 결과를 가져올 수 있고,\nstage N - 1, N - 2 는 원본 형태 유지는 되고, N - 2 같은 경우엔 texture 가 더 원본 같다는 것도 확인 할 수 있습니다.</p>\n<p>하지만, 원본 object 의 배열을 유지한 상태에서 다양성을 보장하기엔 stage N - 1 결과물이 제일 좋다고 판단이 가능하네요.\nAMT perceptual study 결과도 N - 1 stage 일 때가 가장 좋습니다.</p>\n<p>재밌는 점은 stage N - 1 일 때, <strong>confusion 이 47 %</strong> 인데, 사람이 보기에도 정말 헷갈리나 보네요.</p>\n<h4 id=\"sifid\" style=\"position:relative;\"><a href=\"#sifid\" aria-label=\"sifid permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SIFID</h4>\n<p>기존 FID 를 Single Image 에 맞게 변형해서 SIFID metric 을 제안해서 사용합니다.</p>\n<p>각 scale 별 SIFID 를 측정하고 AMT 에서 Survey 한 결과하고 (Paired, UnPaired) correlation 를 측정해서 유의미한 metric 임을 증명하네요.</p>\n<p>요 부분은 논문 6 ~ 7 페이지에 나와있는데, 여길 참고하세용 (<del>귀찮아</del>)</p>\n<h3 id=\"정성적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\" aria-label=\"정성적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정성적인</h3>\n<p>총 5 개의 application 예시를 보여주고 있는데,</p>\n<h4 id=\"single-image-super-resolution-sisr\" style=\"position:relative;\"><a href=\"#single-image-super-resolution-sisr\" aria-label=\"single image super resolution sisr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Super Resolution (SISR)</h4>\n<p>결과 비교를 위해\ninternal method 인 Deep Image Prior (DIP), Zero-Shot Super Resolution (ZSSR)\nexternal method 인 SRGAN, EDSR 와 결과를 비교했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABOUlEQVR42gXBi0rCUAAA0H1kFGJqJptJaob5GJube4BeF2oKKhZFYazminBjrS4brJfki5lbgmkR/UjnIFL3zDR6uipCXdL1G1URjceupooQ3hnw1oCyft81TUVTLk0oa9r1gy4bUFZ6nSdLQxL7a+mUlyX9ZRChyACe8RwCTABYJuuhSV+tsssxWwThz+e89WoMxz0k4RdAGBRRAOKIUIrSdJgiQwyNths5notQVLBUip20czSNsixarSZbTYIgAiwXbjXwIkhQ+RDLYZJUQZLJ9Vp5r1GJkllfsRC/OiePhJ1s2ttqpi+OUwV2G8c3xQ7TrscYKsjzmCyy9XIkdbBxdsojiy93uXS+l7PZx9CyYP/VmLvDT3cyGb+tFvbPyp1OB4N3yx4925M+hKo9evn7nTvOyJmN/wHUf7Lp8DtRtwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png\"\n        srcset=\"/static/7c177a1b2169c7eb75c0a40183049776/12f09/sisr_result.png 148w,\n/static/7c177a1b2169c7eb75c0a40183049776/e4a3f/sisr_result.png 295w,\n/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png 590w,\n/static/7c177a1b2169c7eb75c0a40183049776/efc66/sisr_result.png 885w,\n/static/7c177a1b2169c7eb75c0a40183049776/c83ae/sisr_result.png 1180w,\n/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png 1543w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>(distortion quality 인 RMSE 가 낮을 수록 좋고, Perceptual Quality 인 NIQE 가 높을 수록 좋음)</p>\n<p>다른 Internal Method 에 비해 RMSE 는 높지만, external method 인 SRGAN 과 NIQE 값은 comparable 합니다.</p>\n<p>1 장의 이미지만 사용해서 이 정도 결과라서 정말 신기하네. 개인적으론 PSNR 같은 다른 metric 하고 모델 들도 넣어줬으면 좋을 것 같네요.</p>\n<h4 id=\"paint-to-image-style-transfer\" style=\"position:relative;\"><a href=\"#paint-to-image-style-transfer\" aria-label=\"paint to image style transfer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Paint-To-Image Style Transfer</h4>\n<p>Paint Image 로 부터 생성하는 style transfer 실험인데, quality 가 꽤괜입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABeUlEQVR42gFuAZH+AMTQ47bF3b7M4dHe8bvK4rG+1MLP5c/a7bvH2r7K27rI3sbY9avF6LjG2arC5MXX9L7L5LPB2bXE3dzl8wC5wMO4urC7uKamt86YqsKXsNaRrdmrucu5u7S2vsG0v8movtqHp9CLrtmLrtyeuN6erMGdtNmSrdi+zeUAlohkk4dhlohkm5OAkolxjYuAiYyMn5eDoo9in5BtoJJznpJ3gXBLcG1XdW5ZiX9wmo1ulo56jIZ3ubWtAIWAXIV/XomEYo6OYoGEUoWGUoSFUI6MYYeDW4aCVoaBV42KUX+AL3t8OnZ3K36AQYaIXYeHWXx9SbCwkQB7gDeGgz2GikKTmUt/ii+BijWDjTKLk0h/hD9+gj+AhECPl0N+jh18ih+EkCCNl0KFizeEizx+hSyvtn4Amp1pn6Fqlp5qo6pznqNloKRmnqZmpq10nKBunaFtm6Nsp65wl6BZmqFXm6NZqq1wnKFooqRrlZ5dw8WdW9zR/ZdEdgcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png\"\n        srcset=\"/static/3389c01a5021789c0be4714d0ba0749b/12f09/paint_to_image_style_transfer_result.png 148w,\n/static/3389c01a5021789c0be4714d0ba0749b/e4a3f/paint_to_image_style_transfer_result.png 295w,\n/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png 590w,\n/static/3389c01a5021789c0be4714d0ba0749b/efc66/paint_to_image_style_transfer_result.png 885w,\n/static/3389c01a5021789c0be4714d0ba0749b/c83ae/paint_to_image_style_transfer_result.png 1180w,\n/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png 1531w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"editing\" style=\"position:relative;\"><a href=\"#editing\" aria-label=\"editing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Editing</h4>\n<p>원본 이미지에 일부 영역들을 임의의 무언가로 넣으면 이 친구가 얼마나 자연스럽게 만들어 주는지 확인 해 주는 task 인데,\n이것도 꽤 재밌는 결과를 보이네요.</p>\n<p>아래 이미지에서 <em>Content Aware Move</em> 가 포토샵 기능인데, 이 것보다 잘하는 거 같네요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADnklEQVR42gGTA2z8AOjr8tPY4MHGzri+yLC7x73G0cHN3K63w9LX4ubq9OLm7uPo8srR2bjBy7bCzrvG0bK+zdLb5+Hn8urt9ABxiLFXeKttkLuBpc2Xn6aOgHZ8en9th6xYfrhlgbNlfapbgLZ2eYeHcmaTk5OGeXF7fYZtiK9PdK+QpcoAh5/Ah6LBpMHbr6ykpXxam29PkmI+gXNtjKvMj4+WepKzfZ/EjZSflGlFkmM9hlQwj186h2RKaXKDtLCwAK++zMDW5c7T0YhgRXBNPY5uXLSQcINhSIqJi6KMd6q7yanB1J3A34iBf2M5JYxpVayAWHdRNnlybcCqlQB0bW2ipaPKsZFwSzVdPzbX2Nbm5NigflyhfVWggWNwa2ujpqS4wL95aWRrTULn7e3XzLiPaUedeFHCq5UAQzxDSzQomndUVTktspp2//C779qnw6BusIlcn4FkLyYvZEg1kW5NUzYqzLWL/+666tShwJxsp4BUvqeRAIlvVoVeOmxKMlY0JaaEVOjBeNivbLOLWqJ5TaF/XoFjSYVeOWJAK148KLqWX+fAdtOraa6GV5twQ8CnjwD66dfYx7W7sau0rKizqqfIvbTIu7C8r6XXybvv4tX459Tf0MGxqKSrpaOyq6nFurC8sKfGu7Hm2Mnw6OAAkKfIb5K8haLAjpqol7HGl6GpjqO3gKTIbpbFh6TOhJ/EeZ3Hi6C4k5mimaq3mKKriKC3faPLcpnLprzbAG+Ns12JtoWhuKJ+YJ13WI1cO4ZdQXxxb2aDonGHomWDqWOSv5GPjpVeNJhrSIpZN4tiRYV4cFt7n52puAClvtOOrceqx9qhiHN4RiqETzCeZTuLVjKCgIKomImetcmYudOkv9CYemF3RSqFTy+kbEGKWDN6cWzFsJ0AkpSYtcTKutPfkH5yWC8gsamo1cSyjWJBi29YooNokJSYs8rXt9rwmYl7Yjcowr/Az7eeglo7jm5Sw6qUAEg8QFlFPLSWd2ZFNpFzXv3z1+7ixLmRZreJWqSCZDYoLG1fWbKlk2xJOKyReP/53ObWtLeNYK+CVMOpkwBdTEZmQCZ3TC5JJRi+mmX81ozsxX/GmmOpe02felpRPjV2SilnOyBRKxvUr3P50ojnv3q+kl2gcEPApIsA17eYxJ9/mX9xk3hspYt32LiR0K6JuZuBw6OGyq+X17aUvp6Bln9zl31xr5R/27uUyqqJu56GxKSG28i45UAFhVnJVbMAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png\"\n        srcset=\"/static/827c27d77bcc338201f45b2bc70727ea/12f09/editing_result.png 148w,\n/static/827c27d77bcc338201f45b2bc70727ea/e4a3f/editing_result.png 295w,\n/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png 590w,\n/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png 739w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"harmonization\" style=\"position:relative;\"><a href=\"#harmonization\" aria-label=\"harmonization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Harmonization</h4>\n<p>Image 2 장을 합쳤을 때 조화롭게 잘 합쳐주는 지를 보는 task 인데, 이것도 꽤 자연스럽게 잘 되는 것 같아요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.72972972972974%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACqklEQVR42gGfAmD9ANLU2cbJ07i7xp6iscDE0c/U3cvN0qSmraKksKaqtqSmsqGlsqqvtczO076/yJ2gq6qtvKGksbG2w97h5QBncpU/SXJOW35TWnhNXYh/i3ess59JVIVJVHlNW4BPXH9XZ4qjqn6IkaE9SHZRXoFMV3ZSYIZmdX+0uY8AgI6iTVxxU2BxXV9ifIWaboOXqbS1Xm6GT191T1+BWWqHXnWYgpSbmaawUF92U2J2TVZfbnmGZnqWnq2sAIGQnDlES3WEj19yjGNwfmRwgJScp1lodEhSWXiKnFltiWByiGp2hpGbp0BNV2Rvdmp+llpqfmBvfoOMmQBjbHkdISAuNDg6SF4xPVY2Qlp9hZI3Q1IaHxwxPUs2QlwuOlRLV290fY0hKzMhJyc3RFkyP1srOlZtd4oAfX+AUlheVVphZmpzam57Zm9/k5aYdm9gYVtUXllYbHBvZ2lsaW1ykZCGamRXXFVRY2JhbHBxXWBmgoaIADZAYw0rayQ9flBTl2NqrzVUs4KOq/vHhNidedGLY8iEW8F6SbBpSdy+iu+6g9aYdM+HX8iHV7loN72GbQB+XGJoOTqNXEyYcGCUhImCgpausJncx4Ocem/Wpkzps0LQo0Wce0PTzYCyrISSiGritEbRqVenjGCkjWcAxohf5ptC89Bnya5wxZZLl3VLsZ1n16FN1YIounsoa1M1OC0rLTA0sqVlyJhJ1IMgmW0uTEhMLTNHXGNkAII8PIcnB7V6LdqfM5VJKik0UoODbOi+T+6VF6s3I2kyPy0jQSsxVaicauy8PN5pEJEvMVcyQhYTNllfcgCBeoZvVmKvhGrftXbBi3p9haeprKfx1HrmsVvLclOYam1wa39haYXMwIrx0nPfl1O4aVqHb3hiY36EipudmTwVEQp14gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png\"\n        srcset=\"/static/261bf4d359af935e3dfea0a8c3c3118d/12f09/harmonization_result.png 148w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/e4a3f/harmonization_result.png 295w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png 590w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png 732w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"single-image-animation\" style=\"position:relative;\"><a href=\"#single-image-animation\" aria-label=\"single image animation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Animation</h4>\n<p>한 장의 이미지를 넣어주면 짧은 video clip 을 만들어 주는 task 인데, 이 것도 자연스럽게 잘 되는 것 같네요</p>\n<p><a href=\"https://www.youtube.com/watch?v=xk8bWLZk4DU&#x26;feature=youtu.be\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">video</a></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>이번에 SinGAN paper review 를 해 보았는데, 이미지 한 장만 사용한다는 점과 Application 들이 정말 좋았던 paper 였어요.</p>\n<p>개인적으론 network 설계나 등등 요소들은 조금 아쉽네요.</p>","excerpt":"TL;DR 이번 포스팅에서는 ICCV 2019 에서 Best Paper Awards 에서 선정된 papers 중에 하나인 SinGAN 을 리뷰해 보겠습니다. 개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#technical-review\">Technical Review</a></p>\n<ul>\n<li>\n<p><a href=\"#multi-scale-architecture\">Multi Scale Architecture</a></p>\n<ul>\n<li><a href=\"#generator\">Generator</a></li>\n<li><a href=\"#discriminator\">Discriminator</a></li>\n<li><a href=\"#single-scale-generation\">Single Scale Generation</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#gan-training\">GAN training</a></p>\n<ul>\n<li>\n<p><a href=\"#loss-function\">loss function</a></p>\n<ul>\n<li><a href=\"#adversarial-loss\">Adversarial loss</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reconstruction-loss\">Reconstruction loss</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiment-result\">Experiment Result</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\">정량적인</a></p>\n<ul>\n<li><a href=\"#amt\">AMT</a></li>\n<li><a href=\"#sifid\">SIFID</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\">정성적인</a></p>\n<ul>\n<li><a href=\"#single-image-super-resolution-sisr\">Single Image Super Resolution (SISR)</a></li>\n<li><a href=\"#paint-to-image-style-transfer\">Paint-To-Image Style Transfer</a></li>\n<li><a href=\"#editing\">Editing</a></li>\n<li><a href=\"#harmonization\">Harmonization</a></li>\n<li><a href=\"#single-image-animation\">Single Image Animation</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/SinGAN/"},"frontmatter":{"title":"SinGAN - Learning a Generative Model from a Single Natural Image","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["GAN","multi-stage","one-shot"],"update":"Mar 14, 2020"},"timeToRead":5}},"pageContext":{"slug":"/SinGAN/","series":[],"lastmod":"2020-03-14"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}