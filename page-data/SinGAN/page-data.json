{"componentChunkName":"component---src-templates-post-tsx","path":"/SinGAN/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 ICCV 2019 에서 <a href=\"https://syncedreview.com/2019/10/29/iccv-2019-best-papers-announced/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Best Paper Awards</a> 에서 선정된 papers 중에 하나인 <strong>SinGAN</strong> 을 리뷰해 보겠습니다.</p>\n<p>개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서 저도 간략한 overview 와 technical review 를 해 보려고 합니다.</p>\n<p>소개 전에 간단하게 SinGAN 으로 뭘 할 수 있는지 보면, <strong>단 한 장의 이미지로 realistic 한 image manipulation 들을 생성</strong>할 수 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVQY0wExAc7+AISYoouir5Skp6Soo5SrtHaXpo6go4OPkarB0aq3uXuEgpartJewu4mXl36RloOeqZGtupulpH2OkHOBfgBOZ2hWcndpdnOGhHxefH8+YmdkcWxPWFJriZZ+iodRV01he35ifoNjbWRLYF1NamtbeYBrcGpGV1E+RDkAj3tUjnhQmH5UxrWenY9uiXhNoYhgnIVgjHhQmX9aoopmk3tSmHhPo4Rel4RfkHdQmoJZo4RdmYFbmYFaAPmyXvm2Xf/GY/vbo/K2bv3AYP3Favy+aPu2XPm5X/3DbfqsVv2vWPqzXvy+bParVPy+ZPe2X/63X//FagCbbz+jcjy1eTfTsIylf1ebaTOhckCneEeRZjepdDy9hkuncTmpcTmtczy8hVCXajqkcjyteUCzfkK+gUN92KAsupI/ZAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png\"\n        srcset=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/12f09/teaser.png 148w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/e4a3f/teaser.png 295w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png 590w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/efc66/teaser.png 885w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/c83ae/teaser.png 1180w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png 1493w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACbUlEQVQozwFiAp39AOnt7czZ37fN1dTd3tnd4qK7+Yal4nqi5KnL/tTVzdLNvM7KvNjb2KGwvqOyv+nr7f///6Sdr56Ou8vA5QCfpaeVo56twMORpZlygXF+kKx7m9dKkPVZjdN/j368tYbUy6HExax5fX90go3y9fn///91apJOOn98b50AeoJrUGAqbHlAWGYncn1QxLaGp45vmZeWjHVraX1SanxMnZ1ssbWMh2RciGBe+/79///8Zll6W0WLb2GRAK27rombgYWcgXWRfoOYl4CItnR6o3N1ooyQuaaqkqmqiKOhgKmtl1NsdWVyerC/zam7xmlfiWpUm1lOeAC+xcehqqyGlZ6GmqKXq7d7jr56kNVLgv9UiPqSmYm5q4Xd063DxrQ2YolaYWdaaHU3Z4pNQmhwWJ5sXo8AkJyBWG1BZndOYXFIgpBtt619oIJlf4SVd3V9aHBKZFw4rKt6qbWVWmt6d2dblIN3ZYGWU095UT9+TkRpAK28poWad36Ud3WScY+ijY6QrWtqg3RsgY6Hm6SkiJaVc5STbKGlj2x4hnd4eYKCg3yRnVFJcWlSnH1toADLztG3vsGmsbirusCzvsl5i8l2jtlNgf9XjP2Zo5TFu5be0rPEx7g4ZYpVYm1Zbn9Ab49kXYFWRYNWTHIAi5R8WWs3ZXZFZXVFhZJwrqd+lnxpc4GmZmyGYXBIXmI1tK56qraWSWJ0ZlxTf3JsUnWLQjtnUjmHbV2WALO6n4qYY5Whb4yaZp6ohqynnp6OgKKMfqyYi56lhpWaeJ+feauulY+RlZ+Uja6hmZumro+Oq3NqjIaBlmnTUbaLoHO2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png\"\n        srcset=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/12f09/manipulation.png 148w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/e4a3f/manipulation.png 295w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png 590w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/efc66/manipulation.png 885w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png 964w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1905.01164.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>official implementation : <a href=\"https://github.com/tamarott/SinGAN\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>기존 GAN 들을 대부분의 연구들을 보면 얼굴, 침실, 풍경 등 한 가지 종류에 focus 한 게 대부분이고, 주로 많은 데이터를 요구했습니다.</p>\n<p>다양한 종류의 object 를 생성하는 것은 여전히 잘 못하고 있고, 이런 문제를 해결할려고 conditional 하게 생성을 하거나 (e.g. cGAN), task 를 특정하는 등의 방법으로 문제를 해결하려 했습니다.</p>\n<p>왜냐면 이전 방법들로는 적은 수의 데이터와 여러 종류의 데이터의 distribution 을 잘 학습하기엔 엄청 어려웠어요</p>\n<p>그럼 이런 문제들을 어떻게 하면 해결할 수 있을까에서,</p>\n<blockquote>\n<p>'단 1 장'의 이미지로 GAN 을 훈련할 수 있을까??</p>\n</blockquote>\n<p>이런 이번 논문인 <strong>SinGAN</strong> 이란 concept 이 나오게 됐습니다. (멋지죠?)</p>\n<p>물론 이전에 이런 노력을 안한건 아니에요. 정확히 논문 이름들은 기억이 안나는데, 대부분이 input 에 대해서 conditional 한 method 를 사용하고 있었습니다.</p>\n<p>또한 이전에 Unconditional Single Image GAN 이라고 하면 Texture Generation 이란 task 로 유일하게 문제를 풀고 있었는데, 이 task 의 한계는\ntexture image 에 대해선 결과가 reasonable 한데, non-texture image 에 대해서는 별로 였어요.</p>\n<p>하지만 이번에 소개할 논문에서는 </p>\n<ul>\n<li>unconditional 하게, noise 로 부터 image 생성</li>\n<li>general purpose 로 natural image target (non-texture) 에도 적용 가능한 방법 제안</li>\n</ul>\n<p>합니다.</p>\n<p>물론 결과는 이전 method 들 보다 훨씬 general 하고 결과도 outperform 합니다!</p>\n<h2 id=\"technical-review\" style=\"position:relative;\"><a href=\"#technical-review\" aria-label=\"technical review permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Technical Review</h2>\n<p><strong>SinGAN</strong> 에 소개된 novelty 를 1 가지로 요약 해 보면 아래와 같아요</p>\n<p><strong>Multi-Scale Architecture (# 2.1)</strong></p>\n<p>완전 새로운 concept 는 아니고, multi-scale architecture 에 대해서는 이전에 LAPGAN 이란 GAN 에서 한 번 비슷하게 소개가 되었는데,\n궁금하시면 한번 봐도 좋을 것 같습니다.</p>\n<h3 id=\"multi-scale-architecture\" style=\"position:relative;\"><a href=\"#multi-scale-architecture\" aria-label=\"multi scale architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi Scale Architecture</h3>\n<p>SinGAN 의 ultimate goal 이라고 하면, single image 의 internal distribution 을 잘 배우는 unconditional generative model 를 만드는 겁니다.</p>\n<p>이런 것을 하려면 다음과 같은 것들을 잘 해야 할텐데,</p>\n<ul>\n<li>\n<p>many different scales 로 복잡한 image structure 의 distribution 을 capture 하기</p>\n<ul>\n<li>global properties : 이미지 내 큰 objects 들의 모양과 배열 e.g.) 하늘 위치, 땅 위치</li>\n<li>local properties : global properties 의 details </li>\n</ul>\n</li>\n</ul>\n<p>그래서 multi-scale architecture 를 선택했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB5klEQVQozyVRSW/TQBj1P+PPcOOABIdKnOiBSggqISGhilYiEKmlUpGglLpJWByi0mx1m5DEzmbSJJ7xFo93xx7H4XP7Du/wLW++94YpFot7ufzn05JM/KWpu2Ir4yU9OGYPWG6ZpH96/3YOT4ZzNY5isY8VTFoS+sB+564E5n3u3cOnW/c3X+7/qkaz8YL9GE1HE1V/9OLVxutdw/Z2P3158Gz77LxuE6dQ6omCdsJVHj9/kjv+xvi+3x8OTovsSBqHvj8biI5FkiQR+4LQ64RheDMZt1u8rqlxTMfSDGGdmIY8n9rEZKC9Xq/TdQbHdacytmwnXQGSrJ7edTJQSrGi6LoRxeAgTZKUAb1Go/GVLWDiOmiml9lIVwzbOSpwXhjdrVWuu3xvGPh+uSLOb8wffHdzbytf4BhV0yxinnG/ueZVbBOlxkFglutBxQuC9e3L1Uu+3RNoTOvNEUJ285rPHe5w1QsGY0xpdmFC4yAMEUJBEEbLyPM8cB5FcGMMEmADSEbYc732aPKz9uai08yWYQgmZnOZaBoqsz5ZYN0Qx1KmuFoBLyyrOxiFYVCu/MXILp2X3+bvHZX2GdC+DWYFyemaNux2FEWJKYXwICEQBYYfcT3XXBjtVkeSJkie12sNVVH+A4CD11KWlOH2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png\"\n        srcset=\"/static/7c27c9c01342508a98366734bfe86dd2/12f09/multi-scale-architecture.png 148w,\n/static/7c27c9c01342508a98366734bfe86dd2/e4a3f/multi-scale-architecture.png 295w,\n/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png 590w,\n/static/7c27c9c01342508a98366734bfe86dd2/efc66/multi-scale-architecture.png 885w,\n/static/7c27c9c01342508a98366734bfe86dd2/c83ae/multi-scale-architecture.png 1180w,\n/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png 1320w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>위 그림에서 x<em>0 가 original training image 이고, x</em>1 ~ x<em>N 가 x</em>0 에서 r 배 (r > 1) 씩 down-sampled image 입니다.</p>\n<p>각 scale 에서...</p>\n<h4 id=\"generator\" style=\"position:relative;\"><a href=\"#generator\" aria-label=\"generator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generator</h4>\n<p>noise (z<em>n) 와 이전 단계에서 생성된 image (~x</em>n-1) 를 받아서 image (~x_n) 을 만듭니다.</p>\n<h4 id=\"discriminator\" style=\"position:relative;\"><a href=\"#discriminator\" aria-label=\"discriminator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Discriminator</h4>\n<p>real image 와 (x<em>n) fake image (~x</em>n) 를 구분.</p>\n<p>하나 차이(?)점이 있다면, 맨 아래 scale stage 에서는 only noise (z_N) 를 사용해서 image 를 생성합니다.</p>\n<p>논문에서 coarse-to-fine fashion 이라고 소개를 하는데, 좀 쉽게 설명 해 보면,</p>\n<p>아래 단계에서는 down-sampled image 를 학습하니, 상대적으로 detail 보단 global 한 feature 에 집중을 하면서 학습을 하고,\n위 단계일 수록 fine feature 에 더욱 집중하게 됩니다. 동일한 receptive field 에 생성하는 image scale 이 다르니,\n위 그림에 <strong>Effective Patch Size</strong> 가 달라지면서 coarse-to-fine fashion 으로 학습이 된다 입니다.</p>\n<h4 id=\"single-scale-generation\" style=\"position:relative;\"><a href=\"#single-scale-generation\" aria-label=\"single scale generation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Scale Generation</h4>\n<p>각 G_N 부분에 해당되는 block 인데, 구조는 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABcklEQVQY031QXUvCYBjdj+2i2/IndKFCULksK4MKgoyQoi+pTI1Y2EzTqbkt2zs3nW2uvcvp1j5ee627LjocHg6H58DhEJN/MbZtz/f/mK7raZqGBZHNZg9SqbtCYWqjYGjCgSx9dOXP976tayL7Irf5D6UH+4o9UC0dZ5DtOCIAWBAxkgyF5hdj8VuKzj0xgOOcdtN6bapM+QtwJwf722urm3Eyf5KeqLItsCPbeaxUTzM3vGIQAgBM9TmZPp8Nx+bIvRJdHrKMXKFZqiCVi1vr8cVoNBqJHO/vqfXyG001WC60vDmzsLR2dk8EQYD7PpTolY3EzmGqw7NGq+ZJbdQFrsjvJrem4XDk8ig16XeEp6I+0PDbUiJ+lc8TCCHP86Fh6JoKIdQloFSKsFUbcnV8a7lr+uriMXPGUfkR3+hVS77r+p5vGsZ0sN8BLcsyTXM0HjvWUJdEVRS0DiaAPclUZEyjK6niG1S6ge+jHwQIfQNxBWVsL3lx2wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png\"\n        srcset=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/12f09/single-scale-generation.png 148w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/e4a3f/single-scale-generation.png 295w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png 590w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png 733w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>% 이전 stage 에서 up-sampled image : x_n+1</p>\n<ol>\n<li>z<em>n + x</em>n+1 가 conv 연산을 통과</li>\n<li>x_n+1 가 residual 하게 마지막에 연결</li>\n</ol>\n<p>가 간단한 구조인데, conv block 부분을 더 자세하게 설명하면,</p>\n<p><code class=\"language-text\">Conv (3x3) - BatchNorm - LeakyReLU</code> </p>\n<p>이 convention 으로 5 층을 쌓았네요.</p>\n<p>처음 (coarsest scale) 엔 32 kernels / block 으로 시작을 하고 4 scales 마다 kernel 을 2 배 늘려 주었다고 합니다.</p>\n<p>이렇게 해 준 이유는 (상대적으로 light 한 구조여서), </p>\n<p>주로 generator 의 capacity 가 커지면 training image 를 외어버리는 경우가 생기는데, 이를 방지하려고 light 하게 설계를 한 것 같습니다.</p>\n<p>또한 fully-convolutional 하게 설계를 한 이유는, arbitrary image size 에도 training / inference 가 가능하게끔 하려고 라고 설명을 합니다.</p>\n<h3 id=\"gan-training\" style=\"position:relative;\"><a href=\"#gan-training\" aria-label=\"gan training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN training</h3>\n<p>이 부분이 이제 GAN을 학습하는데 있어서 제일 중요한 부분인데, 딱히 특별한 부분은 없습니다.</p>\n<h4 id=\"loss-function\" style=\"position:relative;\"><a href=\"#loss-function\" aria-label=\"loss function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>loss function</h4>\n<p>loss 는 adversarial loss + reconstruction loss 로 이뤄졌고</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">total_loss <span class=\"token operator\">=</span> adv_loss <span class=\"token operator\">+</span> alpha <span class=\"token operator\">*</span> rec_loss</code></pre></div>\n<h5 id=\"adversarial-loss\" style=\"position:relative;\"><a href=\"#adversarial-loss\" aria-label=\"adversarial loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial loss</h5>\n<p>WGAN-GP loss 사용 했고. 논문에 보면, 다른 texture single image GAN 과 다르게, patch 별이 아닌 전체 이미지에 대한\nloss 를 사용했더니 네트워크가 boundary conditions (SM) 를 학습할 수 있었다고 합니다.</p>\n<h4 id=\"reconstruction-loss\" style=\"position:relative;\"><a href=\"#reconstruction-loss\" aria-label=\"reconstruction loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reconstruction loss</h4>\n<p>l2 loss 를 사용. 각 stage 에서의 rec loss 를 다음과 같이 정의가 가능한데,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">rec_loss_n <span class=\"token operator\">=</span> l2_loss<span class=\"token punctuation\">(</span>G_n<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">~</span>x_n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x_n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>또한 ~x<em>n 의 역할이 하나 더 있는데, stage n 에서의 noise z</em>n 에 대한 std 값을 결정하는데 쓰여요.\n~x<em>n+1 하고 x</em>n 의 RMSE 값을 구해서 각 scale 에 얼마만큼 더해야 하는지를 알려주는 정도로 사용된다고 합니다.</p>\n<p>또 중요한 부분은 noise 를 넣어줄 때, 첫 단계에만 fixed noise 로 넣어주고 다른 단계에서는 noise 를 따로 만들어 주지 않았는데,\nimage pixel difference 를 줄이려는 것에 focus 를 하려고 이렇게 했다고 캅니다.</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"정량적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\" aria-label=\"정량적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정량적인</h3>\n<p>총 2 가지의 정략적인 방법을 사용했는데,</p>\n<ol>\n<li>Amazon Mechanical Turk (AMT) </li>\n<li>Single Image Frechet Inception Distance (SIFID)</li>\n</ol>\n<p>요즘 GAN paper 들에서 자주 사용하는 metric 들입니다.</p>\n<h4 id=\"amt\" style=\"position:relative;\"><a href=\"#amt\" aria-label=\"amt permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AMT</h4>\n<p>AMT 는 사람들에게 직접 답을 하게 해서 결과를 매기는 서비스입니다.\n여기서는 해당 이미지가 Fake 인지 Real 인지를 구별하게 하는 투표 방식을 사용했습니다.</p>\n<p>여기선 2 가지 방식으로 조사를 하였는데,</p>\n<ol>\n<li>실제 이미지와 SinGAN 이 생성한 이미지를 보여주고 어느 쪽이 가짜인지 맞히는 Paired 실험</li>\n<li>둘 중 하나만 보여주고 얼마나 헷갈렸는지를 물어보는 Unpaired 실험</li>\n</ol>\n<p>실험 조건은, 각 실험 당 1 초의 시간에 1 명당 50 장의 image 를 보여주었답니다.</p>\n<p>생성한 이미지는 stage N, N - 1 에서 생성한 이미지들을 주었다는데, (논문에선 N -2 까지)</p>\n<ul>\n<li>stage N 은 noise 로 부터 생성한 이미지고</li>\n<li>stage N - 1 은 진짜 이미지를 축소해서 G_N-1 에 넣어주는 방식인</li>\n</ul>\n<p>이런 실험에선 노이즈로 부터 생성할 때와 진짜 이미지 기반으로 생성할 때의 차이를 볼 수 있는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 95.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEd0lEQVQ4yxXQ61NSCQDG4fMn7exuTa1JqauGpqIJiHIR8OABBA/XQAUUEcVbpXI7NxBI0EwORz2AgI7ZxTbLkiJpatultmbH2lu7zbT7ZW3m/fDO8/EHfPz0+c37f37+8F/+1W/3nvz047s/SkcfXx99LH349/GLo73iu8cvfj2W0tFfpQ+fC6U/9w7f7h3+8vrtm9///gRsHrxTu1Nsg00w4OhwhLgTSeFVGnJvthocbXoDr3+MYw9yJmnIu8W1IlwtzDMPCsdCIqtjIkAAt4tH1vA2S9zCldbKXIjYtwUiOUPotsjczxZV8aEmmc0GoTl9+I5keFasYMl6W4RGtTqwObqYBV4WH6xen7nigr1zFhwd8nhMKOGMRcajuC3ihik/mMIkFCpdDtsjmCWCWuYRK4FYUbdmZWkKeF9YK0TaDrynC3jFU5SR951+RlQVIy0FP6MYqMoj5Qf+M08xxmGYVfCVPTs+WFkBPXXgO1lc7AAKz/ILsaAPdQdC6PGwgDcYci9EMRR3B+a9WAhF8RmcmI7GEJyYDc57jxGf9wXnNDkaAzL5I8i7U6cwtCglzYqORr2dN3dL6t1t7O1vggRNmr7GoWjj9I7Y/wPLPNcEcptVIHdwVjyGOIkIQG4/6ifSQp1OqOwQq9tBi0kX3BgIbSocEyDM79ELLrlGzGjMFtlQjiMqU7e2H5INXILx1JdgG3FkDVGuIzLSL4/75AlERuNgNqxJol1bROc23n4Tbb6FXNgJy9JYV5qQ05hinZCvzLJzUQOwlyZ2Uf4jhPkQrX/or73vqXlEtORDwgc+5j5+4SFWfx+p20NZ+Yhg38vM4/UHeO0+8v0TovLlKgRkcqEVUkfGVVRcQyXg+A2IpJSppClBKhNxmIqrEitKkpSlU2aS7CZX1Im4ai2h8kyCi/g4QG8ingWeWsuEJHy1UqiQtY5MNUdz3VYnj9/GVUCdagXfYq+LZRWTOFvc2QZJBTaLisdpGHX1Afnna4FlsRD8tuXiGZGEcZFTJhWxokmJSl/e1l6u0Fa1chgdXOb8crdtopLDPdOjrzaYRF1SFhLQAIev0vNxidnOcM4wh13Vg6PVGl1DKCGyus7qLeeuBC9YHJWwlukPS8a91b3G8insvHOaaTRXziFy4FVpd+OmM74Bkxk9mTGQWR2VM9JbjutrvVEKXkrCy6nelQ1Netu5vK65llDHVjU3UnAoql9PuoFiKbuYUbkud/aZQJ1W1G9qv4y0Lm0pJ7wCg7bLOgCZL7WPz7AWsz1z19qNRsmACZqa6O3gNjiGBoDnr7PRlFwMVtTVnBUIK+Sq8wY9N5YEYUMNq7GSL6huqDsrFdYvrCrtk8zmpkqBsJZ9sbaNXR28Zv4SzBPhKPRfKbQn+kbLxpCKHs15ZImtHfymx3hy1M+A+051dZ+bwXlDV0+Bqq+PxWj/Tiw7MX5FDLx5u39r10PRgzeoodV1RyY3RqddO/fc1PrQwnVLgh6mkw6Kdmzf9SQzjoVFS3zVTqdHqDX79l3sf+O7cxm/4e/NAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png\"\n        srcset=\"/static/f68470a6fe3e8debb47d08a2175cacf2/12f09/generate_from_different_scales.png 148w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/e4a3f/generate_from_different_scales.png 295w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png 590w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png 745w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>실험 결과를 보면, stage N, 노이즈로 부터 생성한 것은 원본과는 많이 다른 결과를 가져올 수 있고,\nstage N - 1, N - 2 는 원본 형태 유지는 되고, N - 2 같은 경우엔 texture 가 더 원본 같다는 것도 확인 할 수 있습니다.</p>\n<p>하지만, 원본 object 의 배열을 유지한 상태에서 다양성을 보장하기엔 stage N - 1 결과물이 제일 좋다고 판단이 가능하네요.\nAMT perceptual study 결과도 N - 1 stage 일 때가 가장 좋습니다.</p>\n<p>재밌는 점은 stage N - 1 일 때, <strong>confusion 이 47 %</strong> 인데, 사람이 보기에도 정말 헷갈리나 보네요.</p>\n<h4 id=\"sifid\" style=\"position:relative;\"><a href=\"#sifid\" aria-label=\"sifid permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SIFID</h4>\n<p>기존 FID 를 Single Image 에 맞게 변형해서 SIFID metric 을 제안해서 사용합니다.</p>\n<p>각 scale 별 SIFID 를 측정하고 AMT 에서 Survey 한 결과하고 (Paired, UnPaired) correlation 를 측정해서 유의미한 metric 임을 증명하네요.</p>\n<p>요 부분은 논문 6 ~ 7 페이지에 나와있는데, 여길 참고하세용 (<del>귀찮아</del>)</p>\n<h3 id=\"정성적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\" aria-label=\"정성적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정성적인</h3>\n<p>총 5 개의 application 예시를 보여주고 있는데,</p>\n<h4 id=\"single-image-super-resolution-sisr\" style=\"position:relative;\"><a href=\"#single-image-super-resolution-sisr\" aria-label=\"single image super resolution sisr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Super Resolution (SISR)</h4>\n<p>결과 비교를 위해\ninternal method 인 Deep Image Prior (DIP), Zero-Shot Super Resolution (ZSSR)\nexternal method 인 SRGAN, EDSR 와 결과를 비교했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVQY0wExAc7+AKuroL69sri1p7u3qri4rrSypr66q7a0pry8sL27rb26rLi3q76+s7azpb67rba1qbm4rL27rLOypMPDuABvcFR3dlyDfF6OiGl+e2B5d1mNimaJiGV3eFp/fV2Rj2yBgGB8e1+BflySkG14eFh9fF6MiWiHh2SLi3AAi4ptfn5nf3xjgoBnmZZ8goJnf39ii4tvmpl8f39lg4NmkZB0lpV6fHxghIRol5Z4h4hugIBkg4RnrKuRAHNzVpOPb5WNa3t3XYuKcKehe4+KaHd3W5uYd6CZc4aBYXl5XailgpiSbIF+YYeGaqymgpKMaHRzVqOiiADY2dPb2tLd29HQzsa+vrfEw7nV08nT08vMy8Tc2c/c29PPzsfIx77Myr/MysK3t7DPzMLi4NXR0MnT08zVGrJ3rWnFiAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png\"\n        srcset=\"/static/7c177a1b2169c7eb75c0a40183049776/12f09/sisr_result.png 148w,\n/static/7c177a1b2169c7eb75c0a40183049776/e4a3f/sisr_result.png 295w,\n/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png 590w,\n/static/7c177a1b2169c7eb75c0a40183049776/efc66/sisr_result.png 885w,\n/static/7c177a1b2169c7eb75c0a40183049776/c83ae/sisr_result.png 1180w,\n/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png 1543w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>(distortion quality 인 RMSE 가 낮을 수록 좋고, Perceptual Quality 인 NIQE 가 높을 수록 좋음)</p>\n<p>다른 Internal Method 에 비해 RMSE 는 높지만, external method 인 SRGAN 과 NIQE 값은 comparable 합니다.</p>\n<p>1 장의 이미지만 사용해서 이 정도 결과라서 정말 신기하네. 개인적으론 PSNR 같은 다른 metric 하고 모델 들도 넣어줬으면 좋을 것 같네요.</p>\n<h4 id=\"paint-to-image-style-transfer\" style=\"position:relative;\"><a href=\"#paint-to-image-style-transfer\" aria-label=\"paint to image style transfer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Paint-To-Image Style Transfer</h4>\n<p>Paint Image 로 부터 생성하는 style transfer 실험인데, quality 가 꽤괜입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABeUlEQVQY0wFuAZH+AMPP4LPH5K27zLPD2qK53Zmx2Zqz3aS74LLC2q+/1K3A2bHF45W78pC055e35JG48KO85Zy03pu14p644gC+xcy5wsW4taG1vL+dr8iXrcyUrtiZstm0vMC2vLqzvcW1w9OWs9mGqdWKrdmRteags8+dsMyZs9yYstsApZyBloljlYZempB1m5J8kIt6jIuFkpSVpZZzoZBnoJFvpZuClYRidWxSdXFaeHNnl4tymo90lI5+lI+FAJSPaYR+W4V+XYyJZImKXYKDUoOEUIqKWYuIX4aAWIWBVY2HX4aFPXt7MHl6N3R1KIOGVYaGW4WEVoSEVACGjU2CgzuHhD+OlEyOlkJ/ijOEizWJkz+Kjkl9gj+Bg0GJjkyMmDN6ix2CjyGHlC6Pl0WDiziFjDyHjzoAm6BslphekpZck51lm6Fkmp1ZlZxYn6ZjmqFplplhk5pfnaVqmaNYj5hKkppKm6FWn6Rll5ldl51dlp1cp1rL9OUPCigAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png\"\n        srcset=\"/static/3389c01a5021789c0be4714d0ba0749b/12f09/paint_to_image_style_transfer_result.png 148w,\n/static/3389c01a5021789c0be4714d0ba0749b/e4a3f/paint_to_image_style_transfer_result.png 295w,\n/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png 590w,\n/static/3389c01a5021789c0be4714d0ba0749b/efc66/paint_to_image_style_transfer_result.png 885w,\n/static/3389c01a5021789c0be4714d0ba0749b/c83ae/paint_to_image_style_transfer_result.png 1180w,\n/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png 1531w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"editing\" style=\"position:relative;\"><a href=\"#editing\" aria-label=\"editing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Editing</h4>\n<p>원본 이미지에 일부 영역들을 임의의 무언가로 넣으면 이 친구가 얼마나 자연스럽게 만들어 주는지 확인 해 주는 task 인데,\n이것도 꽤 재밌는 결과를 보이네요.</p>\n<p>아래 이미지에서 <em>Content Aware Move</em> 가 포토샵 기능인데, 이 것보다 잘하는 거 같네요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADnklEQVQozwGTA2z8AOHm78zS3rzCzrS8x624xrnCzL3K2aq0wcrR3t7k8Nvg6tvi7sbN1rW9x7O/zbjCzq67y8vV5Nng7uTo8QBxiLFYeaxvkLuBpcqXnaKQgHR9eHtuhqhZf7hlf7FlfalbgbZ2eYeGb2KSkI2HeG59e4FthqlQdK2LoMYAjKTEiqTDp8Tdsa2kpXxam21NkmA7gnFpjq3Ok5KXf5a3f6DFjpejlWxJkmE8hFErjlw1iWNHbHSEsq2sAK6+y8DX5s7U0ohhR25MPI1uXrWSc4RhSIeHiaCLd6q6yKrC1p3B4IqEg2I4JY1rWa2CXHdSN3lybbymkAByammdn5zJsI9xTDZdQDfY2NXo5tqigF6ifVWggWNuZmagoZ+4vbt5amVsT0Po7+3Z0L2RbEifeVG/p48ART5FSDEll3RRVTkutJt2/++479qkxaNwr4lcoIFkMCgwYEUxjmpIUjQozbaL/+2269Sgwp5tp4BUuqOLAI90W4hhPG5LNFg3KKSCVOa/d9evbLOLW6N6T6KBYIdpTYhhPGNBLl88KreUXua+dtOraa6HWJ1zR72kigD46djXx7e+tK61rqq1ravJv7fKvrS9safXyrzu49b15tTf0cS0q6arpqW1rqzHvbS+s6rHvLPn2svw598AjKTGa4+6gZ+/jJeklrDFlp+njKCzfqLIapPFgaDMgJvCdJjFiJ61kpadmKi0l5+nhp20eqHJbpfKnrbZAHCPtVyItYWiu6SAYZ92VI5bN4VZPH5vamiDoXOHomaEqmORv5GQj5VdMZlqRYpWM4leQId2bF18n5qltACnv9SRr8eqx9ugiXV2RCmEUDKfZz6MVjGBfoCol4egt8mau9SkwdSZfmZ0RCmGUDGmbkKKVzJ6cGvBq5YAj4+RtMHFvNLekH9zVy4gtK2s18u6j2VDjG9WooJnjI6QscfTudrwmox/YjgoxsTG0r6mhFs9kXBRwKaPAEg9QVVAN7KTc2VENZF0Xv3y0+/iwrqTaLaIWqaDZDUoLGlaUrGijmtJN62Sd//42efXtLiPYrCDVcCkjQBlU0tsRit4TzJMKBu3kmD404rqw37EmGGoek2ge1paRTp5Ti5nPiRSLRzNp233z4flvXm8j1uhcUS9n4UA2ruexaSFoId6mYB1p4592LuY0rKRu6CHxqiMzbOd2bqawqOIn4l9nYR5sZmF3b+czrCRvqOLyKqO2sm4t/oFHFIZ2LwAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png\"\n        srcset=\"/static/827c27d77bcc338201f45b2bc70727ea/12f09/editing_result.png 148w,\n/static/827c27d77bcc338201f45b2bc70727ea/e4a3f/editing_result.png 295w,\n/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png 590w,\n/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png 739w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"harmonization\" style=\"position:relative;\"><a href=\"#harmonization\" aria-label=\"harmonization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Harmonization</h4>\n<p>Image 2 장을 합쳤을 때 조화롭게 잘 합쳐주는 지를 보는 task 인데, 이것도 꽤 자연스럽게 잘 되는 것 같아요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.72972972972974%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACqklEQVQozwGfAmD9AJSarnuAnH+Ho3V9nH2Go6eup7a6uHJ4k3N5lHyDn3d+mXqDnKuxn6OotnZ7l3d+mnuCo3N7mYuUncvOvgBlb5JETnVSXoJWXn1QX4mJk3qpr6VIUoFPWX1SX4RSXoNca4qpr4ODjKFBS3ZVYoVRW3tUYolten67wJsAeIedTF1yT1trYGFjeYWdcYeWqbS1V2eBUGB4Slp7WWuIXXOXiZqck6CuTV11UF90TFRabnuMZXqUqLWxAHqJlTxHTXeHlF5wiGhzgGZzg5efqlFga1BbY3aJnVtuiWJ0im96io+apj1KU2p3gGd7k15tfmFxgI2UoQBdZ3YcIB81PUU8S2QyP1o7RmB/h5YyP0whJSI6R1g5RGAxPVhRXHR0fY0gKC8qMTI9S2I1QV8vPFl4gJMAd3p6T1NUU1ZZY2ZsY2hyZW56kZWWY2FYU1JNVFZWY2prXGNpaXF2iYqEWFhPT05LWl5gY2psVFxkiJCRADZEZhU3dipFhlddoGZusTpYspKXqPW+gNOZeMuIZcKEYrh1TaprUt2/jOmxgNOUc8mFY8KHXrFjO7yOewBnSlpRLDt3TEuHY2SFf5VyfKKxtZ/dxomefXTerE/wtkLeqUSshEfd1oiwqomXjXDruEfbrVe1jla0mnAAx4da555E8M5pxqlvx5lNnHlOu6Vo05tJzX0quIEqcVk0PTMpNjg0u6xlwJBHyoEimnMuUE9QMjdIaW9qAI5BPJw8EMKOO9meOI5MMTE+VZONbuzAUe2QG6Y9LmU3RSwlQzk+WrGkb+67PdxoGIw4O1Q3RxoYOW1zfwBjWGdVMkCkcU3bo1itbWBcapahoJDvzF3fmTa9UjN/TVNQSmZGUG/KuXPsxVTXei+mST1tUmE+QGJ3f5BRwTAqj/CYuAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png\"\n        srcset=\"/static/261bf4d359af935e3dfea0a8c3c3118d/12f09/harmonization_result.png 148w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/e4a3f/harmonization_result.png 295w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png 590w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png 732w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"single-image-animation\" style=\"position:relative;\"><a href=\"#single-image-animation\" aria-label=\"single image animation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Animation</h4>\n<p>한 장의 이미지를 넣어주면 짧은 video clip 을 만들어 주는 task 인데, 이 것도 자연스럽게 잘 되는 것 같네요</p>\n<p><a href=\"https://www.youtube.com/watch?v=xk8bWLZk4DU&#x26;feature=youtu.be\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">video</a></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>이번에 SinGAN paper review 를 해 보았는데, 이미지 한 장만 사용한다는 점과 Application 들이 정말 좋았던 paper 였어요.</p>\n<p>개인적으론 network 설계나 등등 요소들은 조금 아쉽네요.</p>","excerpt":"TL;DR 이번 포스팅에서는 ICCV 2019 에서 Best Paper Awards 에서 선정된 papers 중에 하나인 SinGAN 을 리뷰해 보겠습니다. 개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서…","tableOfContents":"<ul>\n<li><a href=\"/SinGAN/#tldr\">TL;DR</a></li>\n<li><a href=\"/SinGAN/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/SinGAN/#technical-review\">Technical Review</a></p>\n<ul>\n<li><a href=\"/SinGAN/#multi-scale-architecture\">Multi Scale Architecture</a></li>\n<li><a href=\"/SinGAN/#gan-training\">GAN training</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/SinGAN/#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"/SinGAN/#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\">정량적인</a></li>\n<li><a href=\"/SinGAN/#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\">정성적인</a></li>\n</ul>\n</li>\n<li><a href=\"/SinGAN/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/SinGAN/"},"frontmatter":{"title":"SinGAN - Learning a Generative Model from a Single Natural Image","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["GAN","multi-stage","one-shot"],"update":"Mar 14, 2020"},"timeToRead":8}},"pageContext":{"slug":"/SinGAN/","series":[],"lastmod":"2020-03-14"}},"staticQueryHashes":["2027115977","694178885"]}