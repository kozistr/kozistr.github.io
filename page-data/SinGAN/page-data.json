{"componentChunkName":"component---src-templates-post-tsx","path":"/SinGAN/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 ICCV 2019 에서 <a href=\"https://syncedreview.com/2019/10/29/iccv-2019-best-papers-announced/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Best Paper Awards</a> 에서 선정된 papers 중에 하나인 <strong>SinGAN</strong> 을 리뷰해 보겠습니다.</p>\n<p>개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서 저도 간략한 overview 와 technical review 를 해 보려고 합니다.</p>\n<p>소개 전에 간단하게 SinGAN 으로 뭘 할 수 있는지 보면, <strong>단 한 장의 이미지로 realistic 한 image manipulation 들을 생성</strong>할 수 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVQY0wExAc7+AMvT28LN2MDJztfb3Nbi6srZ5dDb4dHa3+bu987W2rfAw8rT2szX4LrEyMbR2NDe6djk7Njf4s7Z38bR1ABieHhadXtncW2Li4RUdHZEZ2xjbWdPW1dui5h6hYBQV09ifH9jfoFhaV9LYmBNaGlffINobGRGWFQ9QDQAnIlkkHlQm4BWybqjmIlljnpPpIlhnIVejnlQm4BapItmlHpQmnlPpYZfl4RdknhPnINZpYVemYJanYNbAPq3avy5X//KZfrbpvS0aP/DYv7Ga/y9Zv24Xfu7YP3DbP2tVv2wWfu3YP2+a/isVf7AZfi2X/67YP/HawCiek+mdDy2ejnUs5Kgd02cajWjc0GkdkaSZjerdT29hkulbziqcTmvdT66hE+WaTmmczyuekC0fkK/gkSJLa65PFHxowAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png\"\n        srcset=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/12f09/teaser.png 148w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/e4a3f/teaser.png 295w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png 590w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/efc66/teaser.png 885w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/c83ae/teaser.png 1180w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png 1493w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACbUlEQVQozwFiAp39APb4+Nji5r7Q1s3Z3e/u7L/Q+52164qo27LR/9zg4tfTxcvJv+Hi3b7K1ae0v9re4f///8zJ0ZyRsc7D5gC3uryVoqCyxMmhtrR/jHlziqZ6l9NMkPZRkOd6kpi3sILUyaLNy62Ij5BkdYDa4ur///+fmLNLNnx4Z6EAlpuPUF4wb31KZHI3XWw7vLaJspl2lJqhjH2BbHlXaX9Lm5xttruNmoJ1eEtJ5eLh////jISXSTVzZ1ePAL7GvHmNbYOXc3GLbXiPfYySq3x7lnx3j4yHpZygj5yifJeYd6iqi218fFZeZbbAycfX3np1k2tWnl1PhgDT2dustbqVoayDmqibr7t8k8hzi9hUhPlFgP+Roaq7rYbbz6zZ07hNd5dEXHBjanExZotLTmxiSolqWpIAqrGgWm1GZHVQZnNOc4FhtLKGqYpjhYudcHaRa3ZaWVMspqN3u8GVYXqJZl1Yk31tZ4CRVFyCWkaFSz9xAMLMu3eOZHmOaW6JYn+Wc5udo3dve3Zqbo5+hpubg5CPaoeHYaKkgHaBiXh3d4iBe4iWnVVYdlQ8f3hmogDa3d+9xMiuub+ouMG2xMqCk9BthtdWgvNMhP+Vpq7DupXXy67V0LlTeZdEXXRicHs8bpBnaopgTo1WSXoAp6yhZHNIaXlQb31UgIxurq+NnoVufIuwYXKkaXhhXGM0sKp9wsWaWHaIWVdWgnJnWXiLRkxvSzJ3bFmbAMTKt3yLUoqXYIKRWYqYa6yqm5qJeZaCdKCIfpWYfIaNZ5OUbaqrhoaPkoyDfqKUipKan4eMqmxfi3hwkRQkVYlooc/WAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png\"\n        srcset=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/12f09/manipulation.png 148w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/e4a3f/manipulation.png 295w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png 590w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/efc66/manipulation.png 885w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png 964w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1905.01164.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>official implementation : <a href=\"https://github.com/tamarott/SinGAN\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>기존 GAN 들을 대부분의 연구들을 보면 얼굴, 침실, 풍경 등 한 가지 종류에 focus 한 게 대부분이고, 주로 많은 데이터를 요구했습니다.</p>\n<p>다양한 종류의 object 를 생성하는 것은 여전히 잘 못하고 있고, 이런 문제를 해결할려고 conditional 하게 생성을 하거나 (e.g. cGAN), task 를 특정하는 등의 방법으로 문제를 해결하려 했습니다.</p>\n<p>왜냐면 이전 방법들로는 적은 수의 데이터와 여러 종류의 데이터의 distribution 을 잘 학습하기엔 엄청 어려웠어요</p>\n<p>그럼 이런 문제들을 어떻게 하면 해결할 수 있을까에서,</p>\n<blockquote>\n<p>'단 1 장'의 이미지로 GAN 을 훈련할 수 있을까??</p>\n</blockquote>\n<p>이런 이번 논문인 <strong>SinGAN</strong> 이란 concept 이 나오게 됐습니다. (멋지죠?)</p>\n<p>물론 이전에 이런 노력을 안한건 아니에요. 정확히 논문 이름들은 기억이 안나는데, 대부분이 input 에 대해서 conditional 한 method 를 사용하고 있었습니다.</p>\n<p>또한 이전에 Unconditional Single Image GAN 이라고 하면 Texture Generation 이란 task 로 유일하게 문제를 풀고 있었는데, 이 task 의 한계는\ntexture image 에 대해선 결과가 reasonable 한데, non-texture image 에 대해서는 별로 였어요.</p>\n<p>하지만 이번에 소개할 논문에서는 </p>\n<ul>\n<li>unconditional 하게, noise 로 부터 image 생성</li>\n<li>general purpose 로 natural image target (non-texture) 에도 적용 가능한 방법 제안</li>\n</ul>\n<p>합니다.</p>\n<p>물론 결과는 이전 method 들 보다 훨씬 general 하고 결과도 outperform 합니다!</p>\n<h2 id=\"technical-review\" style=\"position:relative;\"><a href=\"#technical-review\" aria-label=\"technical review permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Technical Review</h2>\n<p><strong>SinGAN</strong> 에 소개된 novelty 를 1 가지로 요약 해 보면 아래와 같아요</p>\n<p><strong>Multi-Scale Architecture (# 2.1)</strong></p>\n<p>완전 새로운 concept 는 아니고, multi-scale architecture 에 대해서는 이전에 LAPGAN 이란 GAN 에서 한 번 비슷하게 소개가 되었는데,\n궁금하시면 한번 봐도 좋을 것 같습니다.</p>\n<h3 id=\"multi-scale-architecture\" style=\"position:relative;\"><a href=\"#multi-scale-architecture\" aria-label=\"multi scale architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi Scale Architecture</h3>\n<p>SinGAN 의 ultimate goal 이라고 하면, single image 의 internal distribution 을 잘 배우는 unconditional generative model 를 만드는 겁니다.</p>\n<p>이런 것을 하려면 다음과 같은 것들을 잘 해야 할텐데,</p>\n<ul>\n<li>\n<p>many different scales 로 복잡한 image structure 의 distribution 을 capture 하기</p>\n<ul>\n<li>global properties : 이미지 내 큰 objects 들의 모양과 배열 e.g.) 하늘 위치, 땅 위치</li>\n<li>local properties : global properties 의 details </li>\n</ul>\n</li>\n</ul>\n<p>그래서 multi-scale architecture 를 선택했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB5klEQVQozyVRSW/TQBj1P+PPcOOABIdKnOiBSggqISGhilYiEKmlUpGglLpJWByi0mx1m5DEzmbSJJ7xFo93xx7H4XP7Du/wLW++94YpFot7ufzn05JM/KWpu2Ir4yU9OGYPWG6ZpH96/3YOT4ZzNY5isY8VTFoS+sB+564E5n3u3cOnW/c3X+7/qkaz8YL9GE1HE1V/9OLVxutdw/Z2P3158Gz77LxuE6dQ6omCdsJVHj9/kjv+xvi+3x8OTovsSBqHvj8biI5FkiQR+4LQ64RheDMZt1u8rqlxTMfSDGGdmIY8n9rEZKC9Xq/TdQbHdacytmwnXQGSrJ7edTJQSrGi6LoRxeAgTZKUAb1Go/GVLWDiOmiml9lIVwzbOSpwXhjdrVWuu3xvGPh+uSLOb8wffHdzbytf4BhV0yxinnG/ueZVbBOlxkFglutBxQuC9e3L1Uu+3RNoTOvNEUJ285rPHe5w1QsGY0xpdmFC4yAMEUJBEEbLyPM8cB5FcGMMEmADSEbYc732aPKz9uai08yWYQgmZnOZaBoqsz5ZYN0Qx1KmuFoBLyyrOxiFYVCu/MXILp2X3+bvHZX2GdC+DWYFyemaNux2FEWJKYXwICEQBYYfcT3XXBjtVkeSJkie12sNVVH+A4CD11KWlOH2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png\"\n        srcset=\"/static/7c27c9c01342508a98366734bfe86dd2/12f09/multi-scale-architecture.png 148w,\n/static/7c27c9c01342508a98366734bfe86dd2/e4a3f/multi-scale-architecture.png 295w,\n/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png 590w,\n/static/7c27c9c01342508a98366734bfe86dd2/efc66/multi-scale-architecture.png 885w,\n/static/7c27c9c01342508a98366734bfe86dd2/c83ae/multi-scale-architecture.png 1180w,\n/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png 1320w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>위 그림에서 x<em>0 가 original training image 이고, x</em>1 ~ x<em>N 가 x</em>0 에서 r 배 (r > 1) 씩 down-sampled image 입니다.</p>\n<p>각 scale 에서...</p>\n<h4 id=\"generator\" style=\"position:relative;\"><a href=\"#generator\" aria-label=\"generator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generator</h4>\n<p>noise (z<em>n) 와 이전 단계에서 생성된 image (~x</em>n-1) 를 받아서 image (~x_n) 을 만듭니다.</p>\n<h4 id=\"discriminator\" style=\"position:relative;\"><a href=\"#discriminator\" aria-label=\"discriminator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Discriminator</h4>\n<p>real image 와 (x<em>n) fake image (~x</em>n) 를 구분.</p>\n<p>하나 차이(?)점이 있다면, 맨 아래 scale stage 에서는 only noise (z_N) 를 사용해서 image 를 생성합니다.</p>\n<p>논문에서 coarse-to-fine fashion 이라고 소개를 하는데, 좀 쉽게 설명 해 보면,</p>\n<p>아래 단계에서는 down-sampled image 를 학습하니, 상대적으로 detail 보단 global 한 feature 에 집중을 하면서 학습을 하고,\n위 단계일 수록 fine feature 에 더욱 집중하게 됩니다. 동일한 receptive field 에 생성하는 image scale 이 다르니,\n위 그림에 <strong>Effective Patch Size</strong> 가 달라지면서 coarse-to-fine fashion 으로 학습이 된다 입니다.</p>\n<h4 id=\"single-scale-generation\" style=\"position:relative;\"><a href=\"#single-scale-generation\" aria-label=\"single scale generation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Scale Generation</h4>\n<p>각 G_N 부분에 해당되는 block 인데, 구조는 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABb0lEQVQY031QS0/CYBDs3zXh5MWDJl40gBjlRCCIIHogQiIYNWgwIEgEwqsVS6GUtpRHgQLtVyiVPvBDbh7czGF3Mzs7u8j639A0TTeMP03DNEVRhAmCYVg6nUZR1DRNWH+rqiQIYDJZiDNVBnyHnfB9bTFXAVAkUQGSaRpQkaFpyEecTqfFYrE5Tis1vEpQnRYlN2syiY+/qjpHpR7uQ4HL0JU/8xhbj7oqS0J1huMy2exgKiNwZ+I57g3Hds88e+7QR6GstvFRrUJkUzxWCng9DpvVbrPd+n0zAuPKearNHHpudo7OXU85xPg96S1fOLA7rC43/olBkkzVFZpY0o2g78J6vIlw0A+NkLn3Id93+gP7jpPru+hmGLqXgTTsdacTYdplh1hJadc1llTpRjERT0Yjr9EImnwxOIpHi9pS0VYq3+sauoZsHwgAmIkimM8XkjhuNwckwbcaEALdmjIUBEx4khAYSl+tIH/73R8OVmXlpHKtjQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png\"\n        srcset=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/12f09/single-scale-generation.png 148w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/e4a3f/single-scale-generation.png 295w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png 590w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png 733w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>% 이전 stage 에서 up-sampled image : x_n+1</p>\n<ol>\n<li>z<em>n + x</em>n+1 가 conv 연산을 통과</li>\n<li>x_n+1 가 residual 하게 마지막에 연결</li>\n</ol>\n<p>가 간단한 구조인데, conv block 부분을 더 자세하게 설명하면,</p>\n<p><code class=\"language-text\">Conv (3x3) - BatchNorm - LeakyReLU</code> </p>\n<p>이 convention 으로 5 층을 쌓았네요.</p>\n<p>처음 (coarsest scale) 엔 32 kernels / block 으로 시작을 하고 4 scales 마다 kernel 을 2 배 늘려 주었다고 합니다.</p>\n<p>이렇게 해 준 이유는 (상대적으로 light 한 구조여서), </p>\n<p>주로 generator 의 capacity 가 커지면 training image 를 외어버리는 경우가 생기는데, 이를 방지하려고 light 하게 설계를 한 것 같습니다.</p>\n<p>또한 fully-convolutional 하게 설계를 한 이유는, arbitrary image size 에도 training / inference 가 가능하게끔 하려고 라고 설명을 합니다.</p>\n<h3 id=\"gan-training\" style=\"position:relative;\"><a href=\"#gan-training\" aria-label=\"gan training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN training</h3>\n<p>이 부분이 이제 GAN을 학습하는데 있어서 제일 중요한 부분인데, 딱히 특별한 부분은 없습니다.</p>\n<h4 id=\"loss-function\" style=\"position:relative;\"><a href=\"#loss-function\" aria-label=\"loss function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>loss function</h4>\n<p>loss 는 adversarial loss + reconstruction loss 로 이뤄졌고</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">total_loss <span class=\"token operator\">=</span> adv_loss <span class=\"token operator\">+</span> alpha <span class=\"token operator\">*</span> rec_loss</code></pre></div>\n<h5 id=\"adversarial-loss\" style=\"position:relative;\"><a href=\"#adversarial-loss\" aria-label=\"adversarial loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial loss</h5>\n<p>WGAN-GP loss 사용 했고. 논문에 보면, 다른 texture single image GAN 과 다르게, patch 별이 아닌 전체 이미지에 대한\nloss 를 사용했더니 네트워크가 boundary conditions (SM) 를 학습할 수 있었다고 합니다.</p>\n<h4 id=\"reconstruction-loss\" style=\"position:relative;\"><a href=\"#reconstruction-loss\" aria-label=\"reconstruction loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reconstruction loss</h4>\n<p>l2 loss 를 사용. 각 stage 에서의 rec loss 를 다음과 같이 정의가 가능한데,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">rec_loss_n <span class=\"token operator\">=</span> l2_loss<span class=\"token punctuation\">(</span>G_n<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">~</span>x_n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x_n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>또한 ~x<em>n 의 역할이 하나 더 있는데, stage n 에서의 noise z</em>n 에 대한 std 값을 결정하는데 쓰여요.\n~x<em>n+1 하고 x</em>n 의 RMSE 값을 구해서 각 scale 에 얼마만큼 더해야 하는지를 알려주는 정도로 사용된다고 합니다.</p>\n<p>또 중요한 부분은 noise 를 넣어줄 때, 첫 단계에만 fixed noise 로 넣어주고 다른 단계에서는 noise 를 따로 만들어 주지 않았는데,\nimage pixel difference 를 줄이려는 것에 focus 를 하려고 이렇게 했다고 캅니다.</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"정량적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\" aria-label=\"정량적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정량적인</h3>\n<p>총 2 가지의 정략적인 방법을 사용했는데,</p>\n<ol>\n<li>Amazon Mechanical Turk (AMT) </li>\n<li>Single Image Frechet Inception Distance (SIFID)</li>\n</ol>\n<p>요즘 GAN paper 들에서 자주 사용하는 metric 들입니다.</p>\n<h4 id=\"amt\" style=\"position:relative;\"><a href=\"#amt\" aria-label=\"amt permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AMT</h4>\n<p>AMT 는 사람들에게 직접 답을 하게 해서 결과를 매기는 서비스입니다.\n여기서는 해당 이미지가 Fake 인지 Real 인지를 구별하게 하는 투표 방식을 사용했습니다.</p>\n<p>여기선 2 가지 방식으로 조사를 하였는데,</p>\n<ol>\n<li>실제 이미지와 SinGAN 이 생성한 이미지를 보여주고 어느 쪽이 가짜인지 맞히는 Paired 실험</li>\n<li>둘 중 하나만 보여주고 얼마나 헷갈렸는지를 물어보는 Unpaired 실험</li>\n</ol>\n<p>실험 조건은, 각 실험 당 1 초의 시간에 1 명당 50 장의 image 를 보여주었답니다.</p>\n<p>생성한 이미지는 stage N, N - 1 에서 생성한 이미지들을 주었다는데, (논문에선 N -2 까지)</p>\n<ul>\n<li>stage N 은 noise 로 부터 생성한 이미지고</li>\n<li>stage N - 1 은 진짜 이미지를 축소해서 G_N-1 에 넣어주는 방식인</li>\n</ul>\n<p>이런 실험에선 노이즈로 부터 생성할 때와 진짜 이미지 기반으로 생성할 때의 차이를 볼 수 있는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 95.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEeElEQVQ4yxWOaVMaBwBA9z+lk1QNBjUqRq0aEcR4QLiWZRdEOcR4ERQUUbQcyy4scsshcolIUEyTZtpJJhrRJNPp9E7SXJ2203qAGmpn3of35c08oPSpdFw4Pzor/f7xnx9/fXN4UjwunP3PWen1+79/fvXh7ce/Tk7Pj06KJ2eld3/++9OrD7+8flconJx/KgHHxfNBfBPU2QV6F38xwTXnxMR9KfGQq8F5GiNf7+EYEqB1S7r0iK8P8iYNPJ0DtqyLprSOWBw4KpYQc7KRQW7vIzM1OMOYY6GbsO0rqpDTwihrZzXcGpnqRe/DjkcMuYrButbDq+2SDYD4htafAi4GXH5iTCXVzN+dty1oUf28E7N5cRSfXZyVuI3CEI54CeWS12TCZi3opNEyNWeeMmDT/lU3cFY4zBPNz9CreVtVHiPtWq4cOOpeuFoPMNJLR/U+Vp63lj23VTx3Ne9byg5w0gF+dd9WsWv+7GWwBygWjhO+mcjS3bhXeyFRpyrlV28ELmQy4dVE3VOrjtGEazQTnI45J9a8mqRHHXONJzH+44wZOCqc317YvClVdyqUdOlA+xjaa/maa37YLlXRpVLa6EK7JkY1bHOt31BHLB39SIditEfnZ6v1i/4Q8Nub9yI0Q+Uz6eymTnZ995AccT4YdNzvVQx3cpuYwg5IOwfb1mWuB8yJOTZM5fXT+pRymMhpAllgd+dJEB/yolKnWea0yD2YLOhUrnomgnZZwiZYt3E38O6srSvtkYfs8rBdEbArfXaFywSuhWeAH757/q2ldRet3bHWPbXUPDaS94gv8kv0p6aaZzhlx3r9ieX6Dn4j76Ttmqr38No9rPoZSs5jFd/HuMDLF/trEUkmhmRi8AWpCC8TF28lFRtRKBtH7sUEmSi4sSrYTinTEX42DmfjUCYq8JvA7Mo8sLv3xOSjyYbbFTKuUsmTK+gGgubfAlX6PpDfJ1eAyqG+GVNrICfUOxhCmCkd4Gq14g5qk06nBj7+8dbq6ejqudxIId3qJjc1kCQwbTl9G4TIbW2VTHZtSzOZ3dO4vAbeUde0tVRy+ZQBCYfNavWGhoHi6SEaoElGr47pro9pqqQjlbCoyZXoGhwvk09U6QnK4B0SX0DBfL0qAwmRVcwR9Xd1FAi5ZsSFF/FxKjuysg5HNkSRtDiSRiIpWWpr3L8qcIb5y0kokIRCSXE6pw7EhEtBnjcKBRKQL9h/b9t8ER8F7sGoA5qdRzQz0Owck1jpDW9LjE7O9IzI8KVYN8/C/F3hnMQaZGp18MJCv42QsllUw+w0UDg9dCdZTCblRk0No7OWzWwYG2cEMhxEfKOZUku/2dBUV40ImkMZ4bi2saWhrpPaSG/7gkGt94VUQPHscNHdxhVd4okvD09XTqI1IkkrEaEJFZdEynKV6Rp/oOw2p97q7R7Wfc5BrkyhVYiyvIdVbrIJgdPT43h6yOkD7W7Qs4ysRPs9y4Pp3ITDy7PYud6w0B+EnR4ktanyhSETxnYHoUBYtOTip7Jz/wEp9Hyn7n14zwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png\"\n        srcset=\"/static/f68470a6fe3e8debb47d08a2175cacf2/12f09/generate_from_different_scales.png 148w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/e4a3f/generate_from_different_scales.png 295w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png 590w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png 745w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>실험 결과를 보면, stage N, 노이즈로 부터 생성한 것은 원본과는 많이 다른 결과를 가져올 수 있고,\nstage N - 1, N - 2 는 원본 형태 유지는 되고, N - 2 같은 경우엔 texture 가 더 원본 같다는 것도 확인 할 수 있습니다.</p>\n<p>하지만, 원본 object 의 배열을 유지한 상태에서 다양성을 보장하기엔 stage N - 1 결과물이 제일 좋다고 판단이 가능하네요.\nAMT perceptual study 결과도 N - 1 stage 일 때가 가장 좋습니다.</p>\n<p>재밌는 점은 stage N - 1 일 때, <strong>confusion 이 47 %</strong> 인데, 사람이 보기에도 정말 헷갈리나 보네요.</p>\n<h4 id=\"sifid\" style=\"position:relative;\"><a href=\"#sifid\" aria-label=\"sifid permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SIFID</h4>\n<p>기존 FID 를 Single Image 에 맞게 변형해서 SIFID metric 을 제안해서 사용합니다.</p>\n<p>각 scale 별 SIFID 를 측정하고 AMT 에서 Survey 한 결과하고 (Paired, UnPaired) correlation 를 측정해서 유의미한 metric 임을 증명하네요.</p>\n<p>요 부분은 논문 6 ~ 7 페이지에 나와있는데, 여길 참고하세용 (<del>귀찮아</del>)</p>\n<h3 id=\"정성적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\" aria-label=\"정성적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정성적인</h3>\n<p>총 5 개의 application 예시를 보여주고 있는데,</p>\n<h4 id=\"single-image-super-resolution-sisr\" style=\"position:relative;\"><a href=\"#single-image-super-resolution-sisr\" aria-label=\"single image super resolution sisr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Super Resolution (SISR)</h4>\n<p>결과 비교를 위해\ninternal method 인 Deep Image Prior (DIP), Zero-Shot Super Resolution (ZSSR)\nexternal method 인 SRGAN, EDSR 와 결과를 비교했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVQY0wExAc7+AKanm7++s7ezpr25rLe2q7a1qrq2p7q4qre3rL69sL26q7e2qb+/tbSypL+8rbW0qLm4rby6q7a1prq6rgBzc1V0dVuAelyPiWqBfWB6eVyGg2CRj2x3eFl+e16Oi2iHhWR7e19+e1uSkG19fVx6eV2HhWOPjmt/f2AAiolrg4NqfHpigH9llZJ4jItxe3tehodrm5t9hINqgYBjiopunJt/fX1igYFlkpJ1kJB1fH1ig4RnnJyAAHJyVIyJa5mSb4F9YIB+ZaqkgZWOant5Xo6NbqegeouGZHd2Wp+efaCZc4eDZH19Yaung5eSbHt5W4yMcADY2dPd3Nbd29LY1s+/v7nEw7zSz8XZ2NHMzMXa187h3tXT0szLysLKyL7S0Me6urPIxr7j39XY2NDQz8iCCLIw0jJXnQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png\"\n        srcset=\"/static/7c177a1b2169c7eb75c0a40183049776/12f09/sisr_result.png 148w,\n/static/7c177a1b2169c7eb75c0a40183049776/e4a3f/sisr_result.png 295w,\n/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png 590w,\n/static/7c177a1b2169c7eb75c0a40183049776/efc66/sisr_result.png 885w,\n/static/7c177a1b2169c7eb75c0a40183049776/c83ae/sisr_result.png 1180w,\n/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png 1543w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>(distortion quality 인 RMSE 가 낮을 수록 좋고, Perceptual Quality 인 NIQE 가 높을 수록 좋음)</p>\n<p>다른 Internal Method 에 비해 RMSE 는 높지만, external method 인 SRGAN 과 NIQE 값은 comparable 합니다.</p>\n<p>1 장의 이미지만 사용해서 이 정도 결과라서 정말 신기하네. 개인적으론 PSNR 같은 다른 metric 하고 모델 들도 넣어줬으면 좋을 것 같네요.</p>\n<h4 id=\"paint-to-image-style-transfer\" style=\"position:relative;\"><a href=\"#paint-to-image-style-transfer\" aria-label=\"paint to image style transfer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Paint-To-Image Style Transfer</h4>\n<p>Paint Image 로 부터 생성하는 style transfer 실험인데, quality 가 꽤괜입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABeUlEQVQY0wFuAZH+ANvh67/M4bbE3cTR5c/b77fG3rPA1sjV68vX6brG2rzI2r/N4sHX96vD47jG2qvE58jZ9LrH4LPB2bzK4gDByNC6wcS4t6a4urGis8uYq8aVr9eVr9mxu8W3u7W0vsO1wc2fudmFptGLrdmOseGfttWercSbs9uZsdoAq6OMlIdhlIdfl4tqm5N+kYl0i4qBjY+Popd7n45ino9sopd6mopseWtKcm9XdG5dj4Rvmo5wk4t6ko2BAJuXcYR/W4Z/XoqHYoyNYYKEU4SEUIeIVY6LYYaAWIaCVouFXYuISHt9LXt8OnR2J4GES4eJXYWFVYSFUwCOlFd/gTiHhD+Jj0eSmEiAijGDijaHkTmMkUp+gz6Ag0KFiUiPmT17jBuAjSKFkiaPmEWFiziDizyHjjoAoqd1lZhflplekJpinaRomp1al55bnKNfnqVslZlilZphmaJon6dhjphLlJtMl59So6dnlplemp1elZ5dKiHPgJ8LZm8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png\"\n        srcset=\"/static/3389c01a5021789c0be4714d0ba0749b/12f09/paint_to_image_style_transfer_result.png 148w,\n/static/3389c01a5021789c0be4714d0ba0749b/e4a3f/paint_to_image_style_transfer_result.png 295w,\n/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png 590w,\n/static/3389c01a5021789c0be4714d0ba0749b/efc66/paint_to_image_style_transfer_result.png 885w,\n/static/3389c01a5021789c0be4714d0ba0749b/c83ae/paint_to_image_style_transfer_result.png 1180w,\n/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png 1531w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"editing\" style=\"position:relative;\"><a href=\"#editing\" aria-label=\"editing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Editing</h4>\n<p>원본 이미지에 일부 영역들을 임의의 무언가로 넣으면 이 친구가 얼마나 자연스럽게 만들어 주는지 확인 해 주는 task 인데,\n이것도 꽤 재밌는 결과를 보이네요.</p>\n<p>아래 이미지에서 <em>Content Aware Move</em> 가 포토샵 기능인데, 이 것보다 잘하는 거 같네요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADnklEQVQozwGTA2z8AOrt9Nba4sPHz7m/yLG7x73H0MPP3LC4w9TZ4ujt9eXo7+bq883S2rnCzLjDz7zH0rS/zdPb5+Tp8+vu9ABzibJXeKtsjrqApc2WoKqPgnl8fIJtiK1ZfrllgbVmfqpbgLZ2eYiGc2iSlZeHfHZ7foluirNRdrKLockAiKDBhqLCo8Hcr6+qpn1cmm1OkGA9gXNuiqvOjo+Ye5K0fJ7GjJOflWpGkmQ+hVMujV04h2RLZ3OGrquuAKy8y7rS4czT0oplS3NQQI9uW7KNbYRhSIqLjqGMeKe4x6a+0pu924mBfmY8J4xnUqp+V3lTN3lzb72mkAB2bm6jp6XLs5JyTDVbPTTU1dPn5tugfl2ge1SggWNzbW2kqKa5wcJ7bWhnST/l6+3ZzruOaUeeeVG/p48ART5FSDIom3hUVjousZh1//C779uow6Bur4lcn4JlLycwYUc0k29NUzUqybKJ/+676tWjwJxsqIFVu6OLAIZrVIFbNmxJMVMxIqaEU+nCeNqxbLSMWaJ3S598W3xgRoJaNmE+Klo4JbqWXenCd9ataq6GVptvQruhhwD66dTZxbG6r6eyqaSyqKTJvbHKu628raHYyLfu4NH45tDgzr2wpZ+pop+yqabHuq6+sKTGuK3m1sXw5tsAkqvNcpXAiKbFkZ6tmLTKl6Otj6W6gqbMcJnKiafSh6LJfKDMjaW8lZ2mmqy7mKOuiKK6f6bOdp7Qo7zdAG+Ns1uGtIOfuKB+YZ55W45ePYZdQ3tycmWCo26GpGSCqWGPvZCOjpReNphtSopaOoliR4N4c1p8opaktgCju9CMqsSqxtmhiHR5RyuGUjOfZz6NWDOEgIGnl4ibssWXt9CkvtCYemF3RyyIUjKlbUKMWTN7cW3Bq5gAlJebtcXLutThk4F1WC0fraKh08KvjGJBiG9ZooRpkpebssrZt9rxm4x/YTUlvrm5zracgVg7jW5TwKaPAEs/RFtHQLaafGlJOoxvW/vy1u7jxreQZrWIWaWDZDgrL21hW7SpmG9NO6eKc//53ubXtrWMYLCDVcCljQBdTEdkPyV4TS9JJRi9mGb8143sxoDHm2Opek2felpQPTV0SSlpPCFPKhrSrXL50ojowHu/k16gcUS8n4QA1raXwp58l31vkXZqpIl117iQ0a6IuZp/waKEya2V1rSTvpx+lXxwlXturpN827uTy6qIuZ2Dw6OF2MWzpIEFTr+pmaIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png\"\n        srcset=\"/static/827c27d77bcc338201f45b2bc70727ea/12f09/editing_result.png 148w,\n/static/827c27d77bcc338201f45b2bc70727ea/e4a3f/editing_result.png 295w,\n/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png 590w,\n/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png 739w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"harmonization\" style=\"position:relative;\"><a href=\"#harmonization\" aria-label=\"harmonization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Harmonization</h4>\n<p>Image 2 장을 합쳤을 때 조화롭게 잘 합쳐주는 지를 보는 task 인데, 이것도 꽤 자연스럽게 잘 되는 것 같아요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.72972972972974%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACqklEQVQozwGfAmD9ANPU2sbJ07m8x56isb/D0M7T3czO06WnraKksKeqtqSms6GksqmvtcvN0r/Byp2gq6qsvKGksrG1wt3g5QBpc5Y/SXFOW35SWnhNXoh9iXevtZ9KVYZJVHhNW4FPXH9WZoqiqX6MlaE9SHZRXYFMV3ZSYIVldICzuI0AgpCkTVxxU2ByXF5hfYaabIKXqbS0YHCHT151Tl6AWGmFXnWYgJObmqewUWF3U2J2TVVdbnmFZnuXm6qrAISSnzlDSnaFkGF1jmRwfmNxgJObpltqdkhSWHqNnlpuimFziGp1hZGcp0JPWGRvdm2BmVpqf2FwfoGKlwBmcHweIiEvNjk7SV8zP1c2Qll9hJI5RVQbIB0zP0w3RF0vO1VKV251f44jLTYiKSg4Rlo0P1wtO1ZqdIgAgIKCVFpeVltiZ2tybG97Z29+kpeZdW9hYFtUXVpYa3BuZ2ptaG1ykJGIaWRYXFVRYmJhbHFwXmFogIWHADhBZAwrayQ9gFBTl2RrsDZVtX6LrP7JhtufetOMZMqFW8N7S7FoSNy9ivO9hdmadtGIX8qIWLtqN7yCagB7W2RgNDuGV0yVbWGRgot/gZmqrZveyYSce3HVpU/rtUTVpkegfETTzIC1r4WQh23htEjVq1asjmCki2QAxIZf45Y+8s1kya5uxJVKlnNJrppl16FL1YEmunola1EyNiwqKSwxrqJjx5hI1IEdmWsqS0ZILDFGV15fAIlFRJAvD7p+NNylO5tRMDE5VYSFcOjAWPCbILBAK3A6RzYrRzE3Waieceu/ReFxGZY3OV46SSAcPVxgdABqYnBPNEWaaEzYo1u4dF9ja5WUmpfuy2HgojzCWDGFTVBWT2dCTG2/snTuylnZhDOsTTpyU15GRmZqcYU0iDiCacRfAAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png\"\n        srcset=\"/static/261bf4d359af935e3dfea0a8c3c3118d/12f09/harmonization_result.png 148w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/e4a3f/harmonization_result.png 295w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png 590w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png 732w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"single-image-animation\" style=\"position:relative;\"><a href=\"#single-image-animation\" aria-label=\"single image animation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Animation</h4>\n<p>한 장의 이미지를 넣어주면 짧은 video clip 을 만들어 주는 task 인데, 이 것도 자연스럽게 잘 되는 것 같네요</p>\n<p><a href=\"https://www.youtube.com/watch?v=xk8bWLZk4DU&#x26;feature=youtu.be\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">video</a></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>이번에 SinGAN paper review 를 해 보았는데, 이미지 한 장만 사용한다는 점과 Application 들이 정말 좋았던 paper 였어요.</p>\n<p>개인적으론 network 설계나 등등 요소들은 조금 아쉽네요.</p>","excerpt":"TL;DR 이번 포스팅에서는 ICCV 2019 에서 Best Paper Awards 에서 선정된 papers 중에 하나인 SinGAN 을 리뷰해 보겠습니다. 개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서…","tableOfContents":"<ul>\n<li><a href=\"/SinGAN/#tldr\">TL;DR</a></li>\n<li><a href=\"/SinGAN/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/SinGAN/#technical-review\">Technical Review</a></p>\n<ul>\n<li><a href=\"/SinGAN/#multi-scale-architecture\">Multi Scale Architecture</a></li>\n<li><a href=\"/SinGAN/#gan-training\">GAN training</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/SinGAN/#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"/SinGAN/#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\">정량적인</a></li>\n<li><a href=\"/SinGAN/#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\">정성적인</a></li>\n</ul>\n</li>\n<li><a href=\"/SinGAN/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/SinGAN/"},"frontmatter":{"title":"StarGAN-v2 - Diverse Image Synthesis for Multiple Domains review","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["GAN","one-shot"],"update":"Mar 14, 2020"},"timeToRead":8}},"pageContext":{"slug":"/SinGAN/","series":[],"lastmod":"2020-03-14"}},"staticQueryHashes":["2027115977","694178885"]}