{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/SinGAN/",
    "result": {"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 ICCV 2019 에서 <a href=\"https://syncedreview.com/2019/10/29/iccv-2019-best-papers-announced/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Best Paper Awards</a> 에서 선정된 papers 중에 하나인 <strong>SinGAN</strong> 을 리뷰해 보겠습니다.</p>\n<p>개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서 저도 간략한 overview 와 technical review 를 해 보려고 합니다.</p>\n<p>소개 전에 간단하게 SinGAN 으로 뭘 할 수 있는지 보면, <strong>단 한 장의 이미지로 realistic 한 image manipulation 들을 생성</strong>할 수 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVQY0wExAc7+AM/X38PO2MPM0tXa29vl7czb5tPf5tLb3+jv+NPa37nCxcrT2s7a4r3HzMfR19Ph7Njl7tzj5tDa4MvV2gBnfH5WcXZpdXKHhX1de30/YmdkcGxPWFJqh5R9iYdRV05hen1ifoFibWRLYF1NaWtbeX9rcWpHV1I+RDkAnYxpjnhQmH5UxrWenI9tiXhNoYhgnIVgjHhQmX9aoopmk3tSmHhPo4Rel4RfkHdQmoJZo4RdmYFbmYFaAPi3bPm2Xf/HY/vbpPK1bf7AYP3Favy+aPu2XPm5X/3CbfqsVv2vWPq0Xvy+bParVPy+ZPe2X/63X//FagCkflSjcjy1eTfTsYykflabaTOhckCneEeRZjepdDy9hkuncDmpcTmtdDy8hVCXajqkcjyteUCzfkK+gUMA5674pzwcUQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png\"\n        srcset=\"/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/12f09/teaser.png 148w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/e4a3f/teaser.png 295w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/fcda8/teaser.png 590w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/efc66/teaser.png 885w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/c83ae/teaser.png 1180w,\n/static/8cd3f0f4e8a5f5a5ac1669b31c5811d2/f705a/teaser.png 1493w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACbUlEQVQozwFiAp39AOnt7czZ37fN1dTd3tnd4qK6+Yal4nqj5KnM/dTVzdLNvM7KvNjb2KCwvaOyv+nr7f///6Sdr56PvMvA5QCepaaVo56twMORpZlygXF+kKx7m9dKkPVajdJ/j369tYfUy6HExax5fX90go3y9fr///91apJPO398b50AeYFpUF8qbHk/V2YncXxPw7WGp45vmZaVjHVqaXxSanxMm5xrsLSMhmJah19e/P/+/v77Zll5XEeNb2GSAKy6rYmbgYWcgXWRfoOYl4CItnR6o3N2ooyQuaaqkqmqiKOhgKmtl1JsdWVyerDAzai7xWlfiWpUm1lOeAC9xcahqqyGlZ6GmqKXq7d7jr56kNVLgv9UiPqSmYm6rIXd063DxrQ1YolaYWdZaHU3Z4pNQmhwWZ9sXo8Ajpp+Vmo+ZHVMX29EgI5rua57oIJhgYSReXV5Z3BJYVo1q6l3qLSUW2t5d2dblYN3ZYGXU095Tz58TENoAK27pYWad36Ud3WScY+ijY6QrGtqg3Rtgo6Hm6SkiJaVc5STbKGlj2t4hnd4eYKCg3uQnVFJcWpSnX1toADLztC3vsGmsbirusCzvsl5i8l2jtlMgf9XjPyZo5TFu5be0rPEx7g4ZIpVYm1ZboBBb49kXYFWRYNWTHIAiZJ5V2o0Y3VDZHNChJFur6h+lntndIGjZmuCYG9GW2A0sq15qrWWSWJ0ZlxSgHJsU3WLQjpnVDyJb1+YALK6nYqYY5Whb4yaZp6ohqynnZ6OgKKNfqyZi56lhpWaeJ+feauulY+RlZ+Uja6hmZqmro+Oq3NqjIaBltv3USyBBeTIAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png\"\n        srcset=\"/static/cb21c27cc9e586cfbbed37d8942a1ae2/12f09/manipulation.png 148w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/e4a3f/manipulation.png 295w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/fcda8/manipulation.png 590w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/efc66/manipulation.png 885w,\n/static/cb21c27cc9e586cfbbed37d8942a1ae2/72aae/manipulation.png 964w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1905.01164.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>official implementation : <a href=\"https://github.com/tamarott/SinGAN\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>기존 GAN 들을 대부분의 연구들을 보면 얼굴, 침실, 풍경 등 한 가지 종류에 focus 한 게 대부분이고, 주로 많은 데이터를 요구했습니다.</p>\n<p>다양한 종류의 object 를 생성하는 것은 여전히 잘 못하고 있고, 이런 문제를 해결할려고 conditional 하게 생성을 하거나 (e.g. cGAN), task 를 특정하는 등의 방법으로 문제를 해결하려 했습니다.</p>\n<p>왜냐면 이전 방법들로는 적은 수의 데이터와 여러 종류의 데이터의 distribution 을 잘 학습하기엔 엄청 어려웠어요</p>\n<p>그럼 이런 문제들을 어떻게 하면 해결할 수 있을까에서,</p>\n<blockquote>\n<p>'단 1 장'의 이미지로 GAN 을 훈련할 수 있을까??</p>\n</blockquote>\n<p>이런 이번 논문인 <strong>SinGAN</strong> 이란 concept 이 나오게 됐습니다. (멋지죠?)</p>\n<p>물론 이전에 이런 노력을 안한건 아니에요. 정확히 논문 이름들은 기억이 안나는데, 대부분이 input 에 대해서 conditional 한 method 를 사용하고 있었습니다.</p>\n<p>또한 이전에 Unconditional Single Image GAN 이라고 하면 Texture Generation 이란 task 로 유일하게 문제를 풀고 있었는데, 이 task 의 한계는\r\ntexture image 에 대해선 결과가 reasonable 한데, non-texture image 에 대해서는 별로 였어요.</p>\n<p>하지만 이번에 소개할 논문에서는</p>\n<ul>\n<li>unconditional 하게, noise 로 부터 image 생성</li>\n<li>general purpose 로 natural image target (non-texture) 에도 적용 가능한 방법 제안</li>\n</ul>\n<p>합니다.</p>\n<p>물론 결과는 이전 method 들 보다 훨씬 general 하고 결과도 outperform 합니다!</p>\n<h2 id=\"technical-review\" style=\"position:relative;\"><a href=\"#technical-review\" aria-label=\"technical review permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Technical Review</h2>\n<p><strong>SinGAN</strong> 에 소개된 novelty 를 1 가지로 요약 해 보면 아래와 같아요</p>\n<p><strong>Multi-Scale Architecture (# 2.1)</strong></p>\n<p>완전 새로운 concept 는 아니고, multi-scale architecture 에 대해서는 이전에 LAPGAN 이란 GAN 에서 한 번 비슷하게 소개가 되었는데,\r\n궁금하시면 한번 봐도 좋을 것 같습니다.</p>\n<h3 id=\"multi-scale-architecture\" style=\"position:relative;\"><a href=\"#multi-scale-architecture\" aria-label=\"multi scale architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi Scale Architecture</h3>\n<p>SinGAN 의 ultimate goal 이라고 하면, single image 의 internal distribution 을 잘 배우는 unconditional generative model 를 만드는 겁니다.</p>\n<p>이런 것을 하려면 다음과 같은 것들을 잘 해야 할텐데,</p>\n<ul>\n<li>many different scales 로 복잡한 image structure 의 distribution 을 capture 하기\n<ul>\n<li>global properties : 이미지 내 큰 objects 들의 모양과 배열 e.g.) 하늘 위치, 땅 위치</li>\n<li>local properties : global properties 의 details</li>\n</ul>\n</li>\n</ul>\n<p>그래서 multi-scale architecture 를 선택했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB60lEQVQozyVQyW7TUBT1n/Ez7FggwQKJFV1QCQESEhKqaCUCkVoqFQlKqTMwOI1KM+E2IYMzmTSJ33Nsx/PzyxtsI7dncRf36Jx7zxFKpdJeLv/5tKw5aGObgdLO5oYdHIsHorThye/+v53Dk/FyRQlVhlCHTlsFH8Tv0uVAeJ97d//J9t2tF/u/amQxXYsfyXwyW5kPnr969HrX8sLdT1/uPX1ZOG94jl8s95WBcSJVHz57nDv+JiCEhuPRaUmcqFOM0GKk+K7DOVeGg0G/izG+nk07bdk0VpSyqboA0HRsS1vOPccWMMZpmiZpBj8I5hp0PT+J4zjm2T65ZTIwxqCum6ZFKCU04TwRKGXNZvOrWIRO4IOFWRGJqVuef1SUQkxuZdWrntwfRwhVqsry2v4h97b2tvNFSVgZhuvYBelMal1Sz9Hr0sY23SAsSGdhFKU3l2t/5E5/wChrtCYAeK0rOXe4I9UuBAghY9mHnNEIYwBAFGGyIWEYcs4JIZTSNMlyJEmsARgGYWcy+1l/c9FtZWLOOaV0sdQcwwAVETlraFrKVM0c4zhN07Xr9kYTjKNK9S8EXvm88jZ/56i8LxBCboqJMcamYYx7XV3XKWN+EDDGKKWMMYRQEAb22uq0u6o6A9qyUW+udP0/gIPXUl4EI5wAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png\"\n        srcset=\"/static/7c27c9c01342508a98366734bfe86dd2/12f09/multi-scale-architecture.png 148w,\n/static/7c27c9c01342508a98366734bfe86dd2/e4a3f/multi-scale-architecture.png 295w,\n/static/7c27c9c01342508a98366734bfe86dd2/fcda8/multi-scale-architecture.png 590w,\n/static/7c27c9c01342508a98366734bfe86dd2/efc66/multi-scale-architecture.png 885w,\n/static/7c27c9c01342508a98366734bfe86dd2/c83ae/multi-scale-architecture.png 1180w,\n/static/7c27c9c01342508a98366734bfe86dd2/78958/multi-scale-architecture.png 1320w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>위 그림에서 x_0 가 original training image 이고, x_1 ~ x_N 가 x_0 에서 r 배 (r > 1) 씩 down-sampled image 입니다.</p>\n<p>각 scale 에서...</p>\n<h4 id=\"generator\" style=\"position:relative;\"><a href=\"#generator\" aria-label=\"generator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generator</h4>\n<p>noise (z_n) 와 이전 단계에서 생성된 image (~x_n-1) 를 받아서 image (~x_n) 을 만듭니다.</p>\n<h4 id=\"discriminator\" style=\"position:relative;\"><a href=\"#discriminator\" aria-label=\"discriminator permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Discriminator</h4>\n<p>real image 와 (x_n) fake image (~x_n) 를 구분.</p>\n<p>하나 차이(?)점이 있다면, 맨 아래 scale stage 에서는 only noise (z_N) 를 사용해서 image 를 생성합니다.</p>\n<p>논문에서 coarse-to-fine fashion 이라고 소개를 하는데, 좀 쉽게 설명 해 보면,</p>\n<p>아래 단계에서는 down-sampled image 를 학습하니, 상대적으로 detail 보단 global 한 feature 에 집중을 하면서 학습을 하고,\r\n위 단계일 수록 fine feature 에 더욱 집중하게 됩니다. 동일한 receptive field 에 생성하는 image scale 이 다르니,\r\n위 그림에 <strong>Effective Patch Size</strong> 가 달라지면서 coarse-to-fine fashion 으로 학습이 된다 입니다.</p>\n<h4 id=\"single-scale-generation\" style=\"position:relative;\"><a href=\"#single-scale-generation\" aria-label=\"single scale generation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Scale Generation</h4>\n<p>각 G_N 부분에 해당되는 block 인데, 구조는 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABdElEQVQY032QXU+CUACG+bNt3ZZ/oc3caplkZdZmbm7imqv1tTIrXWMz/EixEsxAQCFBhSMICpzTrLsuei6f7bl4Xwz9y9S2Xc/7I+dzV1VVhBCWz+fTBFEsFhYa+hND1yRx1JPAl2IP1S7zLnXaI7mvK7KtDcyhihC0HafL84s4guOBwOomvntPUg8VmmdZp/NmfrwN6OqMY8/SqaPdnYMo/niWRQPJ5hjLdp5r9fObu7Y8xjiep+sv8ezlcjCygicr5eqEoaUaxZBFsVqK70U3QqHQ+vppKjloVj8p8pVhA+GDpbXw3uUT5vs+QoikqO3YfiJDiB/MuNVwxQ7s8bNu+/gwvoiDwesTAikCVykNNTWRIbZi0VyhgEEIXdc1dH080gAAQ4Hr10p6qwHYpt6q04+5yu0VdXPBkgWz/SrXy9587nnexDAWm38PtCzLAMCaTh3THEmCKvCayGti15AloPSA0tP7oipwxpfsex78wYfwG4olZbeDUAHtAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png\"\n        srcset=\"/static/771b67b3bff99ade4f7b25f40bbbdf29/12f09/single-scale-generation.png 148w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/e4a3f/single-scale-generation.png 295w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/fcda8/single-scale-generation.png 590w,\n/static/771b67b3bff99ade4f7b25f40bbbdf29/00b70/single-scale-generation.png 733w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>% 이전 stage 에서 up-sampled image : x_n+1</p>\n<ol>\n<li>z_n + x_n+1 가 conv 연산을 통과</li>\n<li>x_n+1 가 residual 하게 마지막에 연결</li>\n</ol>\n<p>가 간단한 구조인데, conv block 부분을 더 자세하게 설명하면,</p>\n<p><code class=\"language-text\">Conv (3x3) - BatchNorm - LeakyReLU</code></p>\n<p>이 convention 으로 5 층을 쌓았네요.</p>\n<p>처음 (coarsest scale) 엔 32 kernels / block 으로 시작을 하고 4 scales 마다 kernel 을 2 배 늘려 주었다고 합니다.</p>\n<p>이렇게 해 준 이유는 (상대적으로 light 한 구조여서),</p>\n<p>주로 generator 의 capacity 가 커지면 training image 를 외어버리는 경우가 생기는데, 이를 방지하려고 light 하게 설계를 한 것 같습니다.</p>\n<p>또한 fully-convolutional 하게 설계를 한 이유는, arbitrary image size 에도 training / inference 가 가능하게끔 하려고 라고 설명을 합니다.</p>\n<h3 id=\"gan-training\" style=\"position:relative;\"><a href=\"#gan-training\" aria-label=\"gan training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GAN training</h3>\n<p>이 부분이 이제 GAN을 학습하는데 있어서 제일 중요한 부분인데, 딱히 특별한 부분은 없습니다.</p>\n<h4 id=\"loss-function\" style=\"position:relative;\"><a href=\"#loss-function\" aria-label=\"loss function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>loss function</h4>\n<p>loss 는 adversarial loss + reconstruction loss 로 이뤄졌고</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">total_loss <span class=\"token operator\">=</span> adv_loss <span class=\"token operator\">+</span> alpha <span class=\"token operator\">*</span> rec_loss</code></pre></div>\n<h5 id=\"adversarial-loss\" style=\"position:relative;\"><a href=\"#adversarial-loss\" aria-label=\"adversarial loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial loss</h5>\n<p>WGAN-GP loss 사용 했고. 논문에 보면, 다른 texture single image GAN 과 다르게, patch 별이 아닌 전체 이미지에 대한\r\nloss 를 사용했더니 네트워크가 boundary conditions (SM) 를 학습할 수 있었다고 합니다.</p>\n<h4 id=\"reconstruction-loss\" style=\"position:relative;\"><a href=\"#reconstruction-loss\" aria-label=\"reconstruction loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reconstruction loss</h4>\n<p>l2 loss 를 사용. 각 stage 에서의 rec loss 를 다음과 같이 정의가 가능한데,</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">rec_loss_n <span class=\"token operator\">=</span> l2_loss<span class=\"token punctuation\">(</span>G_n<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">~</span>x_n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x_n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>또한 ~x_n 의 역할이 하나 더 있는데, stage n 에서의 noise z_n 에 대한 std 값을 결정하는데 쓰여요.\r\n~x_n+1 하고 x_n 의 RMSE 값을 구해서 각 scale 에 얼마만큼 더해야 하는지를 알려주는 정도로 사용된다고 합니다.</p>\n<p>또 중요한 부분은 noise 를 넣어줄 때, 첫 단계에만 fixed noise 로 넣어주고 다른 단계에서는 noise 를 따로 만들어 주지 않았는데,\r\nimage pixel difference 를 줄이려는 것에 focus 를 하려고 이렇게 했다고 캅니다.</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"정량적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\" aria-label=\"정량적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정량적인</h3>\n<p>총 2 가지의 정략적인 방법을 사용했는데,</p>\n<ol>\n<li>Amazon Mechanical Turk (AMT)</li>\n<li>Single Image Frechet Inception Distance (SIFID)</li>\n</ol>\n<p>요즘 GAN paper 들에서 자주 사용하는 metric 들입니다.</p>\n<h4 id=\"amt\" style=\"position:relative;\"><a href=\"#amt\" aria-label=\"amt permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AMT</h4>\n<p>AMT 는 사람들에게 직접 답을 하게 해서 결과를 매기는 서비스입니다.\r\n여기서는 해당 이미지가 Fake 인지 Real 인지를 구별하게 하는 투표 방식을 사용했습니다.</p>\n<p>여기선 2 가지 방식으로 조사를 하였는데,</p>\n<ol>\n<li>실제 이미지와 SinGAN 이 생성한 이미지를 보여주고 어느 쪽이 가짜인지 맞히는 Paired 실험</li>\n<li>둘 중 하나만 보여주고 얼마나 헷갈렸는지를 물어보는 Unpaired 실험</li>\n</ol>\n<p>실험 조건은, 각 실험 당 1 초의 시간에 1 명당 50 장의 image 를 보여주었답니다.</p>\n<p>생성한 이미지는 stage N, N - 1 에서 생성한 이미지들을 주었다는데, (논문에선 N -2 까지)</p>\n<ul>\n<li>stage N 은 noise 로 부터 생성한 이미지고</li>\n<li>stage N - 1 은 진짜 이미지를 축소해서 G_N-1 에 넣어주는 방식인</li>\n</ul>\n<p>이런 실험에선 노이즈로 부터 생성할 때와 진짜 이미지 기반으로 생성할 때의 차이를 볼 수 있는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 95.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEeElEQVQ4yxXP+VNSCQAA4PcX7e60205NipvlgeaGIqASoNyHyHvAI15xRHIolHaIwLs4FSjDBwo871Wbjs2tNop0pt3Krtmx9uqY2V2hHf+A74cPqFarL99+fP7uv9LT328/fPbrmz+3d96/ePth+92/95/sbGy9+fnJb9s777d3/t5+9095+6+Nzdcbm69evH75x4dPQKX6ecBPc2C7wOLku+I8X1F4saD0L3fCzi4j3H16iHs2wj1XUAZWeLYQTw92I2eEQzGRzekLk3vYGl873tvBETdLvbgouCpFVwyxm0LEyhYd5Svb5XaHAlvWx2/2DY71qttluk6BCdSGf3Cnl4FKZXfm6qULw1BgzEpgjvFxM0a6UwlvkrAn/GAuJKNxcQ6TTMXPJnBrArNGURuJ2jA/lLlyHqhWKuVEVyl4sEwcfoQxSsGDj8kjW4mOcoixRR4pobUP0JpHOGMzznoUPPQYZzzGD5WxAw+C326l+UClUplMRYKYPxzDwjEMCwciMf9kEscIfzgawGMYRlwiyJFkCiXIy5FoIBwNENFgZAxaKuB7Z3VgrVWtZ2t62apuFuzg+9dkgRvHdQhL2cOCTCzHBGt0TRK63YFcbJdx2Fppl31UPBTykAng5avXp8k5ocEg1PD7BnpkVsQQmbfEltVOnxTk9xsF5mEXgqXsiXmNN6Q1K/SnlXLLSZCgPelFoPywNItq8qicCqmmg6osKi8QssU4VMSkK2TvKtGzhrVfR4+tx+VzuHSOVBVwdZ5UZS5zlpIw8PzZ01vYifsY8y7WejfUdGe88T7ZUYoJfwoy7xHH7uKtd9CWDYxVSgjuBZglovUB0XQPPfqQrP9lRglsbW1mKAM1rc1NQ7ksOH1NSeU0dNGcpTTZDJib1mYzGoqSz9EIRSmozEB2Wjub1Y77ZGnCC2xulscnurUQU97H16oFKhnbda49uaSwurv5PI5KIRpQ8S2OltSS2odz+oQ8hfiEzdLfxW0bGj4FfPz0ITzVJ5R+08GuEUkYHdxDEhErWRRrjbVd/Fq1/giby+DzmLEphd1bz+XV9BsbYLNIKmGhYQioVnejGTEyyHBfZg4ON5zxNECGtlhWZBuug23fXYgeszrrdXommujzBo7qTDUjONMz0mxC6sdQFVCtVubX3NQCRC0YqQWYWtTnlkyFlcGrs7pkDrxSBKdoXWYemlt1T81CE9mB9Ax0jQZjk8Z80Q9UqrvpBe3waO8ps8ygF50y94yinVdWNL6AANZLbRYlcrLHe4mVXuwfS/SYTGIrojzv0/F5bU6HZQ8naVWv7HBLY51AdFilbYaNvFRRCsKNrO/rTwga2lrqJMLWyRmNw9fcfrxeIGzisJu6OI2RCQT4/Hl3LN6p1H+hBPchrgOeYJ1a14Cm2aDtKzX8tStQM4DsF8trLxJc++h+ieZLV6AGdhwQSfcNjYj2zjduB2dpJzXjzBc9S8s+euHc9R+Ds0Vn+tqZmaKbnvfkac/6rdDcoic1Zc/mXfT8UL7gXL9F/g9RMXu1xCGZEQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png\"\n        srcset=\"/static/f68470a6fe3e8debb47d08a2175cacf2/12f09/generate_from_different_scales.png 148w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/e4a3f/generate_from_different_scales.png 295w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/fcda8/generate_from_different_scales.png 590w,\n/static/f68470a6fe3e8debb47d08a2175cacf2/7e509/generate_from_different_scales.png 745w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>실험 결과를 보면, stage N, 노이즈로 부터 생성한 것은 원본과는 많이 다른 결과를 가져올 수 있고,\r\nstage N - 1, N - 2 는 원본 형태 유지는 되고, N - 2 같은 경우엔 texture 가 더 원본 같다는 것도 확인 할 수 있습니다.</p>\n<p>하지만, 원본 object 의 배열을 유지한 상태에서 다양성을 보장하기엔 stage N - 1 결과물이 제일 좋다고 판단이 가능하네요.\r\nAMT perceptual study 결과도 N - 1 stage 일 때가 가장 좋습니다.</p>\n<p>재밌는 점은 stage N - 1 일 때, <strong>confusion 이 47 %</strong> 인데, 사람이 보기에도 정말 헷갈리나 보네요.</p>\n<h4 id=\"sifid\" style=\"position:relative;\"><a href=\"#sifid\" aria-label=\"sifid permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SIFID</h4>\n<p>기존 FID 를 Single Image 에 맞게 변형해서 SIFID metric 을 제안해서 사용합니다.</p>\n<p>각 scale 별 SIFID 를 측정하고 AMT 에서 Survey 한 결과하고 (Paired, UnPaired) correlation 를 측정해서 유의미한 metric 임을 증명하네요.</p>\n<p>요 부분은 논문 6 ~ 7 페이지에 나와있는데, 여길 참고하세용 (<del>귀찮아</del>)</p>\n<h3 id=\"정성적인\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\" aria-label=\"정성적인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정성적인</h3>\n<p>총 5 개의 application 예시를 보여주고 있는데,</p>\n<h4 id=\"single-image-super-resolution-sisr\" style=\"position:relative;\"><a href=\"#single-image-super-resolution-sisr\" aria-label=\"single image super resolution sisr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Super Resolution (SISR)</h4>\n<p>결과 비교를 위해\r\ninternal method 인 Deep Image Prior (DIP), Zero-Shot Super Resolution (ZSSR)\r\nexternal method 인 SRGAN, EDSR 와 결과를 비교했습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABPElEQVQY0wExAc7+AKyuosC/tLq2qb25rLq5r7W0qL+8rbe1qL29s769r7+8rrq5rb+/tbi1p7+8rre3rLq6rb+9rrWzpsTEuwBwcVR3dlyEfV6OiWl/fF95eFmNimeKiGV4eFqAfF2Sj2yCgGB8e1+Bf12SkG15eFl9e16MimiHh2SLi3EAi4lsfn5mf3xjgoFnmpd9g4Jof39ji4tvmpp8f39lg4NmkZF0l5V6fHxfg4Rol5d5h4hvgIFkg4Rnra2TAHNzVpOPb5WNa3t3XYuKcKehe4+KaHd3W5uYd6CZc4aBYXp5XqilgpiSbIB9YYeGaqymgpGMaHRzVqOjiQDa3NXc29Tf3NPRz8i/v7nFw7vX1MvV1c7Ozcbe2tHf3dTR0MrJyL/Oy8HNzMS5ubLQzcTk4dfT0svV1M5lI7Nj1wxs0QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png\"\n        srcset=\"/static/7c177a1b2169c7eb75c0a40183049776/12f09/sisr_result.png 148w,\n/static/7c177a1b2169c7eb75c0a40183049776/e4a3f/sisr_result.png 295w,\n/static/7c177a1b2169c7eb75c0a40183049776/fcda8/sisr_result.png 590w,\n/static/7c177a1b2169c7eb75c0a40183049776/efc66/sisr_result.png 885w,\n/static/7c177a1b2169c7eb75c0a40183049776/c83ae/sisr_result.png 1180w,\n/static/7c177a1b2169c7eb75c0a40183049776/acd79/sisr_result.png 1543w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>(distortion quality 인 RMSE 가 낮을 수록 좋고, Perceptual Quality 인 NIQE 가 높을 수록 좋음)</p>\n<p>다른 Internal Method 에 비해 RMSE 는 높지만, external method 인 SRGAN 과 NIQE 값은 comparable 합니다.</p>\n<p>1 장의 이미지만 사용해서 이 정도 결과라서 정말 신기하네. 개인적으론 PSNR 같은 다른 metric 하고 모델 들도 넣어줬으면 좋을 것 같네요.</p>\n<h4 id=\"paint-to-image-style-transfer\" style=\"position:relative;\"><a href=\"#paint-to-image-style-transfer\" aria-label=\"paint to image style transfer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Paint-To-Image Style Transfer</h4>\n<p>Paint Image 로 부터 생성하는 style transfer 실험인데, quality 가 꽤괜입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABeUlEQVQY0wFuAZH+ANbd6bzK4bbF3crX6szZ7rTC2bfE2c7a7sjU57rG2LvI28TS57zU96/D3rXF3LDI7Mva8rfE3LXC2cHO5QC+xcy5wsW4taG1vL+dr8iXrcyUrtiZstq0vMC2vLqzvcW1w9OWs9mGqdWKrdmRteags8+dsMyZs92YstsApZqAlYhilIZdmY90m5F6j4p5i4qEkpOUpJVyoI9mn5BupJqBlYNhdWtRdHBZd3Jll4pxmY5zlI19k4+EAJSPaYR+W4R+XYyJZIiKXYKDUoOEUIqKWYuIX4aAWIWBVY2HX4aFPXt7MHl6N3R1KIOGVYaGW4WEVoSEVACGjEyCgzqGhD+Nk0yOlUF/iTKDijSJkz+Kjkh9gj+Ag0GJjkuMmDN6ihyCjiCHlC6PlkSDijiEjDyHjjoAm6BslphekpZck51lm6Fjmp1ZlZxYoKZjmqFplplhk5pfnaVqmaNYj5hKkppKm6FWn6Rll5ldl51dlp1cRwrPVS/1STYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png\"\n        srcset=\"/static/3389c01a5021789c0be4714d0ba0749b/12f09/paint_to_image_style_transfer_result.png 148w,\n/static/3389c01a5021789c0be4714d0ba0749b/e4a3f/paint_to_image_style_transfer_result.png 295w,\n/static/3389c01a5021789c0be4714d0ba0749b/fcda8/paint_to_image_style_transfer_result.png 590w,\n/static/3389c01a5021789c0be4714d0ba0749b/efc66/paint_to_image_style_transfer_result.png 885w,\n/static/3389c01a5021789c0be4714d0ba0749b/c83ae/paint_to_image_style_transfer_result.png 1180w,\n/static/3389c01a5021789c0be4714d0ba0749b/8b84a/paint_to_image_style_transfer_result.png 1531w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"editing\" style=\"position:relative;\"><a href=\"#editing\" aria-label=\"editing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Editing</h4>\n<p>원본 이미지에 일부 영역들을 임의의 무언가로 넣으면 이 친구가 얼마나 자연스럽게 만들어 주는지 확인 해 주는 task 인데,\r\n이것도 꽤 재밌는 결과를 보이네요.</p>\n<p>아래 이미지에서 <em>Content Aware Move</em> 가 포토샵 기능인데, 이 것보다 잘하는 거 같네요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADnklEQVQozwGTA2z8AOHm78zS3rzCzrS8x624xrnCzL3K2aq0wcrR397k8Nvf6tvi7sbN1rW9x7O/zbjCzq67y8vV5Nng7uTo8QBxiLFYeaxvkbuBpcqXnaKQgHR9eHtuhqhZfrhlf7FlfalbgbZ2eYeGb2KSkI2HeG59fIFthqlQc62LoMYAjKTEiqTDp8Tdsa2kpXxam21NkmA7gnFqjq7Ok5KXf5a3f6DFjpejlWtIkmE8hFArj1w1iWNHbHSEsq2sAK6+y8DX5s7U0YhhR25MPI5vXrWSc4RhSIiHiaCLd6q7yarC1p3B4IqDgmI4JY1sWayCW3dSN3pybrymkAByammdn5zJr45xTDZdQDfY2dbo5tqigF6ifVWggWNuZmagoZ+4vbt5aWNsT0Pp7+7Z0LyRbEifeVG/p48ART5FSDEll3RRVTkutJt2/++479qkxaJvr4lboIFkMCcwYEUxjmpIUjQozbaL/+2269Sfwp5tp4BUuqOLAJF1XIliPW1LNFk3KKKAU+W+d9aua7GKWaN6T6OBYYlrTohhO2NBLl89K7SRXOS9ddGoaK2FV55zR76kiwD16NfUxba8s66zraqzrKrHvrbJvbS8sKfUyLzs4NXz5NPdz8KyqqaqpaWzrazFvbS8sqrFu7Pj18ru5t4AjKTGa4+6gZ+/jJeklrDFlp+mjKCzfaLIapPFgaDMf5vCdJjFiJ61kpadmKi0l5+nhp60eqHJbpfKnrbZAHCPtVyItYaiuqSAYZ92VI5aN4VZPH5va2iDoXOHomaEqmORv5GQj5VdMZlqRYpWM4peQId2bF18n5qltACnv9SRr8erx9ugiXV2RCmEUDKfZz6MVjGBfoCol4egt8mau9SkwdSZfWV0RCmGUTKmbkKKVzJ6cGvBq5YAj4+RtMHFvNLdkH9zVy4gta6t18u6jmVDjG9WooJnjI6RscfTudrwmYp9YjgoxsXH0r2lhFs9knBSwKaPAEg9QVVAOLKTc2VENpJ1X/7z0+/iw7qUaLaJWqWDZDUnLGlZUrCgjWtIN66SeP/42efWs7iPYrCCVcCkjQBlU0tsRit3TjJMKBu3kmD40orqw37EmGGoek2ge1pZRTp5Ti5nPiRSLR3Np232z4flvHm8j1ugcUS9n4UA3L6gx6WGoId6mYF2qZGA2buX0rKQvaGJx6mOzrSe3Lycw6SJn4l9nod7s5uI3b+bzrCRv6WOyauQ3Mq6AyME9YoHgXAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png\"\n        srcset=\"/static/827c27d77bcc338201f45b2bc70727ea/12f09/editing_result.png 148w,\n/static/827c27d77bcc338201f45b2bc70727ea/e4a3f/editing_result.png 295w,\n/static/827c27d77bcc338201f45b2bc70727ea/fcda8/editing_result.png 590w,\n/static/827c27d77bcc338201f45b2bc70727ea/f1d1f/editing_result.png 739w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"harmonization\" style=\"position:relative;\"><a href=\"#harmonization\" aria-label=\"harmonization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Harmonization</h4>\n<p>Image 2 장을 합쳤을 때 조화롭게 잘 합쳐주는 지를 보는 task 인데, 이것도 꽤 자연스럽게 잘 되는 것 같아요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.72972972972974%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACqklEQVQozwGfAmD9AN3e4dbY38DCy6iruNDS3Nre6M3O06uusayutrCyu62ut6mstq2yutfY3MbIzqWosLS1wamrtr7Czefq8ABkbpFFTnVSXoJWXn1QX4mJk3qpr6VIUoFPWX1SX4RSXoNcbIqpr4SCi6FCS3ZVYoVRW3tUYolten67wJsAd4acTF1yT1trYGFjeYWdcYeWqbS1V2eBUGB4Slp7WWuIXXOXiZuck6CtTV11UF90TFRabnuMZXqUqLWxAHmIlDxHTXeHlF5wiGhzgGZzg5efqlFga1BbY3aJnVtuiWJ1im96io+apj1JUmp3gGd7k15tfmFxgI2UoQBcZnUcIB81PUU8S2QyP1o7RmB/h5YyP0whJSI6R1g5RGAxPlhSXXRzfY0gKC8qMTM9S2I1QV8vPFl4gJMAd3l5T1NUU1ZZY2ZsY2hyZW56kZWWY2FYU1JNVFZWY2prXGNpaXJ2iIqEWFdPT05LWl9gY2psVFxkiJCRADE/ZA8zdSZBhVRan2NtszZWtJGWqfe/gdSZecyIZcOEYLx1TK1sUt/AjeqygNOUdMqFYsWHW7RjOL6PegBmSVlRLDt3TEuHY2SFf5VyfKKxtZ/dxomefXTerE/wtkLdqUSshEjd14iwqomYjXDsuEbbrVe1jla0mnAAyopa66NF89FqyKxwyZpLnnlLvKVm0plHz30nuH8obVYzOC4nMzYzu6xkwI5FzYIgl3EtS0tPLjRIZm1qAI1BO5s7EcKOO9meOI5MMjI+VpKNbuzAUe6QG6Y9LmU4RSwlQzk/W7Klb++7PdtnGIw4O1Q3RxsYOW1zfwBjWGdVM0GkcU7bolmubWFda5eioJDvzF3emTe9UjOATlRQS2ZHUXDLunPsxlTWejCmSj1uU2E/QGN3f5D8uzsaMM5GCwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png\"\n        srcset=\"/static/261bf4d359af935e3dfea0a8c3c3118d/12f09/harmonization_result.png 148w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/e4a3f/harmonization_result.png 295w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/fcda8/harmonization_result.png 590w,\n/static/261bf4d359af935e3dfea0a8c3c3118d/d0cc0/harmonization_result.png 732w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"single-image-animation\" style=\"position:relative;\"><a href=\"#single-image-animation\" aria-label=\"single image animation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Image Animation</h4>\n<p>한 장의 이미지를 넣어주면 짧은 video clip 을 만들어 주는 task 인데, 이 것도 자연스럽게 잘 되는 것 같네요</p>\n<p><a href=\"https://www.youtube.com/watch?v=xk8bWLZk4DU&#x26;feature=youtu.be\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">video</a></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>이번에 SinGAN paper review 를 해 보았는데, 이미지 한 장만 사용한다는 점과 Application 들이 정말 좋았던 paper 였어요.</p>\n<p>개인적으론 network 설계나 등등 요소들은 조금 아쉽네요.</p>","excerpt":"TL;DR 이번 포스팅에서는 ICCV 2019 에서 Best Paper Awards 에서 선정된 papers 중에 하나인 SinGAN 을 리뷰해 보겠습니다. 개인적으로 정말 재밌게 본 논문이고, ICCV 2019 논문들 중 최고였던거 같아요. 그래서…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#technical-review\">Technical Review</a></p>\n<ul>\n<li><a href=\"#multi-scale-architecture\">Multi Scale Architecture</a></li>\n<li><a href=\"#gan-training\">GAN training</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"#%EC%A0%95%EB%9F%89%EC%A0%81%EC%9D%B8\">정량적인</a></li>\n<li><a href=\"#%EC%A0%95%EC%84%B1%EC%A0%81%EC%9D%B8\">정성적인</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/SinGAN/"},"frontmatter":{"title":"SinGAN - Learning a Generative Model from a Single Natural Image","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["GAN","multi-stage","one-shot"],"update":"Mar 14, 2020"},"timeToRead":8}},"pageContext":{"slug":"/SinGAN/","series":[],"lastmod":"2020-03-14"}},
    "staticQueryHashes": ["2027115977","2744905544","694178885"]}