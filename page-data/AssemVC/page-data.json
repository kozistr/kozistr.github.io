{"componentChunkName":"component---src-templates-post-tsx","path":"/AssemVC/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>최근 mindslab 에서 Cotatron에 이어 새로운 VC (Voice Conversion) 논문이 나와서 논문을 읽게 됐습니다.</p>\n<p>code는 곧 나올 예정인 듯합니다. <a href=\"https://github.com/mindslab-ai/assem-vc/issues/1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">issue</a>를 보니 mid-june 에 release 할 가능성이 있다고 카네요.</p>\n<p>paper : <a href=\"https://arxiv.org/pdf/2104.00931.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/mindslab-ai/assem-vc\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">github</a></p>\n<p>demo : <a href=\"https://mindslab-ai.github.io/assem-vc/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">demo</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요 논문과 관련높은 reference</p>\n<ol>\n<li>Cotatron : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a>, my review : <a href=\"https://kozistr.tech/Cotatron/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">review</a></li>\n</ol>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>논문 제목부터 보면 <em>Assem-VC</em> 인데, 가존에 제안된 여러 VC 모델들의 장점들을 잘 조합해 하나로 만들어 둔 느낌입니다.\n<em>2. Approach</em> 를 보면, <code class=\"language-text\">Cotatorn-VC</code>, <code class=\"language-text\">Mellotron-VC</code>, <code class=\"language-text\">PPG-VC</code> 이렇게 3개의 모델을 baseline으로 해서 만들었다고 합니다.</p>\n<p>그리고 3가지의 components로 나눠 살펴보는데, 다음과 같습니다.</p>\n<ol>\n<li>linguistic encoder</li>\n<li>intonation encoder</li>\n<li>decoder</li>\n</ol>\n<p>% <code class=\"language-text\">PPG</code> : phonetic posteriorgrams</p>\n<p><code class=\"language-text\">Auto-VC</code> 논문에서 저자가 source speaker의 linguistic, intonation 정보가 bottleneck 부분에 들어가게 되면 self-reconstruction 퀄리티를 떨어트린다고 합니다.\n즉, <strong>speaker independent</strong> 하게 학습해야 양질의 conversion 이 가능하다고 합니다.</p>\n<p><code class=\"language-text\">Auto-VC</code> 논문을 읽어보진 않았지만, speaker specific 한 정보가 reconstruction 과정에 들어가면, speaker-biased 된 학습이 이뤄질 수 있어 speaker-overfitted, not generalized 한 모델이 만들어 짐을 유도하는 맥락같다는 생각이 드네요. (저도 이런 argument에 동의합니다)</p>\n<p>쨋든, 이런 이유로 encoder design 을 speaker independent하게 만들었다고 합니다.</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2c573c0f2f52e3de28161d9afa9cc503/0786c/architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACUklEQVR42nVSTY/TMBDNL+fEFYkzIECABFw4LCy7sOyBsqwAqa1a7aabxIk/EidO0qb5sh07X5CWExJPGkszfm+eZjTG+H80UhKMYhoi1yOENEr9QzDGcZRCVGVVVeUwjHnmVkW0Lfi1QxcoBhC+np2//XaR7fdHgda6rkVdcynlJC6KPEmSJE26fkzYoth5G0hffr85WUHPD55cnz2anSbbnR1lTrynjNnWAnrL3W5rHPsNh+j7TsqyrvZFUehGtmoCiUKPIIDJ0yvz8ew2THc5m+XJpVL84JwXjEVZlqumZP7JNp75JIDQwxhbjn3v7NX9z2+4FK3WZZHfRdnHpXM6t+cwOswspRBCa933Pa9LKXnXdUJKIaVSKt3v0mw3DIPWrajKSyt8+GX+4PznuzWexHHMFvP5crlK05AFF9vo+sOvzfOr2xc/7BVkj7++f3b1qRVNlmWc83EcEcx9f9+2g9H3nRAiCqOQhuk2qSvciAinxR3L7+I8q5uV794EcBzHru+mtxsWc+4CVVWdURRFWZZN03DOhZRt21UVV41UkivBGym7tm3k9FuWpRD8QOZKCaUaA2O0Xq/DMAwoxRhDzzNNkxCy2WwsyzI3m4AGHvRsy/K8aYUsjhFCtu0QQow0TQnGAABCiOd5rusdSY4DMCYQQuC6lFLXdU3TBMDxgyCOY4xxQOkkRgiFBwAAgONM/hA6DogZ831iWRZC2Pd94LpRGCVpCiFkjIUhNSilR9vA9xFCrusSnwAAEER/U0Js28Z4KkjZaK0551rrP+f5G8dTwZsMA1QkAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"architecture\"\n        title=\"\"\n        src=\"/static/2c573c0f2f52e3de28161d9afa9cc503/fcda8/architecture.png\"\n        srcset=\"/static/2c573c0f2f52e3de28161d9afa9cc503/12f09/architecture.png 148w,\n/static/2c573c0f2f52e3de28161d9afa9cc503/e4a3f/architecture.png 295w,\n/static/2c573c0f2f52e3de28161d9afa9cc503/fcda8/architecture.png 590w,\n/static/2c573c0f2f52e3de28161d9afa9cc503/0786c/architecture.png 663w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>구체적으로 하나하나 보기 전에 전체적으로 한번 훑어보면,\n<code class=\"language-text\">Cotatron-VC</code> 와 <code class=\"language-text\">Normalized-F0</code> 로 linguistic, intonation features (speaker-independent)를 생성하고,\n이 정보들과 target speaker를 encoding한 정보와 같이 decoder에 넣어 mel-spectrogram 을 뽑고, 마지막으로 <code class=\"language-text\">HiFi-GAN</code> 으로 raw audio를 생성하는 방식입니다.</p>\n<h3 id=\"linguistic-encoder\" style=\"position:relative;\"><a href=\"#linguistic-encoder\" aria-label=\"linguistic encoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Linguistic Encoder</h3>\n<p>결론부터 말하자면 <code class=\"language-text\">Cotatron-VC</code> 에 있는 linguistic encoder 를 채택했습니다.</p>\n<p><code class=\"language-text\">PPG-VC</code> 같은 경우엔 time-frame 별로 phoneme 의 posterior probability를 <em>pretrained speaker-independent ASR</em> 베이스로 주는 방식인데,\n일단 ASR 를 훈련하려면 많은 양의 transcription 으로 학습한 ASR이 있어야 하고 (-> 어렵), output이 각 phoneme 의 확률만 주는 친구라서 사용하기 흠터레스팅하다 언급합니다.</p>\n<h3 id=\"intonation-encoder\" style=\"position:relative;\"><a href=\"#intonation-encoder\" aria-label=\"intonation encoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Intonation Encoder</h3>\n<p>이 부분도 결론부터 말하자면 <code class=\"language-text\">Cotatron-VC</code> 에 있는 intonation encoder에 개선점을 추가해 사용했습니다.</p>\n<p><code class=\"language-text\">Cotatron-VC</code>에서 intonation encoder에 해당하는 residual encoder가 <strong>unseen</strong> speakers or noises 에 대해서 잘 하는지는 여전히 의문이라고 합니다.</p>\n<p>그래서 <code class=\"language-text\">PPG-VC</code> 와 <code class=\"language-text\">Mellotron-VC</code>에서 채택해 사용하던 <code class=\"language-text\">Normalized-F0</code> 를 사용했다고 합니다.</p>\n<p>speaker voice 를 time frame 별로 log f0를 뽑고 mean/std normalize 후, unvocied segments는 <em>constant -10</em>으로 채우는 작업을 했다는데, 구체적인 구현 사항은 논문을 참고하세용.</p>\n<p>이런 작업을 해서 pitch contour는 speaker independent해 지면서 decoder 는 speaker 의 pitch range를 잘 잡아 학습 가능하다고 캅니다.</p>\n<h3 id=\"decoder\" style=\"position:relative;\"><a href=\"#decoder\" aria-label=\"decoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decoder</h3>\n<p>결론은 non-casual decoder를 채택한 듯 합니다.</p>\n<p>3개 baseline 중 <code class=\"language-text\">Mellotron-VC</code>만 autoregressive casual decoder 를 사용하는데, 이런 구조가 non-casual decoder 들 보다 더 좋은 퀄의 mel-spectrogram 을 뽑을 수 있다고 합니다.</p>\n<p>하지만, teacher forcing method 로 학습하는 과정이 source-speaker에 대해 cheating할 수 있다는 점을 언급해, 실제로는 speaker disentanglement 를 못 지킬수도(?) 있다고 합니다.</p>\n<p>논문에선 non-casual decoders (<code class=\"language-text\">PPG-VC</code>, <code class=\"language-text\">Cotatron-VC</code>) 차이도 설명하는데, 논문 참조~</p>\n<h3 id=\"vocoder\" style=\"position:relative;\"><a href=\"#vocoder\" aria-label=\"vocoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vocoder</h3>\n<p>최근 VC methods를 보면 raw audio 만드는 부분에서 <code class=\"language-text\">WaveNet</code> 기반 vocoder를 사용하는데, 논문에선 real-world 에서는 <code class=\"language-text\">WaveNet</code> 기반 vocoder 는 latency가 나오지 않기 때문에 <code class=\"language-text\">HiFi-GAN</code> 기반으로 생성한다고 합니다.</p>\n<p>이 중 포인트는, Ground Truth Alignment (GTA, 왠지 차 훔쳐야 할 거 같은 이름) mel-spec을 finetune시 사용했다고 하는데,\n이 논문에선 GTA mel-spec 부분을 reconstructed mel-spec으로 해석해서 튜닝을 했다고 하네요.</p>\n<h2 id=\"train-recipe\" style=\"position:relative;\"><a href=\"#train-recipe\" aria-label=\"train recipe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Train Recipe</h2>\n<p>훈련 방식에도 차이가 있습니다. 3개의 baselines을 모두 훈련하는데,</p>\n<p>먼저, <code class=\"language-text\">Cotatron-VC</code>를 <em>LibriTTS train-clean</em> 으로 훈련 후 <em>LibriTTS</em> + <em>VCTK</em> 로 progressively 훈련헀다고 합니다.\n그리고 <code class=\"language-text\">Cotatron-VC</code> 는 freeze하고 <code class=\"language-text\">Assem-AC</code> 전체 pipeline 을 훈련했다고 합니다.</p>\n<p>다른 recipes는 논문에</p>\n<h2 id=\"performance\" style=\"position:relative;\"><a href=\"#performance\" aria-label=\"performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Performance</h2>\n<h3 id=\"vc\" style=\"position:relative;\"><a href=\"#vc\" aria-label=\"vc permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VC</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dd252b1175e9a01f0fc126c31ac6bc91/d4b10/VC-benchmarks.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.83783783783784%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABQUlEQVR42i2Q647rIAyE8/5PuD3q7kJjAgQbG8ItSaWump7vhzW2ZHk8kzHweDwMgFbqLS4e80xEIsLMIlJrTSnxxUekiwkAaq0iklJCxNvt61cpvhZaa/s+FrsAgDFmXHjv53l2zo0xJmYupazr+jkFAIgYQpALZlZKxRh/fr6JqJTinMs5A8C2bdPt9k8pNcZorfXe930/jmOMse/7eZ7Oe2MgxuicQwzv4XGc53lcdULEWmuMERG3bQshvFuOpbxtE5ExBpEAIGAotaace+8551rrVEp5vV6999ba8/n8fCEiOefWml3s/X533mutnXOROYSQroRKKdOy2N67sBBR7805xyzrujJzrcVaq7X23i9mWdc1pywi27allHLO/9NGooAh5+y9IyLvPSKKiF0W9atmAK31PM9IhBd2sSGEP4Y8vYbyw1EiAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"benchmark\"\n        title=\"\"\n        src=\"/static/dd252b1175e9a01f0fc126c31ac6bc91/fcda8/VC-benchmarks.png\"\n        srcset=\"/static/dd252b1175e9a01f0fc126c31ac6bc91/12f09/VC-benchmarks.png 148w,\n/static/dd252b1175e9a01f0fc126c31ac6bc91/e4a3f/VC-benchmarks.png 295w,\n/static/dd252b1175e9a01f0fc126c31ac6bc91/fcda8/VC-benchmarks.png 590w,\n/static/dd252b1175e9a01f0fc126c31ac6bc91/efc66/VC-benchmarks.png 885w,\n/static/dd252b1175e9a01f0fc126c31ac6bc91/c83ae/VC-benchmarks.png 1180w,\n/static/dd252b1175e9a01f0fc126c31ac6bc91/d4b10/VC-benchmarks.png 1394w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><code class=\"language-text\">Assem-VC</code>가 <code class=\"language-text\">MOS</code>, <code class=\"language-text\">DMOS</code>에서 제일 좋은 성능을 보여주고 있네요.</p>\n<h3 id=\"degree-of-disentanglement\" style=\"position:relative;\"><a href=\"#degree-of-disentanglement\" aria-label=\"degree of disentanglement permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Degree of disentanglement</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/12065443d42115b2b077eab971846735/0786c/sca.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.108108108108105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABFklEQVR42k2QwY7CMAxE+/+/BXtkOXBpk9ixE6cVbdiWQlzBYVkVCZYnyxrPyAe7aowZhmyMYeau65jZWStJiAgRrbXOOe9927aIAIAxRmYWSfM8V4heojBTEAEA5xwCxBitNUTsnKvrum07RO8R17UYEaFpmv7YVyFGWBNPRF2fH4/H75sP+WH9u1UpRVXLGy26LPrkZTzrOa/tFd1ut2q7/dpstuDsfr8HgFLK925XN01R7Y/HIedSSgwrRDRNEzOnJMba83muDodD3/fX69UYIyLLshATh6Cqz/OwqP6cMnovIvf7Pef1uzGuuqrr5pTzeZoQIKV2GkfyxBzGcWQmj/gzjnkYOISU2svlMgwDIiZJqvoHeTh986cPLA4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"sca\"\n        title=\"\"\n        src=\"/static/12065443d42115b2b077eab971846735/fcda8/sca.png\"\n        srcset=\"/static/12065443d42115b2b077eab971846735/12f09/sca.png 148w,\n/static/12065443d42115b2b077eab971846735/e4a3f/sca.png 295w,\n/static/12065443d42115b2b077eab971846735/fcda8/sca.png 590w,\n/static/12065443d42115b2b077eab971846735/0786c/sca.png 663w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>disentanglement가 잘 됐는지를 측정하기 위해 <code class=\"language-text\">SCA</code>로 benchmark 합니다. 아마 <code class=\"language-text\">Cotatron-VC</code>논문에서도 <code class=\"language-text\">SCA</code>를 사용했던 것 같은데, 앞으로 다른 연구에서도 <code class=\"language-text\">SCA</code>를 사용하길 바란다고 합니다 ㅋㅋㅋㅋ</p>\n<p>개인적으론, <code class=\"language-text\">SCA</code> 값은 일반적인 trend를 보여주는 데엔 괜찮을 수도 있지만, <code class=\"language-text\">SCA</code> 측정에 사용하는 모델 성능과 방식에 따라 variance가 많이 커지지 않을까란 생각이 있어서 더 solid?한 tolerant한 method가 뭐가 있을까 한번 생각해 보게 되네요.\n(higlhy personal thought :), not the answer)</p>\n<p>% <code class=\"language-text\">SCA</code> : Speaker Classification Accuracy</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>어쩌면 단순 조합이라고도 할 수 있지만, 여러 architecture를 benchmark하고 각 components를 meaningful하게 가져와 사용한 점과 성능도 outperform 해서 좋아 보였습니다.</p>\n<p>요즘의 연구 트렌드들이 architecture design을 revisit 하는 연구보단 training recipe &#x26; combining network design로 outperform한 성능을 내는 연구가 많아지는 것 같아서, 갠적으로 튜닝을 좋아하는 사람 중 하나고, 이런 recipes를 발견하고 적용하는 게 더 큰 성능 향상을 낼 수 있다 생각하기 때문에, 더 재밌게 읽었던 것 같습니다.</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 최근 mindslab 에서 Cotatron에 이어 새로운 VC (Voice Conversion) 논문이 나와서 논문을 읽게 됐습니다. code는 곧 나올 예정인 듯합니다. issue를 보니 mid-june 에 release 할 가능성이 있…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"#linguistic-encoder\">Linguistic Encoder</a></li>\n<li><a href=\"#intonation-encoder\">Intonation Encoder</a></li>\n<li><a href=\"#decoder\">Decoder</a></li>\n<li><a href=\"#vocoder\">Vocoder</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#train-recipe\">Train Recipe</a></p>\n</li>\n<li>\n<p><a href=\"#performance\">Performance</a></p>\n<ul>\n<li><a href=\"#vc\">VC</a></li>\n<li><a href=\"#degree-of-disentanglement\">Degree of disentanglement</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/AssemVC/"},"frontmatter":{"title":"Assem-VC - Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques","date":"Apr 19, 2021","tags":["Deep-Learning"],"keywords":["voice-conversion","speech-synthesis"],"update":"Apr 19, 2021"},"timeToRead":3}},"pageContext":{"slug":"/AssemVC/","series":[],"lastmod":"2021-04-19"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}