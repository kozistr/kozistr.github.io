{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/deepnet/",
    "result": {"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>최근 논문을 보면 complex 한 architecture를 design 하기보다는 training recipes을 제안하거나 large-scale 모델을 더 stable하게 학습하는 방법 등의 논문들이 많이 나오는 경향입니다 (개인적으로도 좋은 연구 방향이라고 생각함). 이번 microsoft에서 나온 <code class=\"language-text\">DeepNet</code>은 NLP에서 주로 <code class=\"language-text\">Transformer</code>를 쌓는데, <code class=\"language-text\">어떻게 하면 깊게 쌓아도 모델을 stable하게 학습할 수 있을까?</code>에 대한 연구를 진행했습니다.</p>\n<p>아래는 시간에 따른 Transformers depths 변화인데, 최근 deepmind의 <code class=\"language-text\">Gopher</code>만 해도 ~ 200 depths 인 데 엄청나게 변화가 생겼네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/84b4b660b7309d4c7c1c57902d7b10a8/5c98f/transformer-depths.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA5UlEQVQoz5WP2W6DMBBF+f+fLBiSiHhjFrxiuzKtojQPlZinkWbOXYZ955xzuzS11lm0dR2YOaV0DS6led9aG0IIl51LaaffYIzJ+fg/Y631E46xw0x8HEf9Oy/sXeNX6ITrD+zc/ur86dDfChM55/r13fls2jvHGBERAFJKIQSt9ShmqdT4NY6TEMti7AbGtlJ2xPX+eMy32zID4pBzRsRtA1CKjXHQF6u0fUorNQGTtc6FZRJWSgKySk3i/nzKnNJQayUi5h5eaQ2A3vtt24wxIfidWUpJRM57KSUghhgBIZ6dvwHN7Q2SEYogEgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer-depths\"\n        title=\"transformer-depths\"\n        src=\"/static/84b4b660b7309d4c7c1c57902d7b10a8/fcda8/transformer-depths.png\"\n        srcset=\"/static/84b4b660b7309d4c7c1c57902d7b10a8/12f09/transformer-depths.png 148w,\n/static/84b4b660b7309d4c7c1c57902d7b10a8/e4a3f/transformer-depths.png 295w,\n/static/84b4b660b7309d4c7c1c57902d7b10a8/fcda8/transformer-depths.png 590w,\n/static/84b4b660b7309d4c7c1c57902d7b10a8/efc66/transformer-depths.png 885w,\n/static/84b4b660b7309d4c7c1c57902d7b10a8/c83ae/transformer-depths.png 1180w,\n/static/84b4b660b7309d4c7c1c57902d7b10a8/5c98f/transformer-depths.png 1283w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>github : <a href=\"https://github.com/microsoft/unilm\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">repo</a></li>\n<li>paper : <a href=\"https://arxiv.org/pdf/2203.00555.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arxiv</a></li>\n</ul>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>depth가 깊은 Transformer를 학습할 때 학습을 stablize 하는 여러 논문이 있는데, 관련 있는 최근 논문 하나를 적어보면 <code class=\"language-text\">Gopher</code>가 있습니다.</p>\n<ul>\n<li><code class=\"language-text\">Gopher</code> : <a href=\"https://arxiv.org/abs/2112.11446\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n</ul>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<p>논문을 1장 요약하면 아래 이미지로 요약 가능합니다. 기존에는 <code class=\"language-text\">Post-LN</code> -> <code class=\"language-text\">Pre-LN</code> scheme을 통해 stability + performance 를 올리는 연구가 있었는데, <code class=\"language-text\">Pre-LN</code> 는 layer depths가 뒤로 갈 수록 앞 layer보다 gradient norm이 커지면서 performance degradation이 올 수 있다는 점을 이야기합니다.</p>\n<p><code class=\"language-text\">DeepNet</code>에서는 이 부분은 <code class=\"language-text\">initializations</code>을 잘해서 해결합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c96c1b00b31ad924105c0128fdec152d/71e8d/initializers.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABCElEQVQY0yWOyY6DMBBE+f+fm4jNTubAxIZugw0hMV4arHgU8g6lOjyVqljXME2z1nrbXIjRe++8J6LjOIwxUsrH4xFCNMYIcXcny7J0XeecL5Yldd0sRI+Ie6R0HBQpnwBAy5hzLgQfQkgpbdsWYySi/cNR7JRRPQeB05/Q92Hp1SyGFaeccy9lWZWPdT3VPaVEtIcQrLXnYije7/x8eq0XA0pP04yj0dpuNudsrTXGvF6vr/r97L0P/nOEiIph6I3RSilQOI4jKgREAAAENSpERIVSSiEEIsIwAEDf998sGGNlWTV1zVhb13V19svlp2nb6oQx1jRNXVX8euWcc8Zvt1/OuVLqH75gSpLnZ1qWAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"initiaizers\"\n        title=\"initiaizers\"\n        src=\"/static/c96c1b00b31ad924105c0128fdec152d/fcda8/initializers.png\"\n        srcset=\"/static/c96c1b00b31ad924105c0128fdec152d/12f09/initializers.png 148w,\n/static/c96c1b00b31ad924105c0128fdec152d/e4a3f/initializers.png 295w,\n/static/c96c1b00b31ad924105c0128fdec152d/fcda8/initializers.png 590w,\n/static/c96c1b00b31ad924105c0128fdec152d/efc66/initializers.png 885w,\n/static/c96c1b00b31ad924105c0128fdec152d/c83ae/initializers.png 1180w,\n/static/c96c1b00b31ad924105c0128fdec152d/71e8d/initializers.png 1304w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"initialization\" style=\"position:relative;\"><a href=\"#initialization\" aria-label=\"initialization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>initialization</h3>\n<p>더 구체적으로 보면 크게 2가지인데,</p>\n<ol>\n<li>residual connection 하는 부분에서 input 에 대해 scaling</li>\n<li>거의 비슷한데 (기존과 같이 xavier normal), 일부 layer에 gain 수정</li>\n</ol>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span> 는 depth가 깊어질수록 커지고, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span> (gain) 은 작게 사용하네요.</p>\n<p>formula 로 일반화 하면 아래와 같습니다.</p>\n<blockquote>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>L</mi><mi>N</mi><mo stretchy=\"false\">(</mo><mi>α</mi><msub><mi>x</mi><mi>l</mi></msub><mo>+</mo><msub><mi>G</mi><mi>l</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>l</mi></msub><mo separator=\"true\">,</mo><msub><mi>θ</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">x_{l + 1} = LN(\\alpha x_{l} + G_{l}(x_{l}, \\theta_{l}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">))</span></span></span></span></span></p>\n</blockquote>\n<ul>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span> : scaling factor at the residual connection</li>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span> : <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span>-th Transformer sub-layer (e.g. FFN or MSA) with parameter (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span>)</li>\n</ul>\n<h2 id=\"performance\" style=\"position:relative;\"><a href=\"#performance\" aria-label=\"performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Performance</h2>\n<p>다양한 NLP task와 benchmarks를 진행했는데, large-scale 에서 diverge하지 않고 더 깊게 쌓았을 때 더 좋은 성능을 보여주고 있습니다.</p>\n<h3 id=\"bleu-score\" style=\"position:relative;\"><a href=\"#bleu-score\" aria-label=\"bleu score permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BLEU score</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ffc741d661cd37f139eeb4a2cb78e5f0/86a1e/bleu-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABkUlEQVQozzWR25KjIABE/f+vm0RRbhEjCoJyk+BkUSdb7tT2U7+drtMFhA0A9bqu/or7X7yUsu97hFDf95TSpmk4519ft+ezh01z+7qNQhQY47KsKKXdFUYpNcY45yY59Zwz1o5C5D9527aUEgCg67qcc0op51ywliGMIUKEEIKxUirGeByH1ppzrrTy3ocQhJQ5Z601xnjWet/34zgKQnBdNwAACGHf9wTjYRw/n4+1Vms9a+2ce7+/vffneTLG5nkOIWilLvI4jviCkpaxgQ+Y4Ph6vd9va+28LPM8r2v0zkkpz/MU4uIrpQAAKaWCUlqWZVmVhBBQVV3XLcvivbfWCCGUUt6HX3/7vgNQh7BuKXHOP59PwTknhLD2UiWEMMbMs845W2OVUsaYGGMIQUr583M+n8+0bc45Ssg1+0EfCKG2bRljr9fr96q4ruM4dl03DMMkJ845hNB73zSN1loIgRCKMRZ1UzPGIISPx6Oqqvu9rKqKYHy73bXWxphpmpZlXpZF/5M3TZNSKoRgrf0LEInnRW1WnFQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"bleu\"\n        title=\"bleu\"\n        src=\"/static/ffc741d661cd37f139eeb4a2cb78e5f0/fcda8/bleu-benchmark.png\"\n        srcset=\"/static/ffc741d661cd37f139eeb4a2cb78e5f0/12f09/bleu-benchmark.png 148w,\n/static/ffc741d661cd37f139eeb4a2cb78e5f0/e4a3f/bleu-benchmark.png 295w,\n/static/ffc741d661cd37f139eeb4a2cb78e5f0/fcda8/bleu-benchmark.png 590w,\n/static/ffc741d661cd37f139eeb4a2cb78e5f0/efc66/bleu-benchmark.png 885w,\n/static/ffc741d661cd37f139eeb4a2cb78e5f0/c83ae/bleu-benchmark.png 1180w,\n/static/ffc741d661cd37f139eeb4a2cb78e5f0/86a1e/bleu-benchmark.png 1296w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"bleu-score-by-depths\" style=\"position:relative;\"><a href=\"#bleu-score-by-depths\" aria-label=\"bleu score by depths permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BLEU score by depths</h3>\n<p>다른 methods 는 18 dethps (약 <code class=\"language-text\">large</code> recipe) 정도만 돼도 diverge 하네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/712143abe3ee5a029f303e0dad1d2777/20785/bleu-by-depths.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABjUlEQVQozy2R0ZajIBBE/f+/SwwCkQACKqBAK0lkxoyzJ5m9/dIvVV2nuuKcI4TWNS8fcv6/OGuVUowxpfWNMUqpMaaua2MMofR8Po+jrYQQp9NJSqm1VloLIaZpijFaa7XWUghtTNm2++PxuN8vl4vSaitlWZZSSqWVRk2DMcaEXCnVWuec93333kshJ+9jTOuyWGv3/dtZSymdpvnnQyWlbFBDKaEf5Y3ftDHHcbyTOxfmOaW0bRsAHD8/nZTeeQAIIbxer2oYBtw09HO26zqMcUqplBJjdM5572FZYozDMBzHobUuX192HOu63rat6jp1udRN0/w1J4RIKb4nxnEc52mCZQEA59y+75iQnO/Px+Pt9ftb9X1PCOGct22rlAohhHkupaSUrLUhhHy/A8A4jsdxcM6fz2eMsW3bfd8rKQQltGXsxhh8iDGu6zr0A+fcGNMPQ9d1lBAAwBh774e+J5Su61qhBnWdJIQwxhBCdV0jhK7X6/l0ss79PTyEEGOc5xkA/Jsp5wyw/AOIeud9RFDqMQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"blue-depths\"\n        title=\"blue-depths\"\n        src=\"/static/712143abe3ee5a029f303e0dad1d2777/fcda8/bleu-by-depths.png\"\n        srcset=\"/static/712143abe3ee5a029f303e0dad1d2777/12f09/bleu-by-depths.png 148w,\n/static/712143abe3ee5a029f303e0dad1d2777/e4a3f/bleu-by-depths.png 295w,\n/static/712143abe3ee5a029f303e0dad1d2777/fcda8/bleu-by-depths.png 590w,\n/static/712143abe3ee5a029f303e0dad1d2777/efc66/bleu-by-depths.png 885w,\n/static/712143abe3ee5a029f303e0dad1d2777/c83ae/bleu-by-depths.png 1180w,\n/static/712143abe3ee5a029f303e0dad1d2777/20785/bleu-by-depths.png 1307w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>갠적으로도 training recipe 를 통한 성능 향상이나 이런 느낌의 연구들을 좋아하는데, 1000 depths large-scale transformer 를 간단한 방법으로 stable 하게 학습할 수 있다는 점에 재밌는 거 같습니다.</p>\n<p>결론 : 굳굳</p>","excerpt":"TL;DR 최근 논문을 보면 complex 한 architecture를 design 하기보다는 training recipes을 제안하거나 large-scale 모델을 더 stable하게 학습하는 방법 등의 논문들이 많이 나오는 경향입니다 (개인적으로…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"#initialization\">initialization</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#performance\">Performance</a></p>\n<ul>\n<li><a href=\"#bleu-score\">BLEU score</a></li>\n<li><a href=\"#bleu-score-by-depths\">BLEU score by depths</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/deepnet/"},"frontmatter":{"title":"DeepNet - Scaling Transformers to 1,000 Layers","date":"Mar 20, 2022","tags":["Deep-Learning"],"keywords":["large-scale","transformer"],"update":"Mar 20, 2022"},"timeToRead":2}},"pageContext":{"slug":"/deepnet/","series":[],"lastmod":"2022-03-20"}},
    "staticQueryHashes": ["2027115977","2744905544","694178885"]}