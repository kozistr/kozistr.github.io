{"componentChunkName":"component---src-templates-post-tsx","path":"/SAN/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 리뷰할 논문은 <em>SAN</em> (Second-order Attention Network) 이라는 Image Super Resolution task 에서 현재 여러 test set 에서 제일 높은 성능 (19년도 기준)을 보이고 있는 architecture 입니다.</p>\n<p>이 때 까지도 여러 attention module 들을 붙여서 super resolution network 의 성능을 올리는 데 trend 였는데, 재밌는 (?) approach 를 해서 리뷰 해 보게 됐습니다.</p>\n<p>paper : <a href=\"http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">CVPR19</a></p>\n<p>official implementation : <a href=\"https://github.com/daitao/SAN\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요건 다른 Super Resolution 들 paper list 입니다.</p>\n<ul>\n<li>DBPN : <a href=\"https://arxiv.org/pdf/1803.02735.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n<li>ESRGAN : <a href=\"https://arxiv.org/pdf/1809.00219\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n<li>RCAN : <a href=\"https://arxiv.org/pdf/1807.02758.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n</ul>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이전에 여러 SISR (Single Image Super Resolution) task 들의 network 들은 <em>더 넓고 깊은 구조</em>를 띄면서, 상대적으로 network 의 </p>\n<ol>\n<li><em>representation 능력</em></li>\n<li><em>각 중간 layer 들의 feature correlation</em></li>\n</ol>\n<p>들을 덜 고려하는 경향을 보였다면서, 이번에 <strong>SAN</strong> (Second-order Attention Network) 를 제안하면서, 이런 문제들을 더 고려한 구조를 맨들어 봤다고 캅니다.</p>\n<p>기존 <em>SE</em> (Squeeze and Excitation) Module 은 <em>Squeeze</em> stage 에서 <em>GAP</em> (Global Average Pooling) 을 하면서 first-order distribution feature 밖에 학습하지 못했는데,\n여기 <strong>SOCA Module</strong> 이라 부르고 attention network 에선 second-order-tic 하게 연산을 위한 operation 이 들어가서 adaptive 하게 feature 를 rescaling 했다고 하는데, 궁금해지네요.</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<p><em>SAN</em> 는 4 가지 부분으로 구성이 되어 있는데요,</p>\n<ol>\n<li>shallow feature extraction</li>\n<li>non-locally enhanced residual group (NLRG)</li>\n<li>up-scale module</li>\n<li>reconstruction part</li>\n</ol>\n<p>입니다.</p>\n<p>전반적인 <strong>SAN</strong> architecture 는 아래 사진과 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cef6e15ae5ee81d2fccc9deffe34e3be/9f9a4/overall_architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABiElEQVQY01XPP0/bQACGcX/QDqhSt24MdO5SYAEyFVSpXfgnFRQkYCEpVCEh1MVOjM/GiRM7d7bx3dln5xwiktg5gpj4re/yvFI2ZJTiagc+T4qAdM8apfHTpKbtmT1Z7VTu9LLmJWW9K2azIku3fpWO7+2b3dWDr0tKrSKppz8Pt799Pr/1IK4rR3t/ln0v+H29ooDqyfV6s71zIPfWL/8lYQiB+mHlU6mu3f/40lj7WK+eSuwR9fr2XxuJQnQ9We4cF7k4a2yEFNXa+4Z9hdJnh6VCzKcZVw0t5E8X5cPd75vdjimJN3n+GjadiXw+jRl3B2M/yCDkLpxi/LouFIV4T+IpawK1aT3EJKCPhGCnrl3SsGUq/6MQZ5zzIbcNfWC2DN2wLGsAYTbKLNMw9ZYUUXwHVAD0UUSTdEgioqg3Tt9weq6HPIIQRr5tAd9SFLUNALBNM4JIvm20lCspXyTlxYhQv+8QQoIgcB3X83zGGCUYDyCGMI4ZjeIFliQYoUmSLi7MhXgBiN5oPhUWONYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/cef6e15ae5ee81d2fccc9deffe34e3be/fcda8/overall_architecture.png\"\n        srcset=\"/static/cef6e15ae5ee81d2fccc9deffe34e3be/12f09/overall_architecture.png 148w,\n/static/cef6e15ae5ee81d2fccc9deffe34e3be/e4a3f/overall_architecture.png 295w,\n/static/cef6e15ae5ee81d2fccc9deffe34e3be/fcda8/overall_architecture.png 590w,\n/static/cef6e15ae5ee81d2fccc9deffe34e3be/efc66/overall_architecture.png 885w,\n/static/cef6e15ae5ee81d2fccc9deffe34e3be/c83ae/overall_architecture.png 1180w,\n/static/cef6e15ae5ee81d2fccc9deffe34e3be/9f9a4/overall_architecture.png 1560w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"1-shallow-feature-extraction\" style=\"position:relative;\"><a href=\"#1-shallow-feature-extraction\" aria-label=\"1 shallow feature extraction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. shallow feature extraction</h3>\n<p>논문에서 자기들은 shallow feature extraction 을 위해서 LR (Low Resolution) 이미지로 부터 오직 convolution layer 1 층만 쌓았다고 합니다.</p>\n<p>그냥 평범한 convolution2d 하나입니다.</p>\n<h3 id=\"2-non-locally-enhanced-residual-group-nlrg\" style=\"position:relative;\"><a href=\"#2-non-locally-enhanced-residual-group-nlrg\" aria-label=\"2 non locally enhanced residual group nlrg permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. non-locally enhanced residual group (NLRG)</h3>\n<p><em>MLRG</em> module 은 크게 2 가지로 구성되어 있는데,</p>\n<ol>\n<li>여러 개의 region-level non-local (RL-NL)</li>\n<li>하나의 share-source residual group (SSRG)</li>\n</ol>\n<h4 id=\"rl-nl\" style=\"position:relative;\"><a href=\"#rl-nl\" aria-label=\"rl nl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RL-NL</h4>\n<p>논문에서 RL-NL Module 은 <em>LR</em> 에서 구조적인 feature 들을 잘 따내고, <em>HR</em> 의 nature-scene 의 self-similarities 도\n잘 가져올 거라고 합니다.</p>\n<p>위치 상으로 역할이 SSRG 구조 이전/후에 사용되면서, high-level 에서 여러 넓은 범위의 정보들을 잘 catch 하는 module 이네요.</p>\n<p>논문에서 이전에 global-level non-local operation 을 사용 하는 거에 대한 한계점들을 들었는데요,</p>\n<ol>\n<li>unacceptable computational burden</li>\n<li>non-local operations at a proper neighborhood size are preferable for low-level tasks</li>\n</ol>\n<p>즉, global-level 에서 사용하면, feature size 가 큰 경우에 연산이 너무 heavy 해 진다는 단점과,\n주로 이런 연산들은 low-level, global-level 이 아닌 곳에서 사용되었다 라는 점인데요.</p>\n<p>그래서 이 논문에선, region-level 에서 해당 연산을 합니다. 즉 <em>k x k</em> 개의 regions 들로 나누고, 해당 region 들에 대해서 non-local operations 을 합니다.</p>\n<p>아래는 RL-NL Module 구조 사진입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 427px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/25c765a2afbd567cbbf5f7757717009b/a7c74/RL-NL-module.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABhklEQVQY001Q3UvCcBTdf9e/0FNRQZT0kAUVQSE99VIQWKlpUfpQgQmaH4UF4bc15/bbTN2c08k0P3KSmxPc1k+D6jxc7uXewzn3IO1WS5LlwUDudrv9fr8rirARRVFRFFmWxX/o9Xp0sVit8oJQ13WdIikEQ1HnlXPPZAIAlNkyAARN01yZ87jvsExG/4WmwaIMZEAQ53aH1XwUCgaRuZnZ5cUly8npY+iBwPEcRZGArHCVWDR64XCkU+kaz8Ox0fhIJZPoK7puXDEYps27G5eWY2Rt1RgKhiIvL/c+H4ZhOcimcmWWlSXpKRyOR2N1QWBLpWazFfQH/F7f9s7WgcvybJ1yHS4gZzZbIh6HOng2CysFSDyL0zRTFYQiwwyGQ+hWnXhudzqdVvPa492038bc84GbfQTGAMMY/6MoUIFlWchk8nkuEuVxgkskK4DkUJSLxVk005ek0Wj0JX7C+0KhgEiSNIljjL94hkMNJzS+ptGMXq2pmayaetPo0s9SVdUxOf/+DSatXWVeeQ36AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/25c765a2afbd567cbbf5f7757717009b/a7c74/RL-NL-module.png\"\n        srcset=\"/static/25c765a2afbd567cbbf5f7757717009b/12f09/RL-NL-module.png 148w,\n/static/25c765a2afbd567cbbf5f7757717009b/e4a3f/RL-NL-module.png 295w,\n/static/25c765a2afbd567cbbf5f7757717009b/a7c74/RL-NL-module.png 427w\"\n        sizes=\"(max-width: 427px) 100vw, 427px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"ssrg\" style=\"position:relative;\"><a href=\"#ssrg\" aria-label=\"ssrg permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SSRG</h4>\n<p><em>SSRG</em> 는 Local Source Residual Attention Group (LSRAG) 들이 share-source skip connection (SSC) 로 구성 되어 있습니다.</p>\n<p>LSRAG Module 은 <em>simplified</em> residual blocks (w/ local-source skip connection) 들로 구성이 되어 있고,\nfeature inter-dependencies 를 잘 구하기 위한, 이 논문에서 제안한 SOCA Module 들이 붙어 있습니다.</p>\n<p>특징은 다른 SR architecture 보다 light 하게 network 를 쌓았는데, 논문에서 residual blocks 을 깊게 쌓으면 여러 가지 문제가 있을 수 있다며\nLSRAG Module 을 기본 param 들을 사용했다고 했어요.</p>\n<p>그냥 이렇게 간단하게만 쌓으면 좋은 성능이 나올 수 없다고 하면서 share-source skip connection (SSC) 를 언급했는데요,\nSSC 사용으로 깊은 network 를 잘 훈련 시키면서 충분하게 LR 이미지로 부터 low-frequency 정보들을 잘 가져올 수 있다고 합니다.</p>\n<p>아래와 같은 convention 으로 쌓았는데, LSRAG <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span></span></span></span> 번 째 group 을 식으로 표현하면 이렇게 계산합니다.</p>\n<blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>F</mi><mi>g</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>S</mi><mi>S</mi><mi>C</mi></mrow></msub><msub><mi>F</mi><mn>0</mn></msub><mo>+</mo><msub><mi>H</mi><mi>G</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>G</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">F_g = W_{SSC} F_0 + H_G(F_{G-1})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">g</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">G</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.328331em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">G</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></p>\n</blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mi>S</mi></msub><mi>S</mi><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">W_SSC</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 는 convolution weight 인데 <strong>0 으로 초기화</strong>를 하고, 학습하면서 점차점차 shallow feature 들을 add 하는 방향으로 학습합니다.\nbias 는 false 네요 (<del>배우신 분</del>)</p>\n<p>결론적으로 SSRG structure </p>\n<p>아래는 LSRAG / SOCA Module 구조 입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bbef54344fd4bcff25dc6459dd094ce6/ffe34/LSRAG-module.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 14.18918918918919%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAIAAAAcOLh5AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAtUlEQVQI1w3B0Q+BQBwA4P7/Jy8988pmNk8ZY8VabcSMWLrTL1znrujkijLxfQolx64xGi7XyFsDBBihEC4R8ztjdeI6CW3PzYVtGEKImPMkjmPGsuxpzfTNaqH4+7k66PWdLWOQpHcAeIhHFIHrrcypdksCyzD14YQQQjknlFLGPBT0Ww1XayqZlNz3xW7/PKA8uuavony/z6cTxtixdZGmVVn95bmUYSghlPhYFEVdf+vv5weGEZwhhEqItwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/bbef54344fd4bcff25dc6459dd094ce6/fcda8/LSRAG-module.png\"\n        srcset=\"/static/bbef54344fd4bcff25dc6459dd094ce6/12f09/LSRAG-module.png 148w,\n/static/bbef54344fd4bcff25dc6459dd094ce6/e4a3f/LSRAG-module.png 295w,\n/static/bbef54344fd4bcff25dc6459dd094ce6/fcda8/LSRAG-module.png 590w,\n/static/bbef54344fd4bcff25dc6459dd094ce6/efc66/LSRAG-module.png 885w,\n/static/bbef54344fd4bcff25dc6459dd094ce6/ffe34/LSRAG-module.png 1055w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"second-order-channel-attention-soca\" style=\"position:relative;\"><a href=\"#second-order-channel-attention-soca\" aria-label=\"second order channel attention soca permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Second-Order Channel Attention (SOCA)</h4>\n<p>Introduction 에서 간단하게 설명은 했는데, 구체적으로 한번 다뤄볼께요.</p>\n<p>이전 channel-attention module 에서 <em>squeeze</em> stage 에선 <em>GAP</em> 등의 연산을 통해 정보를 압축했어요.\n즉, first-order statistics 이상의 정보를 고려하지 않았다는 것을 논문에서 언급합니다.</p>\n<h5 id=\"covariance-normalization\" style=\"position:relative;\"><a href=\"#covariance-normalization\" aria-label=\"covariance normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Covariance Normalization</h5>\n<p>Covariance Normalization 를 하는 이유는, 더 discriminative representation 을 학습하는 데 중요한 역할을 한다고 하네요.</p>\n<p>간단한 연산이니, 아래 공식만 적어두겠습니다.</p>\n<blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><mo>=</mo><msup><mi>σ</mi><mi>α</mi></msup><mo>=</mo><mi>U</mi><msup><mi>A</mi><mi>α</mi></msup><msup><mi>U</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\hat{Y} = \\sigma ^ \\alpha = U A^\\alpha U^T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\">^</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.664392em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord\"><span class=\"mord mathdefault\">A</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span></p>\n</blockquote>\n<p>결론적으로, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha &lt; 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> 일 때, eigenvalues 들이 non-linear 하게 잘 동작하고, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha = 1 / 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mord\">/</span><span class=\"mord\">2</span></span></span></span> 일 때 제일 좋다고 하네요.</p>\n<p>이후 Channel-Attention part 는 SE operation 하고 동일합니다.</p>\n<p>즉, 연산적으로 결론은 기존 <em>Global Average Pooling (GAP)</em> 대신 <em>Global Covariance Pooling (GCP)</em> 로 바꾼겁니다.</p>\n<p>이후에 GPU 에서 EIG 를 빠르게 구하기 위해서 <em>Newton-Schulz iteration</em> 을 했다고 하는데, 이 부분은 생략하겠습니다.</p>\n<h3 id=\"3-up-scale-module\" style=\"position:relative;\"><a href=\"#3-up-scale-module\" aria-label=\"3 up scale module permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. up-scale module</h3>\n<p>논문에서 여러 이유를 들려고 하는데, 결론은<em>bi-linear</em> up-sampling 을 사용했어요.</p>\n<h3 id=\"4-reconstruction-part\" style=\"position:relative;\"><a href=\"#4-reconstruction-part\" aria-label=\"4 reconstruction part permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. reconstruction part</h3>\n<p>이 부분도 특별한 점 없이 RL-NL Module 이후 convolution layer 1 층 쌓은 구조 입니다.</p>\n<h3 id=\"etc\" style=\"position:relative;\"><a href=\"#etc\" aria-label=\"etc permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Etc</h3>\n<ul>\n<li>Channel-Attention 때 <em>fc</em> 가 아닌 <em>1x1 conv2d</em> 로 projection 함.</li>\n<li>reduction ratio 는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r = 16</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">6</span></span></span></span></li>\n<li>나머지 convolution 들 <em>kernel size = 3</em>, <em>channel = 64</em></li>\n<li>LSRAG Module <em>group = 20</em></li>\n<li>RL-NL <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">k = 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span></li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">m = 10</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span></span></span></span> residual block, single SOCA module at the tail</li>\n</ul>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"set5-psnr-performance\" style=\"position:relative;\"><a href=\"#set5-psnr-performance\" aria-label=\"set5 psnr performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Set5 PSNR Performance</h3>\n<p>논문에서 제안된 각 module 들을 one by one 추가 했을 때 성능 차이를 보여주는데, 확실 하게 <em>FOCA vs SOCA</em> 비교만 봐도 성능 차가 있네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/529f5e57d9059b60dc1497705f50cafe/6b26f/SAN-psnr-performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAlklEQVQI1zWO4Q6DIAyEff+HHEiBIUgcjGW4gHqV7PtRLkev7eTck7HWGuvcAqwx4iFIkSaSUhKR0UZrDR/NwXskYOScJ1iklBQivVLvvfVev8ynlFrr76a1Br1tW37nUkpKCfU8Tw6reZ55A79xje0Pvo/jgBh1MMbt+85hv3i+1TE433sfY0Rdb6CHQMswsR+REALCF9zW37EsGo//AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/529f5e57d9059b60dc1497705f50cafe/fcda8/SAN-psnr-performance.png\"\n        srcset=\"/static/529f5e57d9059b60dc1497705f50cafe/12f09/SAN-psnr-performance.png 148w,\n/static/529f5e57d9059b60dc1497705f50cafe/e4a3f/SAN-psnr-performance.png 295w,\n/static/529f5e57d9059b60dc1497705f50cafe/fcda8/SAN-psnr-performance.png 590w,\n/static/529f5e57d9059b60dc1497705f50cafe/efc66/SAN-psnr-performance.png 885w,\n/static/529f5e57d9059b60dc1497705f50cafe/c83ae/SAN-psnr-performance.png 1180w,\n/static/529f5e57d9059b60dc1497705f50cafe/6b26f/SAN-psnr-performance.png 1658w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"urban100-psrn--ssim-benchmark\" style=\"position:relative;\"><a href=\"#urban100-psrn--ssim-benchmark\" aria-label=\"urban100 psrn  ssim benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Urban100 PSRN / SSIM Benchmark</h3>\n<p>이전에 나온 다른 논문들하고 성능 비교를 해 봐도 SAN 이 확실하게 성능이 더 좋은걸 볼 수 있네요</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/81d14ab312aeddb63b8b0db6f6f80384/3f20e/SAN-performance-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADYUlEQVQozwFWA6n8AMfL0ZqepoyNlZacpJCXn9TW2Z2hqnB2goKNnHN6h3V9jXuHmm50gXN/knqDlW91gn2KnXN6iXJ5h4CNogCKkJiLkZtdY26TmaOBiJPGys+UmKFdaXtrd4hyd4dmdYpteItsc4RjdItxeIprdYhoeI5yeIlodYpvfZMAmqe7fJCpSmB3n6q5iZu0xMzX0NLVoaqzsra8trnBpa64trm/rbK7oqqzuLvApq64q7G5trnApa23r7W8AGKBoDhiig4wUY+fr06BrKHD3sfHypGVn7C2wqWpsp2hqrG5xJudpZ+lsK+1vpSXnKy0vqSpsJebobG6xQBIbYkANVsAAAt0gZVIc5R8p8eQkJpPXHFdbodfZXdcan9fbYRZYXJZaoNfaXxYYnJeb4hdZnZaZ3pic4wAq7vGbH6LXFxhm6KsuMTRuMjXsbK6eYaXhpGekZWhgIydjJOfiZCefYqbkJSfho+ehZCfj5Wggo2di5OhAP///////////v/////+//////Lz9dbZ3Ors7enq7Nnc3vDx8+Hj5d3g4vHy9Nja3ebp6u3u8Nze4unr7QCSdW2fho9olKuKdWigbljMubDWz86rl5O7qqjEuLatmJXFt7W6q6iwm5jJvruxn5y6p6XGurerl5TDsq8ARCEeimJsH1R2SScTbB8AtJqMvbe1blBJgmRjoJKNbkxGlX99j312b0lFoZCOfmVefl1bo5aQcE9Jjm9sAFxDRKaDfyxVblM4KGwuDbailNHNzIt1cpmDgbmvq4Zsaa2cmKial4dqaLqtq5WDgJV7ebqwrYdvbKWMiQBmTUmKZmQ5T2JtSC9jLg6ym4/q5+e3q6rSyMfUzMu7r67a0c/FvLrAtLHa0tG5rq3KwL7Vzcy4rKvWzcoAf15YimtmWUxGWC4TZisHuJ+RwLm3eVxVjnJvpJWQeFdRn4qHlYJ8e1hVqJiVhWxlimxpqJmWel1WmXx6AKqXlJ6OjZaGdYt1ZJp3Y9HEur+6uHhcVoZnZqibmHBOS5uFgpiIgnBMSaiZloVuZ39eXauemnZZVJFybwD////5+Pjq6uv6+/z////////n5OTAuLfVzcve2djAtrbh2tnQy8nFvLrl39/Fvr3Rycff29rAt7bc09JTmfZXV+gJFQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/81d14ab312aeddb63b8b0db6f6f80384/fcda8/SAN-performance-benchmark.png\"\n        srcset=\"/static/81d14ab312aeddb63b8b0db6f6f80384/12f09/SAN-performance-benchmark.png 148w,\n/static/81d14ab312aeddb63b8b0db6f6f80384/e4a3f/SAN-performance-benchmark.png 295w,\n/static/81d14ab312aeddb63b8b0db6f6f80384/fcda8/SAN-performance-benchmark.png 590w,\n/static/81d14ab312aeddb63b8b0db6f6f80384/efc66/SAN-performance-benchmark.png 885w,\n/static/81d14ab312aeddb63b8b0db6f6f80384/c83ae/SAN-performance-benchmark.png 1180w,\n/static/81d14ab312aeddb63b8b0db6f6f80384/3f20e/SAN-performance-benchmark.png 1275w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"computational-and-parameter-comparison\" style=\"position:relative;\"><a href=\"#computational-and-parameter-comparison\" aria-label=\"computational and parameter comparison permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Computational and Parameter comparison</h3>\n<p>network 규모 대비 성능을 비교해 놓은 benchmark table 인데, 꽤 괜찮은 가성비(?)를 보이네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2df55a2302179629efc5c41cabdbd80b/63ec5/SAN-parameter-comparison.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.243243243243242%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAuklEQVQI1xVOyw6DIBD0/79NG2O1VkBgAQGFhban9mK6zmkyr0wDYBBzjLvWCrS2xsYYnduMAWsINmcM3kspvfcxxFprCJ6s8zyb3/f7eb8355SUmBP1AaAgkkKUCCnOumPfpVSlVL9twYdXrRRotFKIeB8GxljOqe97A4a8eRrneaG5rm2vOyGQpZSaxrsQopbymKbm1t1oY10FXUzHLji/vpXCl4VzkVOikLVXeXk+aVcwZgBI77r2DzPy1K0BWnrpAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/2df55a2302179629efc5c41cabdbd80b/fcda8/SAN-parameter-comparison.png\"\n        srcset=\"/static/2df55a2302179629efc5c41cabdbd80b/12f09/SAN-parameter-comparison.png 148w,\n/static/2df55a2302179629efc5c41cabdbd80b/e4a3f/SAN-parameter-comparison.png 295w,\n/static/2df55a2302179629efc5c41cabdbd80b/fcda8/SAN-parameter-comparison.png 590w,\n/static/2df55a2302179629efc5c41cabdbd80b/63ec5/SAN-parameter-comparison.png 812w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>뭔가 attention 끝을 보려고 하는 논문이네요.</p>\n<p>SISR task 도 요즘 light-weight 지향하는 trend 도 많이 보이고 있는데,\n이 쪽 연구 쪽도 재밌는 게 많이 나오면 좋겠네요.</p>\n<p>결론 : 기승전 attention</p>","excerpt":"TL;DR 이번 포스팅에서는 리뷰할 논문은 SAN (Second-order Attention Network) 이라는 Image Super Resolution task 에서 현재 여러 test set 에서 제일 높은 성능 (19년도 기준)을 보이고 있…","tableOfContents":"<ul>\n<li><a href=\"/SAN/#tldr\">TL;DR</a></li>\n<li><a href=\"/SAN/#related-work\">Related Work</a></li>\n<li><a href=\"/SAN/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/SAN/#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"/SAN/#1-shallow-feature-extraction\">1. shallow feature extraction</a></li>\n<li><a href=\"/SAN/#2-non-locally-enhanced-residual-group-nlrg\">2. non-locally enhanced residual group (NLRG)</a></li>\n<li><a href=\"/SAN/#3-up-scale-module\">3. up-scale module</a></li>\n<li><a href=\"/SAN/#4-reconstruction-part\">4. reconstruction part</a></li>\n<li><a href=\"/SAN/#etc\">Etc</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/SAN/#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"/SAN/#set5-psnr-performance\">Set5 PSNR Performance</a></li>\n<li><a href=\"/SAN/#urban100-psrn--ssim-benchmark\">Urban100 PSRN / SSIM Benchmark</a></li>\n<li><a href=\"/SAN/#computational-and-parameter-comparison\">Computational and Parameter comparison</a></li>\n</ul>\n</li>\n<li><a href=\"/SAN/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/SAN/"},"frontmatter":{"title":"SAN Second-order Attention Network for Single Image Super-Resolution","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["SISR"],"update":"Mar 14, 2020"},"timeToRead":6}},"pageContext":{"slug":"/SAN/","series":[],"lastmod":"2020-03-14"}},"staticQueryHashes":["2027115977","694178885"]}