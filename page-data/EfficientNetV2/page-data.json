{"componentChunkName":"component---src-templates-post-tsx","path":"/EfficientNetV2/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요.</p>\n<p>이번에 나온 논문은 <strong>효율성</strong>을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 comparable 하거나 더 좋은 성능을 달성했다고 합니다.</p>\n<p>paper : <a href=\"https://arxiv.org/pdf/2104.00298.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/google/automl/tree/master/efficientnetv2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">github</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요 논문과 관련높은 reference</p>\n<ol>\n<li>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a></li>\n</ol>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>최근에 나온 convolution 기반 architectures 를 보면 (e.g. ResNet-RS, NFNet),\n성능은 좋지만, 모델 파라메터가 너무 많고 FLOPs 도 엄청나게 커서 웬만한 장비 아니면 훈련하기도 빡센 문제가 있어요.</p>\n<h2 id=\"training-efficiency\" style=\"position:relative;\"><a href=\"#training-efficiency\" aria-label=\"training efficiency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Efficiency</h2>\n<h3 id=\"training-with-very-large-image-sizes-is-slow\" style=\"position:relative;\"><a href=\"#training-with-very-large-image-sizes-is-slow\" aria-label=\"training with very large image sizes is slow permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training with very large image sizes is slow</h3>\n<p>큰 크기의 이미지를 사용하면 작은 batch size를 사용해야 하는 점이 속도 저하의 원인임을 언급하면서,\n훈련 시에 progressively 이미지 크기를 조정하는 방향으로 이런 문제를 개선했다고 합니다.</p>\n<h3 id=\"depthwise-convolutions-are-slow-in-early-layers\" style=\"position:relative;\"><a href=\"#depthwise-convolutions-are-slow-in-early-layers\" aria-label=\"depthwise convolutions are slow in early layers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Depthwise Convolutions are slow in early layers</h3>\n<p>EfficientNet architecture에는 <em>MBConv</em> 라는 block 이 있는데, depth-wise convolution 이 사용되죠.\n그런데, 요 연산이 tpu/gpu 에서 제대로 가속을 못 받아서 일반적으로 사용하는 convolution 연산보다 파라메터나 FLOPs 는 작지만 속도가 느려요.</p>\n<p>최근 연구들에는 이런 문제때문에 <em>Fused-MBConv</em> 라는 걸 만들었는데,\n아래 그림처럼 <code class=\"language-text\">Conv 1x1 + depthwise Conv 3x3</code> -> <code class=\"language-text\">Conv 3x3</code> 으로 replace 한게 더 좋다는 연구를 언급하면서</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAABaklEQVR42o2TiZKCMBBE+f9vs/wALV0U5ZTDg1uY5Y2LG69lUzVFJk06nZmOJW9GWZZyPB6lbVsNc9RNLVmWSdu8YgzreaGuKonjWPI817wa8r7vb1hdK1YUxT2fJGyaRlzXlTAMVQH5SHi9XsX3fQmCQMnAJglRxJVRwSbCVMg6wT+TCruu01OpURRFL4Rg1DYMI51DOqnwdDrJbrdT0mdCsO12K2ma6U2IPwnZeDgcZDabyXw+V5VmDVGXJLGcz+f7gZOEeV5oUzabjXbaVIgiz/MUR+2/mgIJyqgTc1MhXQ8CXwmLotR8xD77cFBk27Y4Qx0hMwlpWpqmN2P/WOqjwl9rVEOdErlcLrqJTkJkGhts7DL7TJUvCgEdx1EDD9nDYSjb7/eyWq10/m5Y4wugCdSGTq7XayXlRUCwXC7VSuSQ2faX2mexWOgX1Q+EKBifG9fBOvxEvbg+RKzdbJNogLNOA7HRSPgNdrpCYEMsiQYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MBConv\"\n        title=\"\"\n        src=\"/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png\"\n        srcset=\"/static/ca0824dfec86d1879498e934bc24af2c/12f09/mbconv.png 148w,\n/static/ca0824dfec86d1879498e934bc24af2c/e4a3f/mbconv.png 295w,\n/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png 590w,\n/static/ca0824dfec86d1879498e934bc24af2c/efc66/mbconv.png 885w,\n/static/ca0824dfec86d1879498e934bc24af2c/c83ae/mbconv.png 1180w,\n/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png 1622w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>EfficientNet-B4 에 gradually <em>Fused-MBConv</em> 를 적용해 봤는데, <strong>early layers</strong> (1 ~ 3 stages) 에만 적용하는게,\n속도도 빠르면서 성능도 제일 좋게 가져갈 수 있었다고 합니다.</p>\n<h3 id=\"equally-scaling-up-every-stage-is-sub-optimal\" style=\"position:relative;\"><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\" aria-label=\"equally scaling up every stage is sub optimal permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Equally scaling up every stage is sub-optimal</h3>\n<p>EfficinetNet 에선 compound scaling rule 에 따라서 scaling 하는데, 만약 depth coef 가 2라면, 모든 stages 에서 2로 scaling 합니다.\n그런데, 각 stages 에서 훈련 시간과 파라메터 수는 equally contributed 안하는 문제점을 들면서, <em>non-uniform</em> 한 scaling 전략을 선택하겠다고 합니다.</p>\n<p>이미지 사이즈 같은 경우도 훈련 시간과 memory 에 큰 영향을 주기 때문에, (image size에 대한) scaling rule 도 변견했다고 합니다.</p>\n<h2 id=\"training-aware-nas-and-scaling\" style=\"position:relative;\"><a href=\"#training-aware-nas-and-scaling\" aria-label=\"training aware nas and scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training-Aware NAS and Scaling</h2>\n<h3 id=\"nas-search\" style=\"position:relative;\"><a href=\"#nas-search\" aria-label=\"nas search permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NAS Search</h3>\n<p>모델 훈련 속도를 위한 best combination 을 찾기위해, training-aware NAS 을 제안합니다.</p>\n<p>EfficientNet에서 사용한 NAS 기반을 했는데, 아래와 같은 목표를 joinly optimize 했다고 합니다.</p>\n<ol>\n<li>accuracy</li>\n<li>parameter-efficiency</li>\n<li>training-efficiency (on modern accelerators)</li>\n</ol>\n<p>구체적인 settings 은 논문에</p>\n<h3 id=\"efficientnetv2-architecture\" style=\"position:relative;\"><a href=\"#efficientnetv2-architecture\" aria-label=\"efficientnetv2 architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Architecture</h3>\n<p>NAS를 사용해서 찾은 architecture (EfficientNetV2-S, baseline) 가 아래와 같은 구조라 합니다. EfficientNet 과 크게 4가지 차이점이 있다 하는데,</p>\n<ol>\n<li><em>MBConv</em> 와 <em>Fused-MBConv</em> 를 섞어서 씀</li>\n<li>더 작은 expansion ratio (for <em>MBConv</em>) 를 사용 -> 더 적은 overhead 를 가지기 때문</li>\n<li>3x3 kernel sizes 를 선호. (하지만 작은 receptive field를 사용하는 만큼 layer를 더 쌓게 됨)</li>\n<li>EfficientNet 에 있던 맨 마지막 stride-1 stage 를 제거. -> 이것도 메모리 때문에</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABkUlEQVR42k2SiZKbMBBE+f9v89oVzCGMDiRACzG3kViudRp7K8mUikJo3kxPC4cxyhm736soIkqlnLM0TbMsr+53xphUMsvzeZ5vtwhRlGVRFEhLkkRr7QRB4HlXJRM/CMIwRCGKRWPfD0IS+q8oynr6+glrLWpN04SnsyyL53mERE3T9F2vlLpcLoSQqqr6vkf5j9NJcBFFN/SglAohAt+XUu777jyfz65tq6pGJby3TU2iCLrXdd22bTSmbuph6Cdr8zzrhwei6zo8kXzAaA4xbdtwIdDTdd0MQ2uNBXn2Fd/f+++ynOflrRzID4y9MQYwSN/3zuezSg/nYAyOhmF4w59aA4ZAa81/8HTAnzr/5V4ZjeGqSGRECOPcThNg9N+2FXKwBT2Oj/kvjDPMJgU/fZylTOA8YLgY32Jj7TiOSFiXhXOBtGWZYeQ/GKpgRKqke71yzmEpvCVhSGP6GEecHvC6JIk05rgnGPZ214HjqNS2LUbKshQ2QR5+D1xJnmvcwtteGI7vx3W+ArPs+/4HwjEmKDlFvgwAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"EfficinetNetV2-S\"\n        title=\"\"\n        src=\"/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png\"\n        srcset=\"/static/25eda3ffc1b887abae2f1aaa5829a298/12f09/efficientnetv2-s.png 148w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4a3f/efficientnetv2-s.png 295w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png 590w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png 591w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"efficientnetv2-scaling\" style=\"position:relative;\"><a href=\"#efficientnetv2-scaling\" aria-label=\"efficientnetv2 scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Scaling</h3>\n<p>위에서 만든 EfficientNetV2-S 기반으로 M/L 버전도 만들었는데, 몇 가지 제한을 두고 scaling 했다고 합니다.</p>\n<ol>\n<li>maximum inference image size to 480</li>\n<li>add more layers to later stages (stage 5, 6)</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB9UlEQVR42m2Ti7aiMAxF/f8f1FFA3qhIoSj0BTIb6nVmuW6QkqQ9yUlSd8uyTNNkrUVZXuuPZ13cIkJRVmV6S7tOrpu4f1YvO97nJuvGvD32tTSLrWzXdb3p53kGMG2CgvkPjG2M8ZkBLu1iL7YVrTTSvZxP9ZHvzEbrLMvGcVw3utdQDo1p1KRW4s5dr5csy+NzfL1eb7eaY0opVh9ih5Hn+TRPnK7Pda97CrbGcgI6kNJaSSlbIagCs+8fspc/mY0hs7baStuWLS4fGwUselney7J8PNamjOMAhaZpfAvXzHVd04Zn/kSHKisbZhPoWTsTi+QAhBB930vZD8P4BodRqKRqL62ZzDis9ehNhmEQG1vOYP7SMKIWRTGasVOdVpp5+OaDBPYBfMRP7t0w3gfyfBhn9JpAA8YBTw+b/5PvOfNSJxhfLSuRkK/Z/j5nf8PowdoM2UMVZZuQVlvfUSiNKmgVCqQ8hTfYx1Oj8gXX9S0Mo6LI0zRlisyJi6C3eM5ZP0UP2RXs5IUQDZ8kSYqyBBaGYZbncRzjCYIgy7Pqcqn4l1RVIxp27/c7Be7SJImTmCS4jscjB4LghIk3iiJ0QoRBsN/vz6sZ/DkeMQ+HQ9OIXVVVcDvhgmsUhWHAaTwop9OJq0wtJHHWOue/Fgdzcc79BTn6lEp3R8F/AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"acc_vs_step\"\n        title=\"\"\n        src=\"/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png\"\n        srcset=\"/static/dfccbfac89e8eb29126044fee0fdf98a/12f09/accuracy_vs_training_step.png 148w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/e4a3f/accuracy_vs_training_step.png 295w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png 590w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png 740w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"progressive-learning\" style=\"position:relative;\"><a href=\"#progressive-learning\" aria-label=\"progressive learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Progressive Learning</h2>\n<p>훈련 시 image size 를 dynamic 하게 사용하는데, 이전 연구들은 accuracy drop 이 발생했다고 합니다.\n이번 논문에선 그런 accuracy drop이 <em>imbalanced regularization</em> (다른 이미지 크기로 학습하면 거기에 맞는 regularization strength를 사용해야 한다)에서 오지 않을까 추측합니다.</p>\n<p>아래와 같이 regularization strength를 실험해 본 결과, 추측한 대로 image size 가 작을 땐 weak augmentations, 클 땐 strong augmentations이 성능향상에 더 도움됐다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABCElEQVR42jWPbXODIBCE/f+/rkkxCoIgiqhRCPiaZpJ005neh2OHub19LpGyokxYaxljeZ5LKZWqjGmEEFrruq71X3EhINu2083N2NgP8zwfidJjmqmyFOnlAjNWMEpzSgn5TtOUUsq5SAnBdudcCME5P00T9PP5TB6Pn3Vdtm3b/2tZFnTTNJxzgIQQq6qCRvfeF59iYHm/38n92LMMgYWSkvOCMgbg4zi0ruBEBPgwiqiINTEaY/Cs6/oxH/v2dToTQrAwyzICVMa8c23b4uAQwzLPp/PJdh38AMZREDfvP+Z924CENFmWSBalUEphCITXcRynEXf2XQ+Jn2G4IsPabhiG1+v1CxuSQ9W9I6iLAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"regularization_strength\"\n        title=\"\"\n        src=\"/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png\"\n        srcset=\"/static/5411322c7d21461bb1aafd0796cd7304/12f09/regularization_strength.png 148w,\n/static/5411322c7d21461bb1aafd0796cd7304/e4a3f/regularization_strength.png 295w,\n/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png 590w,\n/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png 813w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Progressive Learning 을 하기 위해서 fomulation을 세웠는데, 아래와 같습니다.</p>\n<p>전체 훈련을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> steps를 하고 훈련 과정을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> stages로 나눴고, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span> 는 regularizations 종류 (e.g. RandAugment, MixUp, Dropout, ...)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABb0lEQVR42lVQ226CQBT0///IF9Qmpm1o5aILyG0pyB0FXeQu7UjpQyeBTHZ35sycBc/z69Wa47jN5oUQcgAIOU4QRVFVNVUBiCCAq0dNk0RRluXdbheG4aIsS6Ioruf5vh/4QRRFQRCcz+c0TdI0Bf89wS1IHMcgTdPUddX3/WIYhhXH8R+flmnqumHoOqa5rguB53mmaT7nEqJpmk2pbdvQf/9hAYPlcvn69m5ZFiLjqaYdkdBx6GG/R2bUwBXOTcNQFBWOVVXN4rZpttttnCRIJkkSSJalqIQX1+v1PqHruilqzVhZN00/DLMYH2ojHrUpWl8uF8ZuWZZBiYSwwJ/dblgPxOM44v04YRYzxlAJqSzLRq0ojLCwPM+TJIERysML2yqKApyVLIqeRrP4fi/l/QHF/NMJERzHieMEo0CwW0ppmmZf1PaDENt9PB5oMU9u21YUBEmS87zAXdd2Xd/hEN7thO4f0PcJrHkYhh/mmOjnGtln3QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"progressive_learning\"\n        title=\"\"\n        src=\"/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png\"\n        srcset=\"/static/88a8f124fae964c40aa0bce818b45469/12f09/progressive_learning.png 148w,\n/static/88a8f124fae964c40aa0bce818b45469/e4a3f/progressive_learning.png 295w,\n/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png 590w,\n/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png 603w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>각 모델은 아래와 같은 recipes 로 훈련했다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 526px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABW0lEQVR42jVQ2XKDMAzk/78sfQgNZyCY04AvwNw0BNJuO1OPRyPLK+2uDN/3TNPMi+Lue5btBEHwCMMoimzHsSyLkNg0rygmMXE972p+eq6LOqJUytBd1yjVdbptWylllmVd1wnB/5K2rmuttX+/V3WFRAiJIi3LYRiWZTG+/8/7/UYEQkqBWdM0Nk0zjiNjHLlqGhTBpKRc1o0ztr9eBnikEKAVUm7bhqQo8oJSMECtUg0hBBhKaZqmUIQnJuJL694o8vzjcnEcF+YBxZWCV3WNQZ7nTdOEnm1dAUvSdBx0kmTHcQA8z7MBV7ZtR4TY1o3EMeO/IudlOc8DsrftCxF24AXK4XMYR1hkjD33Hc2VdbslSWJbVlUzbAIfmLrvT84F0HVVHedZlhTN/dBL1YAZctZ1+5XtOE4QBo9HmGU5hMTgZ0xwnqYZEuCUUlg+bJeU5nmBvYBM6/4H6JOxLIh72iAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"recipe\"\n        title=\"\"\n        src=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n        srcset=\"/static/8fa0df98e27bce0117600108343ea1d2/12f09/progressive_learning_recipes.png 148w,\n/static/8fa0df98e27bce0117600108343ea1d2/e4a3f/progressive_learning_recipes.png 295w,\n/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png 526w\"\n        sizes=\"(max-width: 526px) 100vw, 526px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"benchmark\" style=\"position:relative;\"><a href=\"#benchmark\" aria-label=\"benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Benchmark</h2>\n<h3 id=\"imagenet\" style=\"position:relative;\"><a href=\"#imagenet\" aria-label=\"imagenet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet</h3>\n<p>accuracy, training speed 측면에서 EfficientNet 대비 다 좋네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAC1ElEQVR42l2UeXOiQBDF+f7fKftnks3pqpzKKTAMDANeYDyS/SFmU7WWhdjVr/u91z1jBEFgmqZj24vFYj6fu67DX8temtYyCEUmdKXaNCtzIUWei6JI4sT3/TRdLTzPsC1rOp3OZrMB4jhzc26aMwKW5RB8f3+bz2aTyTvf97e3p6fnyWTy+vJCAl2N7Xa72WySJJnN51EUxYmIV0Wey81mTSvP84iRR7cojvIsi+Mkz/nNeBqfn59fX19N05gW5B0/rPyozkVDsOs6KQutdVVVPNu2JUiny+XCy263u4HbtvE8d7n041WdibWQ7el86buuKkvdNIBBrtdrMnn+Dy7Lcjb947q+7Uk/VJRo2+5w6CFYlhVOKaW0rslE5ggGaByPp4+PD2pPpxPbCS1XOp6o1GakjTbAQuTogjBB+J/P5xuYkJSybQbaYZQnaZOkerPtRjCMmqal7eFwOJ1OY/Cn80hb17VlWn4wgOOVOl1rkycKoeqaEnVd03yUOoLhfwMrBe0/pp15QR3GFW4N4L5jJkUh4zguikJViiAvI5haP267ru16sR8UUSLX2x48nQeMqoUQpNKKzOPxOEKwHcOOGMBIHMdZsHKLwFuKZSAZtdYNVsmyZB+QDR7ZlMBgUAPtGwetHcf1Q+GHJZqzXB8OH9TGy9FtxoFskmtVAz6fqXMyuv2eko2ubQYVxlEsopXORLvZHvb7fVlKDEM2yHHD/hk2uI0wpsX0bNvyFpHlCndZqnrb96frnFN6X1cF/GBY3/cjGLHG+TJMBVaWxaly40Sym3mh2/WOI8OSwBzNOEcOVBEPHuYQMaDB3qVp+uvu7vH3EwKoRW1Y7fY7DlZVKdweRq3UdS7taDh8DfaBMrB6fLjnVNq2ve+G9fo+VRJDkI2j66vm7bfm4WBAiWPM7fHwcP/y8vz6+ua67uL64W7gPmCE3DA8yeFU4yv5+Cel/As/txaieypb4gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"performance\"\n        title=\"\"\n        src=\"/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png\"\n        srcset=\"/static/e901cbbd3e0d1a76a8c62b654920a072/12f09/performance.png 148w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/e4a3f/performance.png 295w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png 590w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png 802w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABGElEQVR42j2P646EIAyFff/n05hMMmuMLqOAyr2AqHucmWx/lPbraWkr7/1JMWtnrDnOI+7ROpsDZeTO7KVQSp7CLYspKq2tSznFnXLOlVLK8I2LxQarrEo5g2i5jQOTxinjraVApLV2UvXDxOTqQyil7Ptercu6aCXVdl1XKQc8fpkEl8rlvJ/neRw33LaViWU1Dvk/rKTkMbpSEAOeeLDhsnBo3uT6QCmF9/YrOr/KKqXonIkxGnN7XIKp3ruUEhFKmIvDD3hrNQTWGJQ+MjSnEALnc9d1Ukq0ERFIjDRN0zzPSLFLCMS5+B1Hxhg45xy8ej6fTdPUdd227ePxQNTUTdf9QDS+7fViwzCw121IMQK+73shxB9t/42bxIys3AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"efficiency\"\n        title=\"\"\n        src=\"/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png\"\n        srcset=\"/static/e5c498ab87386813999ea34af6583e5b/12f09/efficiency.png 148w,\n/static/e5c498ab87386813999ea34af6583e5b/e4a3f/efficiency.png 295w,\n/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png 590w,\n/static/e5c498ab87386813999ea34af6583e5b/efc66/efficiency.png 885w,\n/static/e5c498ab87386813999ea34af6583e5b/c83ae/efficiency.png 1180w,\n/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png 1510w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"transfer-learning-performance-comparison\" style=\"position:relative;\"><a href=\"#transfer-learning-performance-comparison\" aria-label=\"transfer learning performance comparison permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transfer Learning Performance Comparison</h3>\n<p>다른 datasets 에 transfer learning 했는데, 성능이 comparable 하네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.72972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABAElEQVR42iVQ2ZKEIAz0/z/P8hrX+0IYgiPqKHhtu5uHLkLSR8Up8iIMQ9/3gzAIgL4fRZHneVmWpWkKaLuu73vGWIvqnhJCAMdxdNZ1VUphXDedpI8a9Lwsxmz4Zw+NvYUgKUEgIqVIkvygxvG+b+c4DmutIgqC10/K04IYH67zxExwjvVlWaA+DAOstm2Tb7n91XVdjjEGQkQySbI4YXHSVg0tX2PsiahwW78rFrTWQHBgs+87LP+dd2hLKeM4rqoK8RCehllPa885CPM8A2ENf7wRAkJoHjJ6cOq6cV23LEu0E4Z6nCZd1zVOJATv2jbL86IscbI8z7GGc1prfwGE80tNl7cOfwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transfer_learning\"\n        title=\"\"\n        src=\"/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png\"\n        srcset=\"/static/814e99c33821f04e492f5dee4c8b8c87/12f09/transfer_learning.png 148w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4a3f/transfer_learning.png 295w,\n/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png 590w,\n/static/814e99c33821f04e492f5dee4c8b8c87/efc66/transfer_learning.png 885w,\n/static/814e99c33821f04e492f5dee4c8b8c87/c83ae/transfer_learning.png 1180w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png 1496w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>새로운 아이디어보단 여러 가지를 조합하고 training recipe 실험에 가까웠지만, 갠적으로 이런 튜닝 성격의 연구도 좋아하고, 엄청난 개선이 있어서 재밌게 읽었습니다.</p>\n<p>결론 : 굳굳</p>","excerpt":"TL;DR EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요. 이번에 나온 논문은 효율성을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 compara…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#training-efficiency\">Training Efficiency</a></p>\n<ul>\n<li><a href=\"#training-with-very-large-image-sizes-is-slow\">Training with very large image sizes is slow</a></li>\n<li><a href=\"#depthwise-convolutions-are-slow-in-early-layers\">Depthwise Convolutions are slow in early layers</a></li>\n<li><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\">Equally scaling up every stage is sub-optimal</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#training-aware-nas-and-scaling\">Training-Aware NAS and Scaling</a></p>\n<ul>\n<li><a href=\"#nas-search\">NAS Search</a></li>\n<li><a href=\"#efficientnetv2-architecture\">EfficientNetV2 Architecture</a></li>\n<li><a href=\"#efficientnetv2-scaling\">EfficientNetV2 Scaling</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#progressive-learning\">Progressive Learning</a></p>\n</li>\n<li>\n<p><a href=\"#benchmark\">Benchmark</a></p>\n<ul>\n<li><a href=\"#imagenet\">ImageNet</a></li>\n<li><a href=\"#transfer-learning-performance-comparison\">Transfer Learning Performance Comparison</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/EfficientNetV2/"},"frontmatter":{"title":"EfficientNetV2 - Smaller Models and Faster Training","date":"Apr 02, 2021","tags":["Deep-Learning"],"keywords":["Classification","EfficientNet","NAS"],"update":"Apr 02, 2021"},"timeToRead":2}},"pageContext":{"slug":"/EfficientNetV2/","series":[],"lastmod":"2021-04-02"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}