{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/EfficientNetV2/",
    "result": {"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요.</p>\n<p>이번에 나온 논문은 <strong>효율성</strong>을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 comparable 하거나 더 좋은 성능을 달성했다고 합니다.</p>\n<p>paper : <a href=\"https://arxiv.org/pdf/2104.00298.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/google/automl/tree/master/efficientnetv2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">github</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요 논문과 관련높은 reference</p>\n<ol>\n<li>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a></li>\n</ol>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>최근에 나온 convolution 기반 architectures 를 보면 (e.g. ResNet-RS, NFNet),\n성능은 좋지만, 모델 파라메터가 너무 많고 FLOPs 도 엄청나게 커서 웬만한 장비 아니면 훈련하기도 빡센 문제가 있어요.</p>\n<h2 id=\"training-efficiency\" style=\"position:relative;\"><a href=\"#training-efficiency\" aria-label=\"training efficiency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Efficiency</h2>\n<h3 id=\"training-with-very-large-image-sizes-is-slow\" style=\"position:relative;\"><a href=\"#training-with-very-large-image-sizes-is-slow\" aria-label=\"training with very large image sizes is slow permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training with very large image sizes is slow</h3>\n<p>큰 크기의 이미지를 사용하면 작은 batch size를 사용해야 하는 점이 속도 저하의 원인임을 언급하면서,\n훈련 시에 progressively 이미지 크기를 조정하는 방향으로 이런 문제를 개선했다고 합니다.</p>\n<h3 id=\"depthwise-convolutions-are-slow-in-early-layers\" style=\"position:relative;\"><a href=\"#depthwise-convolutions-are-slow-in-early-layers\" aria-label=\"depthwise convolutions are slow in early layers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Depthwise Convolutions are slow in early layers</h3>\n<p>EfficientNet architecture에는 <em>MBConv</em> 라는 block 이 있는데, depth-wise convolution 이 사용되죠.\n그런데, 요 연산이 tpu/gpu 에서 제대로 가속을 못 받아서 일반적으로 사용하는 convolution 연산보다 파라메터나 FLOPs 는 작지만 속도가 느려요.</p>\n<p>최근 연구들에는 이런 문제때문에 <em>Fused-MBConv</em> 라는 걸 만들었는데,\n아래 그림처럼 <code class=\"language-text\">Conv 1x1 + depthwise Conv 3x3</code> -> <code class=\"language-text\">Conv 3x3</code> 으로 replace 한게 더 좋다는 연구를 언급하면서</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAABYUlEQVQ4y42T3ZKCMAyFef8384ZLuGAUFEdUQPxDQNA4X5w4XdfF7UynbdKcniSnnjjjfr/rWte1HA4HuV6vOl1f0zSy3+8/+hieGcx4uVwkz3MNZLRtO+r7CsilzWYjRVHIMAzSdd3L1/e9bLdbBcU+ytBWXj+fzzoBdwE5n04nLQn38I0CGqOiyKUsyx+ATPbYYc/eUv8D8Gnc7XayXC4/MsSXJIk2DLDRGjJut5vWZzKZiO/7UlWV1sn8x+NRU2aaCr4C8jJNWa/XvxjSZdivVivN4F+y4WUCALQg8wGeZZn6v3bZZRHHsQYhE5chGSBqUrcGvse/ALnM2raNdpI6WSctgMfQIYDYXZ/heC7dJ4tBptOp5Hkh74Mux7OZRFGk5fj0bT0TLJdIE3bz+Vz31It9GIaSpqkqYLFYaGNYgyDQ8qCGF0MTNMGAkQZpIV4uUjPssOPMytkEzpnyGOADxddBSugpD7YAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MBConv\"\n        title=\"MBConv\"\n        src=\"/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png\"\n        srcset=\"/static/ca0824dfec86d1879498e934bc24af2c/12f09/mbconv.png 148w,\n/static/ca0824dfec86d1879498e934bc24af2c/e4a3f/mbconv.png 295w,\n/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png 590w,\n/static/ca0824dfec86d1879498e934bc24af2c/efc66/mbconv.png 885w,\n/static/ca0824dfec86d1879498e934bc24af2c/c83ae/mbconv.png 1180w,\n/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png 1622w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>EfficientNet-B4 에 gradually <em>Fused-MBConv</em> 를 적용해 봤는데, <strong>early layers</strong> (1 ~ 3 stages) 에만 적용하는게,\n속도도 빠르면서 성능도 제일 좋게 가져갈 수 있었다고 합니다.</p>\n<h3 id=\"equally-scaling-up-every-stage-is-sub-optimal\" style=\"position:relative;\"><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\" aria-label=\"equally scaling up every stage is sub optimal permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Equally scaling up every stage is sub-optimal</h3>\n<p>EfficinetNet 에선 compound scaling rule 에 따라서 scaling 하는데, 만약 depth coef 가 2라면, 모든 stages 에서 2로 scaling 합니다.\n그런데, 각 stages 에서 훈련 시간과 파라메터 수는 equally contributed 안하는 문제점을 들면서, <em>non-uniform</em> 한 scaling 전략을 선택하겠다고 합니다.</p>\n<p>이미지 사이즈 같은 경우도 훈련 시간과 memory 에 큰 영향을 주기 때문에, (image size에 대한) scaling rule 도 변견했다고 합니다.</p>\n<h2 id=\"training-aware-nas-and-scaling\" style=\"position:relative;\"><a href=\"#training-aware-nas-and-scaling\" aria-label=\"training aware nas and scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training-Aware NAS and Scaling</h2>\n<h3 id=\"nas-search\" style=\"position:relative;\"><a href=\"#nas-search\" aria-label=\"nas search permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NAS Search</h3>\n<p>모델 훈련 속도를 위한 best combination 을 찾기위해, training-aware NAS 을 제안합니다.</p>\n<p>EfficientNet에서 사용한 NAS 기반을 했는데, 아래와 같은 목표를 joinly optimize 했다고 합니다.</p>\n<ol>\n<li>accuracy</li>\n<li>parameter-efficiency</li>\n<li>training-efficiency (on modern accelerators)</li>\n</ol>\n<p>구체적인 settings 은 논문에</p>\n<h3 id=\"efficientnetv2-architecture\" style=\"position:relative;\"><a href=\"#efficientnetv2-architecture\" aria-label=\"efficientnetv2 architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Architecture</h3>\n<p>NAS를 사용해서 찾은 architecture (EfficientNetV2-S, baseline) 가 아래와 같은 구조라 합니다. EfficientNet 과 크게 4가지 차이점이 있다 하는데,</p>\n<ol>\n<li><em>MBConv</em> 와 <em>Fused-MBConv</em> 를 섞어서 씀</li>\n<li>더 작은 expansion ratio (for <em>MBConv</em>) 를 사용 -> 더 적은 overhead 를 가지기 때문</li>\n<li>3x3 kernel sizes 를 선호. (하지만 작은 receptive field를 사용하는 만큼 layer를 더 쌓게 됨)</li>\n<li>EfficientNet 에 있던 맨 마지막 stride-1 stage 를 제거. -> 이것도 메모리 때문에</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABsklEQVQoz03Q2Y6cMBAF0P7/X0u6FejBxoABg9law+LdxRr1JBPlPtVDHV1V3bKMJkkyjiPGuCzKnNKa103TvIYhTbOiKKqq8gCUvtf6r9R1zVjZcH6LSRyGQVnkCCEUIUppmmZZmnx8RBjjKIrCMOTty1nnvXfOGWP+DOu63s7jwAghHM/zrJRsOH88HhjjaZqklMMwPO73PM8JIcGvgBCSF+XzGeZ5vu/77bouJeU4Ts6567rmeSKE1JwDwLZtxtplWZRSsELfdUoZY4xSShtzXdcbb9tmnVuWmWaUxDgIAv51Nufce2+tdc6d5/l6vQBWAHDOwbr+xQCgtRbLQkiCELrf76yqyqJgrPLeSymNted59l3n4R1rLQB8Yw/amKFvgyCk72clRckIIVmWGWuVUtbafd+bhhtj1/Xd5P9h7702hlfsx88HY6yuGKtqhCJCEmOM1to6t29bWZZK63VdhRD/YeekVLyuw+eTUpplGY5JHOMkTYWUzjlr7bZtjDGtNQAIIcD7Nz6OQwgxz/Mw9G3b9n3fdV3JWMVY33Wf46iUWpZFCNG27ThN4jvHcfwGm20mZgmdIvcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"EfficinetNetV2-S\"\n        title=\"EfficinetNetV2-S\"\n        src=\"/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png\"\n        srcset=\"/static/25eda3ffc1b887abae2f1aaa5829a298/12f09/efficientnetv2-s.png 148w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4a3f/efficientnetv2-s.png 295w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png 590w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png 591w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"efficientnetv2-scaling\" style=\"position:relative;\"><a href=\"#efficientnetv2-scaling\" aria-label=\"efficientnetv2 scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Scaling</h3>\n<p>위에서 만든 EfficientNetV2-S 기반으로 M/L 버전도 만들었는데, 몇 가지 제한을 두고 scaling 했다고 합니다.</p>\n<ol>\n<li>maximum inference image size to 480</li>\n<li>add more layers to later stages (stage 5, 6)</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACHUlEQVQoz3WRW3OrIBRG8/9/Xqc9NeoZjYlGRcI1wIYNnFHanvah64Fhhlnsy3fKORtjtNY55xTTTk7Z5UyyGtXCFmG4Bx8PUkoxxvzJKefsvbfW7nJOCVIiyS2OCaaD3j/6ZP/28/yQvfeEEPM0+4NNcYiUU+pozHsFztg8zytZrbXOOUT8ITvnrtdr8D7nrG6KcgoZcszgwDorhFBKrus6jeP1elVKCS4ppR8yAAzD4LxLkMiFYEYMqLQKISCic0DIY11XY/bWEFFKZYzxR7FdXleCCWEBxVXKyTxNWYy1Nsb0fAalpDrYCJmm6X6/042mlE7O2stwAQ38yn301lhELGYI4VCk1lpK6ZwLIeRvnIwxfd+DBS65AwcAXyal1BgTQvi+5xJYuZ9ijHSjQomnf4KDEhsAMMbKYEX7NWdEBABELHkUExG/Z/tV7UdUe0JqH6kMZq19MFZaxYMYIyJ67wH2vrz3pYUPOeccQpBSAnilFGOP6X4nhCzzsswLpXSeZ2utVso5xzn/X7lk4ADkgdZ6XZe6aW63W9/3wzB0XTeOo9bPAjhgjFtr96jatmnadpqmuq7fXl+7vu+6rmh939V13Ry8vf05n6uqqpqmeXl5qeuzEOI0juPlcqmq96Zp2r/t5dKfz+eu69q2fa+qZV2lEI/Hg5BNSMEYI9smpZznWQh5UlIKzh8HG9mWZRFCSCm3bSOElJ3/xj/8jZR6xVa5hwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"acc_vs_step\"\n        title=\"acc_vs_step\"\n        src=\"/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png\"\n        srcset=\"/static/dfccbfac89e8eb29126044fee0fdf98a/12f09/accuracy_vs_training_step.png 148w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/e4a3f/accuracy_vs_training_step.png 295w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png 590w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png 740w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"progressive-learning\" style=\"position:relative;\"><a href=\"#progressive-learning\" aria-label=\"progressive learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Progressive Learning</h2>\n<p>훈련 시 image size 를 dynamic 하게 사용하는데, 이전 연구들은 accuracy drop 이 발생했다고 합니다.\n이번 논문에선 그런 accuracy drop이 <em>imbalanced regularization</em> (다른 이미지 크기로 학습하면 거기에 맞는 regularization strength를 사용해야 한다)에서 오지 않을까 추측합니다.</p>\n<p>아래와 같이 regularization strength를 실험해 본 결과, 추측한 대로 image size 가 작을 땐 weak augmentations, 클 땐 strong augmentations이 성능향상에 더 도움됐다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABEElEQVQY0y2O23KDIBQA8/8f17RVlDtCIso5xQuYZjR20prpvu/snrgwjCnv26qq6rrWShljnHOc88Y2ztnmhdbmern4Lrh28iEB5tttPTXui3GnlaKUcc6FEFIIxnhFCKWUMaa1IYT0fT8MwzzPQ4wIMI7j8/k8/ez7tq3btj1ebNtjXdd937331jrv/f1+v1yv1toQQs5Za900DQAeckrz+f2DEML5USvLklK2LAtnTEoFADFGKSUixhfW2hhjSumQv2/L2/lcFJ+MsbIoPouC1nSaphD6ruumac45F2UZAgzjiIiCc0T8304pCSmPGWOkUlpr6xwiAkDf94g4xNi2bQghxth1nZTCew8Af/IvID9DVB91w9AAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"regularization_strength\"\n        title=\"regularization_strength\"\n        src=\"/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png\"\n        srcset=\"/static/5411322c7d21461bb1aafd0796cd7304/12f09/regularization_strength.png 148w,\n/static/5411322c7d21461bb1aafd0796cd7304/e4a3f/regularization_strength.png 295w,\n/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png 590w,\n/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png 813w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Progressive Learning 을 하기 위해서 fomulation을 세웠는데, 아래와 같습니다.</p>\n<p>전체 훈련을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> steps를 하고 훈련 과정을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> stages로 나눴고, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span> 는 regularizations 종류 (e.g. RandAugment, MixUp, Dropout, ...)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABgElEQVQoz1WP23KqMBRA/f8/6vRBfDi17YyWCCQQDUZpLo2GFoTQEE7sgPah62lfZu3LbL1aBcE8CILFYgEhTJI4SSDOMowxAAClKRqBYRimCGUZBiCMomj99vbO2MwYAyE8HI+cMca5lJJzrrVWJ3U6nfhY+eB8bAoplFLvjHUTfd/PvPfBfP788pqTXYbxFuMEQkopY6woit12TJM4ThHKJ6SU119m3v9/fHz49/SUExJFEYQQpdl4C6VxHCOI4iQhhCCEtluMUFoUx7Zt77JzbrlcCiGF4GADhJSlLlerdWtMc2m6rjPGOOd621trW2Ps9/cwDHf5er0aYwghdJ9zzsuyrOtal7qqaqVU13VKqculFkIYY26On7jLbdNmWUZ2O7Lfj18Jedb66+tz0i5CCK21lLKqKsZ5M1Vug6bNbbOJYpLnnDFCckrph1JCyMOBMsb3lJ7P5yOljIthGLz3zjl/O9taC0AIwKYsP4dhsNb2/fieMcZO9H9xE7fgB3Lg6WBEeV6xAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"progressive_learning\"\n        title=\"progressive_learning\"\n        src=\"/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png\"\n        srcset=\"/static/88a8f124fae964c40aa0bce818b45469/12f09/progressive_learning.png 148w,\n/static/88a8f124fae964c40aa0bce818b45469/e4a3f/progressive_learning.png 295w,\n/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png 590w,\n/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png 603w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>각 모델은 아래와 같은 recipes 로 훈련했다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 526px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABcklEQVQY002P23KiQBRF+f8vS9AxCqIQFIS+nb5xafrYmljN1GRqqma97fW0V3I6FWmaNm1zPhXb7a88z4vj8Xw+7/b7zTYtq2qzSQ+HrKrKojim6eaw3+92H3mWEcIS7/3yH0KIaZrmeaaUeu+11ojYNA1jHBGHYUC/AEjv8Y6YrP+IMa7r6pyTErQxiAgA3iNjbFkWY4yxdpomABHCgzP2eDwSay3pCUiglCLiOI6UkLZt52kuikIqfW2uSinOWV3XRqvq83N2ripLKWUCAO/vb3l+PJ0KLoRSarCWcf56vcqydG4hhNzvdyXhcm0Q/a3rYox1Xc+zS6wxWZa1t1t22F+uDUiJf4LCGuM4jiE8jDExxsFapXUIAb1f1xUAQgiJVmq3+6jrOs+zrifLsjDGxnF8fX8xxrz3lNLn8wlCCADnZv5z6tbenFsSIXhZVpfLpW2bvu8BoOs6IYTWuvuZfd9rrRljlFLBOSHUDvav/A1ESbIGi8ugMAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"recipe\"\n        title=\"recipe\"\n        src=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n        srcset=\"/static/8fa0df98e27bce0117600108343ea1d2/12f09/progressive_learning_recipes.png 148w,\n/static/8fa0df98e27bce0117600108343ea1d2/e4a3f/progressive_learning_recipes.png 295w,\n/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png 526w\"\n        sizes=\"(max-width: 526px) 100vw, 526px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"benchmark\" style=\"position:relative;\"><a href=\"#benchmark\" aria-label=\"benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Benchmark</h2>\n<h3 id=\"imagenet\" style=\"position:relative;\"><a href=\"#imagenet\" aria-label=\"imagenet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet</h3>\n<p>accuracy, training speed 측면에서 EfficientNet 대비 다 좋네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADAklEQVQ4y12UW4+jOBCF8///0va87Myo1er0JBBsAoRrsLENvpAAzgXikQH1SlsPfnF98jmnSt6kaQqAB30YBKF7cH3fBxB4MPS8IE6rM+KsbnHFccWqqmKUFkWRJHFZllEUbSAAjuO4rgsADMMAeN7BdQ8H7+D5B8/b7/8ACFxnv9/tttvt9vNzv99/fm5d143jeNO2rZIqzzPHcYuiyHKUF7hERClZlqXv+0maRnOdTqciz9M0tW1ZViK0eb1exphLqzzPC8PwlNA44yWWxhitNaVUSsnqWgihlLKdl8s4TcaYvu+/4TY4BnGS5OemxKqq1DS9tB4oY0opxmo1lzFGtWpa4K5bYSm44zi+Hx5DkmQ8zRsh+9tNY4xqxjDGTdNwzueX2wV5vV6b51xSCme/98AJBCQ80YpY2cOgEULMwuhbNud8kW3h6/WCcSWljKIwzVCa8zMSqh1meGCrbHa/31erXT9N9mVjzCq7VRJCeIpLXKk0bx7PcYGrquKcM8oE51LKJafpf4FZ2a7jwXNayFPCnitsZVNK8zwnhDRNY4whhDxHe6uUWuHr5QoACII8zVmckmt3G8eX1npmaoSwEOJ6vRpj7vf7OqDLxQY2jqOUEgCYxKcwjKO4ihOKiZSqxRjXdY0QEpwvQ+r7/vF4jOOz67rNYqBVyvf9U4KimKZ5gyqh9V2pllIyp40ZY03DjXk1TX2738fRzmijtW5tWTjLiizHRSlQpS5XvaTNuSCECLEG1nXdf2kPw6BUK6WEEIRR6gfVMSJC9v3wGPQ657IsKaXLkmg9rEsyTZtxjk4KASE8HoOsYAhLjLlUXd93CCFCKcKIEFLXzTRNVvbt/nw8bGB930spUVm+/fP28fEhhJi3ZzTm1XWdXc+6tp4pXUYlhXg8ntM0WXi03h9ciPf3dwjtl6C1XizdbremrlvVci5apdr2YmUP/XI7DMOmPJ/DMNztdm8/fvz78+fv37/cwwFCCOZP4uvry57bL9d1drvd8Xh0HCcMo7pmCKG/LLMVLhqVldAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"performance\"\n        title=\"performance\"\n        src=\"/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png\"\n        srcset=\"/static/e901cbbd3e0d1a76a8c62b654920a072/12f09/performance.png 148w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/e4a3f/performance.png 295w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png 590w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png 802w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABH0lEQVQY0yWMWW6EMBBEuf8JByIRDYNjE4y3brd3O4K8vyq9qgm9Hz4kDQ5cbTWkYJ0rnpK2BkwuhUJEQiTfKQZltHMhxZBDynmy2thDH6dEQocul2yMMafath/lyDhEjCFGa4099brxQxmKodZaS53UpZS1BmCM0WobY3hEcf5qF2qp/WGMoa5LyMt5Gnfurd3lpJTMybd2S7XW1hoiaiNvpbfW2lN2KWUMfozxr9V6/045Z0SIKQFgSqmU0nsnwpRijJGISinPRQGw6dYgxphzvschBO89Y2xe5vM8rbUAYK31RJ/PZ9938h4AEJFz8XlgbOecE9H0fr9fr9eyLOu6rt/rPM/L17JtG38QD4wxIQ4hjn3fLymF4IwxpdQfJbKNrD5WcrUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"efficiency\"\n        title=\"efficiency\"\n        src=\"/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png\"\n        srcset=\"/static/e5c498ab87386813999ea34af6583e5b/12f09/efficiency.png 148w,\n/static/e5c498ab87386813999ea34af6583e5b/e4a3f/efficiency.png 295w,\n/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png 590w,\n/static/e5c498ab87386813999ea34af6583e5b/efc66/efficiency.png 885w,\n/static/e5c498ab87386813999ea34af6583e5b/c83ae/efficiency.png 1180w,\n/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png 1510w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"transfer-learning-performance-comparison\" style=\"position:relative;\"><a href=\"#transfer-learning-performance-comparison\" aria-label=\"transfer learning performance comparison permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transfer Learning Performance Comparison</h3>\n<p>다른 datasets 에 transfer learning 했는데, 성능이 comparable 하네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.72972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABDUlEQVQY0x2OW5aCMBAF2f/2lEEYwxAkgXTozktMgiBzsP7r1i26rrvd6rIsq6r6KU/qur5er4yxtm0Zuwspx2mcpkmIQUophQCAcZREVOSciAhA9Y9hRkfGP59LzjmlpEBJKbXWiLMCRUSIOM+z/XIcR7Hv+7quzrnqVjf3iXUoRtq37TgOxBk0vF4vImPIeO9TSogYY0wpfT6fYnu/rbXGEGPtLxubu+C9PtvrrkErBSkla6133jkXYzTG5Jy37/pZXpbFGGqahnOulJrRID1diAAaEZdlsdaSOQkhGHNeQMRTjjH2fS+EuFwubduGEIL33rsQvJTiMQwAIKVs//4454MQXddxzhljMaZ/qsxLdKnzMIsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transfer_learning\"\n        title=\"transfer_learning\"\n        src=\"/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png\"\n        srcset=\"/static/814e99c33821f04e492f5dee4c8b8c87/12f09/transfer_learning.png 148w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4a3f/transfer_learning.png 295w,\n/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png 590w,\n/static/814e99c33821f04e492f5dee4c8b8c87/efc66/transfer_learning.png 885w,\n/static/814e99c33821f04e492f5dee4c8b8c87/c83ae/transfer_learning.png 1180w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png 1496w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>새로운 아이디어보단 여러 가지를 조합하고 training recipe 실험에 가까웠지만, 갠적으로 이런 튜닝 성격의 연구도 좋아하고, 엄청난 개선이 있어서 재밌게 읽었습니다.</p>\n<p>결론 : 굳굳</p>","excerpt":"TL;DR EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요. 이번에 나온 논문은 효율성을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 compara…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#training-efficiency\">Training Efficiency</a></p>\n<ul>\n<li><a href=\"#training-with-very-large-image-sizes-is-slow\">Training with very large image sizes is slow</a></li>\n<li><a href=\"#depthwise-convolutions-are-slow-in-early-layers\">Depthwise Convolutions are slow in early layers</a></li>\n<li><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\">Equally scaling up every stage is sub-optimal</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#training-aware-nas-and-scaling\">Training-Aware NAS and Scaling</a></p>\n<ul>\n<li><a href=\"#nas-search\">NAS Search</a></li>\n<li><a href=\"#efficientnetv2-architecture\">EfficientNetV2 Architecture</a></li>\n<li><a href=\"#efficientnetv2-scaling\">EfficientNetV2 Scaling</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#progressive-learning\">Progressive Learning</a></p>\n</li>\n<li>\n<p><a href=\"#benchmark\">Benchmark</a></p>\n<ul>\n<li><a href=\"#imagenet\">ImageNet</a></li>\n<li><a href=\"#transfer-learning-performance-comparison\">Transfer Learning Performance Comparison</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/EfficientNetV2/"},"frontmatter":{"title":"EfficientNetV2 - Smaller Models and Faster Training","date":"Apr 02, 2021","tags":["Deep-Learning"],"keywords":["Classification","EfficientNet","NAS"],"update":"Apr 02, 2021"},"timeToRead":4}},"pageContext":{"slug":"/EfficientNetV2/","series":[],"lastmod":"2021-04-02"}},
    "staticQueryHashes": ["2027115977","2744905544","694178885"]}