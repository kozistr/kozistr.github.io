{"componentChunkName":"component---src-templates-post-tsx","path":"/EfficientNetV2/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요.</p>\n<p>이번에 나온 논문은 <strong>효율성</strong>을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 comparable 하거나 더 좋은 성능을 달성했다고 합니다.</p>\n<p>paper : <a href=\"https://arxiv.org/pdf/2104.00298.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/google/automl/tree/master/efficientnetv2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">github</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요 논문과 관련높은 reference</p>\n<ol>\n<li>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a></li>\n</ol>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>최근에 나온 convolution 기반 architectures 를 보면 (e.g. ResNet-RS, NFNet),\n성능은 좋지만, 모델 파라메터가 너무 많고 FLOPs 도 엄청나게 커서 웬만한 장비 아니면 훈련하기도 빡센 문제가 있어요.</p>\n<h2 id=\"training-efficiency\" style=\"position:relative;\"><a href=\"#training-efficiency\" aria-label=\"training efficiency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Efficiency</h2>\n<h3 id=\"training-with-very-large-image-sizes-is-slow\" style=\"position:relative;\"><a href=\"#training-with-very-large-image-sizes-is-slow\" aria-label=\"training with very large image sizes is slow permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training with very large image sizes is slow</h3>\n<p>큰 크기의 이미지를 사용하면 작은 batch size를 사용해야 하는 점이 속도 저하의 원인임을 언급하면서,\n훈련 시에 progressively 이미지 크기를 조정하는 방향으로 이런 문제를 개선했다고 합니다. </p>\n<h3 id=\"depthwise-convolutions-are-slow-in-early-layers\" style=\"position:relative;\"><a href=\"#depthwise-convolutions-are-slow-in-early-layers\" aria-label=\"depthwise convolutions are slow in early layers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Depthwise Convolutions are slow in early layers</h3>\n<p>EfficientNet architecture에는 <em>MBConv</em> 라는 block 이 있는데, depth-wise convolution 이 사용되죠.\n그런데, 요 연산이 tpu/gpu 에서 제대로 가속을 못 받아서 일반적으로 사용하는 convolution 연산보다 파라메터나 FLOPs 는 작지만 속도가 느려요.</p>\n<p>최근 연구들에는 이런 문제때문에 <em>Fused-MBConv</em> 라는 걸 만들었는데,\n아래 그림처럼 <code class=\"language-text\">Conv 1x1 + depthwise Conv 3x3</code> -> <code class=\"language-text\">Conv 3x3</code> 으로 replace 한게 더 좋다는 연구를 언급하면서</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAABZElEQVQ4y42T246CUAxF+f8/82F81QQVQQEFvCOgopXVSQnjGPQkDef0srt7wZHOeTwe+j2fz3I4HOR6vap0bWVZyn6/f2vjOKYwZVEUkqapBnKqquq1fQTEab1eS5ZlUte1XC6X1na73SRJEgVF38vQvmTP81wF8C4g79PppC3BD1sv4P1+VyfYbTabP4AId/RZI9yt9N6St9utLJfLtwyxzedzHRhgvT00hvRnMBjIz3Aou91O+2T24/GoJSO2BR8BmSRDWa1W/xhig30URVrBV2tDZgIAtCCzAR7HkYSNnWRf76HneRKGka5JlyEVsNSUbiv1Gt8C4sy3qkqdJH2ySVoAydhDANF3bYbjdOn+sqhlOp3qcF4PU57NZuK6bpMwl3e/rWMLO5lMtHew831f73Ec6308HstisdAkQRDoYHiPRiNdI7ahZciFfhAMGGVQFsuNIz1Dj/CGpb3x4U17DPAJwYdBSwqjlkgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MBConv\"\n        title=\"MBConv\"\n        src=\"/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png\"\n        srcset=\"/static/ca0824dfec86d1879498e934bc24af2c/12f09/mbconv.png 148w,\n/static/ca0824dfec86d1879498e934bc24af2c/e4a3f/mbconv.png 295w,\n/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png 590w,\n/static/ca0824dfec86d1879498e934bc24af2c/efc66/mbconv.png 885w,\n/static/ca0824dfec86d1879498e934bc24af2c/c83ae/mbconv.png 1180w,\n/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png 1622w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>EfficientNet-B4 에 gradually <em>Fused-MBConv</em> 를 적용해 봤는데, <strong>early layers</strong> (1 ~ 3 stages) 에만 적용하는게,\n속도도 빠르면서 성능도 제일 좋게 가져갈 수 있었다고 합니다.</p>\n<h3 id=\"equally-scaling-up-every-stage-is-sub-optimal\" style=\"position:relative;\"><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\" aria-label=\"equally scaling up every stage is sub optimal permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Equally scaling up every stage is sub-optimal</h3>\n<p>EfficinetNet 에선 compound scaling rule 에 따라서 scaling 하는데, 만약 depth coef 가 2라면, 모든 stages 에서 2로 scaling 합니다.\n그런데, 각 stages 에서 훈련 시간과 파라메터 수는 equally contributed 안하는 문제점을 들면서, <em>non-uniform</em> 한 scaling 전략을 선택하겠다고 합니다.</p>\n<p>이미지 사이즈 같은 경우도 훈련 시간과 memory 에 큰 영향을 주기 때문에, (image size에 대한) scaling rule 도 변견했다고 합니다.</p>\n<h2 id=\"training-aware-nas-and-scaling\" style=\"position:relative;\"><a href=\"#training-aware-nas-and-scaling\" aria-label=\"training aware nas and scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training-Aware NAS and Scaling</h2>\n<h3 id=\"nas-search\" style=\"position:relative;\"><a href=\"#nas-search\" aria-label=\"nas search permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NAS Search</h3>\n<p>모델 훈련 속도를 위한 best combination 을 찾기위해, training-aware NAS 을 제안합니다.</p>\n<p>EfficientNet에서 사용한 NAS 기반을 했는데, 아래와 같은 목표를 joinly optimize 했다고 합니다.</p>\n<ol>\n<li>accuracy</li>\n<li>parameter-efficiency</li>\n<li>training-efficiency (on modern accelerators)</li>\n</ol>\n<p>구체적인 settings 은 논문에</p>\n<h3 id=\"efficientnetv2-architecture\" style=\"position:relative;\"><a href=\"#efficientnetv2-architecture\" aria-label=\"efficientnetv2 architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Architecture</h3>\n<p>NAS를 사용해서 찾은 architecture (EfficientNetV2-S, baseline) 가 아래와 같은 구조라 합니다. EfficientNet 과 크게 4가지 차이점이 있다 하는데,</p>\n<ol>\n<li><em>MBConv</em> 와 <em>Fused-MBConv</em> 를 섞어서 씀</li>\n<li>더 작은 expansion ratio (for <em>MBConv</em>) 를 사용 -> 더 적은 overhead 를 가지기 때문</li>\n<li>3x3 kernel sizes 를 선호. (하지만 작은 receptive field를 사용하는 만큼 layer를 더 쌓게 됨)</li>\n<li>EfficientNet 에 있던 맨 마지막 stride-1 stage 를 제거. -> 이것도 메모리 때문에</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABjElEQVQoz01R23KrIBTN///aqWnD0SiiAkK8Tb0HxahJV9JOpzw4m2Fdt4cojsMwrOva933BRRLHSqvL5VKVJWMR5zxNU7ss8QtWvI5SSkpx0foQUErISfDkfD573hkgcCLGXNeDnOd5hBCdVfM0W2vneTbGfA+32+2w75sPnu83TdP3vdb6/XgEDVmGYYDP0XGSJIEtOREWMiGE67oJ5+u2HR6PxzgMdd1ADHPbNpRSpfWyLOu6mmnqum4cx+W2FEU+jjA2uF6vV4CfZICmJ6hF5iDwP04ntEJtpEBCPEH3fr9XVWWhsSy44vtDxgSlvusoDdHAcRyZpoJzKVOQER7+IBd5js3hQO4P2YJsyiKDZxwx1ONC0oAyxoBDSHz3bUMSYybsCU72lwz5qzE6lf/ejlJKlcJSY/coj4aAIve2rlgVuoKMvVr7S55nZNNKEfIftaMo8gOK8mHI+mFAQzhjL9CFEAK/yPZJ3vcdl7Zty7LMsgzF8iwHDvYYPz9rxMbCgcFr/fqd3wfEL4zIJlJ1lHiMAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"EfficinetNetV2-S\"\n        title=\"EfficinetNetV2-S\"\n        src=\"/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png\"\n        srcset=\"/static/25eda3ffc1b887abae2f1aaa5829a298/12f09/efficientnetv2-s.png 148w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4a3f/efficientnetv2-s.png 295w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png 590w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png 591w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"efficientnetv2-scaling\" style=\"position:relative;\"><a href=\"#efficientnetv2-scaling\" aria-label=\"efficientnetv2 scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Scaling</h3>\n<p>위에서 만든 EfficientNetV2-S 기반으로 M/L 버전도 만들었는데, 몇 가지 제한을 두고 scaling 했다고 합니다.</p>\n<ol>\n<li>maximum inference image size to 480</li>\n<li>add more layers to later stages (stage 5, 6)</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACAklEQVQoz3VT25KrIBDM/39eanMqXlIxJsYLoiCogArbSLIn+7BdFWqE6bn15OCcm+dZSgnDbtbDWaecI04UomY1n5jRZtuBR5zujQN+xhjwPdlZq60lVtWq571cpA/0hg/7Pl9kzjkhZBon3NnZbvnWsY4qujmfgfV9VVUNaRBdKbWu6y9yWVb3+30xBh/iISij2mkQtdKzmoeBCzE0TfMsCrgJITkbWkpf5Kap8zxXRqFgciOrW9dlFVIsy4I8WmtCOpCnaYI3boZBwDZ7skNH8UZWu+pGCyZwhbd9bhalYjxSLmhtAEmItm2fz2dZlrSlcDhUZXXLb1pqdmdmM/M0Izw4CIHkwmOAFiCjZ9y4DxxQ8zW76lmzgSk0qjWYyAk/Suk4jjA+5xwEC/YBUVEDF3xcRjUrNIMTIbquC40Fwp86h8HgDHrA7vsen5/a/mT7JdU+Et+SP4UAGTmDnusPts18IJTwygygMfC1NphP33cYKDanrmoAnWNP/ArvoRlj/zMHDVDrsAP5oXycJI/HI8syP87rtSgKKccAzLTvGWJ5qc7nc5KmUC+O49PX1zXL4B1oMHGZ7Did/kXR2TsnyfF4jOMI4nupADykaXq5XPL8BgLIl0saRVHdNHDCFNAFHzgGSUiLAtEIFsf/MThneKa0a0mLJsM+YZlACDP/C98GAZNQRWNOMwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"acc_vs_step\"\n        title=\"acc_vs_step\"\n        src=\"/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png\"\n        srcset=\"/static/dfccbfac89e8eb29126044fee0fdf98a/12f09/accuracy_vs_training_step.png 148w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/e4a3f/accuracy_vs_training_step.png 295w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png 590w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png 740w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"progressive-learning\" style=\"position:relative;\"><a href=\"#progressive-learning\" aria-label=\"progressive learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Progressive Learning</h2>\n<p>훈련 시 image size 를 dynamic 하게 사용하는데, 이전 연구들은 accuracy drop 이 발생했다고 합니다.\n이번 논문에선 그런 accuracy drop이 <em>imbalanced regularization</em> (다른 이미지 크기로 학습하면 거기에 맞는 regularization strength를 사용해야 한다)에서 오지 않을까 추측합니다.</p>\n<p>아래와 같이 regularization strength를 실험해 본 결과, 추측한 대로 image size 가 작을 땐 weak augmentations, 클 땐 strong augmentations이 성능향상에 더 도움됐다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABA0lEQVQY0zVPCXKEIBD0/4/LJvEEUQQVBYKLV6w1bpl2K2mqhmPoYwKac0IKpdokSdIkLYuCcy6lpJRWopJSVC+UJW/qWnW6br3Sk7Hzuj6CSn4RKsHJMgJCzrByQmgSxxmeCAEtjuO+74dhGMdxGJw1Zrjfz/MMns9j3x/7vv/8A5fjOLquE0IopbZta5pGCqG1nucZuRBEG3uRp2m8vX9AG7Zwi6IozciyLJRQxgpjjHOOMWatdS9AEXWapov8vS5vt1sYfiJhhC0M0zTz3mvdw9z7EW5hFGl9RYVETinqX2xo5IyVZVlxDischJRowxNzXv+ca9sWmWEIOcZyzIIuyL8ptENdiBN+vQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"regularization_strength\"\n        title=\"regularization_strength\"\n        src=\"/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png\"\n        srcset=\"/static/5411322c7d21461bb1aafd0796cd7304/12f09/regularization_strength.png 148w,\n/static/5411322c7d21461bb1aafd0796cd7304/e4a3f/regularization_strength.png 295w,\n/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png 590w,\n/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png 813w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Progressive Learning 을 하기 위해서 fomulation을 세웠는데, 아래와 같습니다.</p>\n<p>전체 훈련을 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> steps를 하고 훈련 과정을 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span> stages로 나눴고, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 는 regularizations 종류 (e.g. RandAugment, MixUp, Dropout, ...)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABcElEQVQoz1VRaW+CQBTk//+jxqRAk2raRgU5lmuRQ+6jCoJyxHZA+qFDWOAtM29mH8O+shzLchzH87yqqooiK4pq6LphGKIoEk0jGiFEFQQBD5RFUZAkabffn4KA8X0fHNfzwiAIwjCO4zAMi6JIszTLsnCqJFixGUVRmqbg3Gb0fc94nvvG8x+fXzalhmFapkmI5rou/j75vmVZKtEURdE1zZ4B9Z8/MEebrlYv7+s1yPADF0TTJy+OI8syUQliUErh2TQN6Pq+1zTNQqbU2mw2URTjEg9iFMdlWW63u6Ztr/UV9tq27fuh7/quu6PY3e/jOC5k3NiGPVhC6KIo66pC5svlgoSgJklSVRUCN0375DxmLGTY0HWdWhY9HqdUUZwXxfn8DXJd16BBC1Ehh4O4zhX0W8i3tpFkZe4cUGo7jpOkKYK4rgMv+MzzHEeAWQzjiJ7DMDyetjEEccIBURGm6zrMACu0uxn9fwwzni+/QSDnwmuj13MAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"progressive_learning\"\n        title=\"progressive_learning\"\n        src=\"/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png\"\n        srcset=\"/static/88a8f124fae964c40aa0bce818b45469/12f09/progressive_learning.png 148w,\n/static/88a8f124fae964c40aa0bce818b45469/e4a3f/progressive_learning.png 295w,\n/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png 590w,\n/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png 603w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>각 모델은 아래와 같은 recipes 로 훈련했다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 526px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABXElEQVQY01WQW3OCMBCF+f+/rAVUCFJA7oSEhJsSieDI2B7t9KH7kNlkd0/Ot8Z+Z5ummeVZ8OXb9s7zPP94DILg4DiWbYZRZFmm65IoCn3/aJqW6zj7/cEjhFJmXP9H0zSXy2Waprqu53nuuk5rnWcZYxzJOI56vgoh51nftDa+/+L5fOJUSkkpur5HTQiBJsYYRPtXDNAVolmWlTO2rquBGq0oBl5faX0+n2tK8zzH577vy7ZNs7RtW85ZHMd910an06QUKKSUBnHdz88P4nlg5rxB3zgMjPNt28IwVOpKKdW3WytFkqRaz0VRwmMSx9OkjDAICCF5URDXSdJUSKlfQAsw4AIOYRfdUITusiyogg5EyA14w/ZgyfNIWVHgAQRj2+OBBDsDznq/i+YVSk38baooCpgysjQNwyhJkjzPqqqCZFmW6Ou7rnxf8YidQwgqDeeU1sM4/D7+AF6EsO27aDWDAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"recipe\"\n        title=\"recipe\"\n        src=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n        srcset=\"/static/8fa0df98e27bce0117600108343ea1d2/12f09/progressive_learning_recipes.png 148w,\n/static/8fa0df98e27bce0117600108343ea1d2/e4a3f/progressive_learning_recipes.png 295w,\n/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png 526w\"\n        sizes=\"(max-width: 526px) 100vw, 526px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"benchmark\" style=\"position:relative;\"><a href=\"#benchmark\" aria-label=\"benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Benchmark</h2>\n<h3 id=\"imagenet\" style=\"position:relative;\"><a href=\"#imagenet\" aria-label=\"imagenet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet</h3>\n<p>accuracy, training speed 측면에서 EfficientNet 대비 다 좋네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAC00lEQVQ4y0VU2ZKiMBTl/39pul9mqqunq+xSFJB9k4RNQgBZRNQ5IVZPHlJI7slZ7kXFtixd0yzLchz3iGWauq5rumUcPdeLwyAmhIZh5Hq+bdue5zqOg6og8F3XVQzD2O/3h8NB1w3btoA8iKVp+lHTdXW3FQXqTt3tvrE2G1VVN5vvw/7geZ7SNA2veRSFqrqP4ziMSBTThGSc10mSgMQPAmddoIqjKAgCURaGCSHK4/F4Pp9twzVNgzDXz72wSmiNl+M45nle13VRloxVnHNR2bbLsuCh7/sfcGOZluf70el8opxm/H5/CHBRAFMWJdRJMG9wdBfgy+UFrlkFM8ejbdqZH1ZBdGZ1P00jpbQsS+zn87mqKklzXyG4Qrmtq64ZItEMV7cy283TTMgehpEQUhQFwIwxyYwrpGywKl3X4gzGHMcOQhJE1Ykw3gwreChW2div1+vyUtvD0XNdL9lIHC1xvYSmDTTPt0WC0zQFFcCsqkAgc3p5/glMyN6rmhEHce36xe0FFrIReBRFWZbBNl7iQZ5C0QvctR3Gw7KiICq8IOsu07KItFdMiSGD567rUAn9rwa1rQgMAUASJsz3XIyg46Wen9Osrnkj0wY/ZMsmQe08z8tyu6BV0kDDOYbJ9Ynj5fBMUjaOV87bPM9k2tjRLGQMIdN0BR9YFWiDeswowGGI8aRxwkjK224cRpF2VTGIx4TJwED4P21ECjAODEO3neBopaaTYUL6Yca9ss8YcsQmh2QcB+n5gSGRHa8ZQ6tM0wrjgtCa0qrmXd9fAM7ynFAC8rI8w+Mqe7rNswgMAYCWJMnbr7e/X1+Qt04PgnhAIaVkHc+0yHPZKtBc5xm3CLCwPs8VY5+fnyC3LBtqpaXrNJ3LsuENbK+5tEL20EvZ8KskpxO+xO12+/b+/vvPn4+PD/wz4Ba0HZ8Kvn+xb773exU1pmniJ+rLsoCjf/ShFGnpAUINAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"performance\"\n        title=\"performance\"\n        src=\"/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png\"\n        srcset=\"/static/e901cbbd3e0d1a76a8c62b654920a072/12f09/performance.png 148w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/e4a3f/performance.png 295w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png 590w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png 802w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABEUlEQVQY0x2P2Y6DMAxF+f8vLFRqBVMmYQjZbWejjMl5iKLj6xtliAAXUDbBB9/ORpmc9xUwG2eDLbUipYgxInwxkbbGe8qJCuVSBmes28y2K0746HMtls2ul+VXe7Q+xpiIkrPW7/Y1i+0wmLC1Vmsb9KG1c8aH67rOdvIJEeT+Zzy12r43t9THIdXhAS8W1/c8vywHrVXJcPbY2YkQjVF3pBt+hKNKqUTACyxbhy9DKSWGkHIOIZaca61sESHnxCAim15RQ/D5jgX2vHUvExEArOs6TuO+7845HvMJiJ8OT9nw14WQP511/QghuHeY5/nxeEzT9GLe73Ecn89pWRbRkR2ulnJjuOtQSkrBRmv9DyDdja8+IwLqAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"efficiency\"\n        title=\"efficiency\"\n        src=\"/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png\"\n        srcset=\"/static/e5c498ab87386813999ea34af6583e5b/12f09/efficiency.png 148w,\n/static/e5c498ab87386813999ea34af6583e5b/e4a3f/efficiency.png 295w,\n/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png 590w,\n/static/e5c498ab87386813999ea34af6583e5b/efc66/efficiency.png 885w,\n/static/e5c498ab87386813999ea34af6583e5b/c83ae/efficiency.png 1180w,\n/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png 1510w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"transfer-learning-performance-comparison\" style=\"position:relative;\"><a href=\"#transfer-learning-performance-comparison\" aria-label=\"transfer learning performance comparison permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transfer Learning Performance Comparison</h3>\n<p>다른 datasets 에 transfer learning 했는데, 성능이 comparable 하네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.72972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA/UlEQVQY0x1Q6ZqEIAzz/V9PXY8VRx2rAgIL3o7HxukP6EeaNMQriiKOkzAMoyj6CZ9KkiQIAsZYnueMZdQ0bdd2XUdUNygiIUTbNlprb9tWXELw6l33ymrjxnHa8LquXHAMSymV6tFjTKHr+79v3fftnee577u1NoqTNOtYoajV53EAw6SQYp5nrY2BqnNQBH9ZFjTXdXnH5wMZgLD5y9o0o7KSz+79lEJyLjCHAWcdFoBmjIGv46v+bJ6mCeQ0Tcuy5Jz3yig92mERAoYVUJCxGzUMA05YwPtDhlhVVUTk+z4SAjwAdBYXwnnXNeLBz/PXC9I1EQJGgzjh/R+qzEt0nk0bEAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transfer_learning\"\n        title=\"transfer_learning\"\n        src=\"/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png\"\n        srcset=\"/static/814e99c33821f04e492f5dee4c8b8c87/12f09/transfer_learning.png 148w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4a3f/transfer_learning.png 295w,\n/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png 590w,\n/static/814e99c33821f04e492f5dee4c8b8c87/efc66/transfer_learning.png 885w,\n/static/814e99c33821f04e492f5dee4c8b8c87/c83ae/transfer_learning.png 1180w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png 1496w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>새로운 아이디어보단 여러 가지를 조합하고 training recipe 실험에 가까웠지만, 갠적으로 이런 튜닝 성격의 연구도 좋아하고, 엄청난 개선이 있어서 재밌게 읽었습니다.</p>\n<p>결론 : 굳굳</p>","excerpt":"TL;DR EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요. 이번에 나온 논문은 효율성을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 compara…","tableOfContents":"<ul>\n<li><a href=\"/EfficientNetV2/#tldr\">TL;DR</a></li>\n<li><a href=\"/EfficientNetV2/#related-work\">Related Work</a></li>\n<li><a href=\"/EfficientNetV2/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/EfficientNetV2/#training-efficiency\">Training Efficiency</a></p>\n<ul>\n<li><a href=\"/EfficientNetV2/#training-with-very-large-image-sizes-is-slow\">Training with very large image sizes is slow</a></li>\n<li><a href=\"/EfficientNetV2/#depthwise-convolutions-are-slow-in-early-layers\">Depthwise Convolutions are slow in early layers</a></li>\n<li><a href=\"/EfficientNetV2/#equally-scaling-up-every-stage-is-sub-optimal\">Equally scaling up every stage is sub-optimal</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/EfficientNetV2/#training-aware-nas-and-scaling\">Training-Aware NAS and Scaling</a></p>\n<ul>\n<li><a href=\"/EfficientNetV2/#nas-search\">NAS Search</a></li>\n<li><a href=\"/EfficientNetV2/#efficientnetv2-architecture\">EfficientNetV2 Architecture</a></li>\n<li><a href=\"/EfficientNetV2/#efficientnetv2-scaling\">EfficientNetV2 Scaling</a></li>\n</ul>\n</li>\n<li><a href=\"/EfficientNetV2/#progressive-learning\">Progressive Learning</a></li>\n<li>\n<p><a href=\"/EfficientNetV2/#benchmark\">Benchmark</a></p>\n<ul>\n<li><a href=\"/EfficientNetV2/#imagenet\">ImageNet</a></li>\n<li><a href=\"/EfficientNetV2/#transfer-learning-performance-comparison\">Transfer Learning Performance Comparison</a></li>\n</ul>\n</li>\n<li><a href=\"/EfficientNetV2/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/EfficientNetV2/"},"frontmatter":{"title":"EfficientNetV2 - Smaller Models and Faster Training","date":"Apr 02, 2021","tags":["Deep-Learning"],"keywords":["Classification","EfficientNet","NAS"],"update":"Apr 02, 2021"},"timeToRead":4}},"pageContext":{"slug":"/EfficientNetV2/","series":[],"lastmod":"2021-04-02"}},"staticQueryHashes":["2027115977","694178885"]}