{"componentChunkName":"component---src-templates-post-tsx","path":"/EfficientNetV2/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요.</p>\n<p>이번에 나온 논문은 <strong>효율성</strong>을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 comparable 하거나 더 좋은 성능을 달성했다고 합니다.</p>\n<p>paper : <a href=\"https://arxiv.org/pdf/2104.00298.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/google/automl/tree/master/efficientnetv2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">github</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요 논문과 관련높은 reference</p>\n<ol>\n<li>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">paper</a></li>\n</ol>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>최근에 나온 convolution 기반 architectures 를 보면 (e.g. ResNet-RS, NFNet),\n성능은 좋지만, 모델 파라메터가 너무 많고 FLOPs 도 엄청나게 커서 웬만한 장비 아니면 훈련하기도 빡센 문제가 있어요.</p>\n<h2 id=\"training-efficiency\" style=\"position:relative;\"><a href=\"#training-efficiency\" aria-label=\"training efficiency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Efficiency</h2>\n<h3 id=\"training-with-very-large-image-sizes-is-slow\" style=\"position:relative;\"><a href=\"#training-with-very-large-image-sizes-is-slow\" aria-label=\"training with very large image sizes is slow permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training with very large image sizes is slow</h3>\n<p>큰 크기의 이미지를 사용하면 작은 batch size를 사용해야 하는 점이 속도 저하의 원인임을 언급하면서,\n훈련 시에 progressively 이미지 크기를 조정하는 방향으로 이런 문제를 개선했다고 합니다.</p>\n<h3 id=\"depthwise-convolutions-are-slow-in-early-layers\" style=\"position:relative;\"><a href=\"#depthwise-convolutions-are-slow-in-early-layers\" aria-label=\"depthwise convolutions are slow in early layers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Depthwise Convolutions are slow in early layers</h3>\n<p>EfficientNet architecture에는 <em>MBConv</em> 라는 block 이 있는데, depth-wise convolution 이 사용되죠.\n그런데, 요 연산이 tpu/gpu 에서 제대로 가속을 못 받아서 일반적으로 사용하는 convolution 연산보다 파라메터나 FLOPs 는 작지만 속도가 느려요.</p>\n<p>최근 연구들에는 이런 문제때문에 <em>Fused-MBConv</em> 라는 걸 만들었는데,\n아래 그림처럼 <code class=\"language-text\">Conv 1x1 + depthwise Conv 3x3</code> -> <code class=\"language-text\">Conv 3x3</code> 으로 replace 한게 더 좋다는 연구를 언급하면서</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAABa0lEQVR42o2TiZKCMBBE/f9vs/wALV0QlEsOD46AMFtvXNx4LZuqKTJp0unMdGbyZtR1LcfjUbqu07CHaY0URSFd+4oxZs8LpmkkTVMpy1LzpmlkGIYbZoxiVVXd80nCtm1lv99LHMeqgHwkvF6vEoahRFGkZGCThCjiyqhgE2ErZJ3gn0mFfd/rqdQoSZIXQjBqG8eJziGdVHg6ncT3fSV9JgTbbreS54XehPiTkI2Hw0Hm87ksFgtVadcQdVmWyvl8vh84SViWlTbFdV3ttK0QRUEQKI7afzUFEpRRJ+a2QroeRaESVlWt+Yh99qEx4jiOeL6vZDYhTcvz/GbsH0t9VPhrjUayLJPL5aKb6CREtrHBxi6zz1b5ohDQ8zw1sMjwcBjKdrudrNdrnb8bs/EF0ARqQyc3m42S8iIgWK1WaiVyyBznS+2zXC71i+oHQhSMz43rYB1+ol5cHyLWbrbJNMBZp4HYaCT8Bna6QmCZ7xT0AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MBConv\"\n        title=\"\"\n        src=\"/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png\"\n        srcset=\"/static/ca0824dfec86d1879498e934bc24af2c/12f09/mbconv.png 148w,\n/static/ca0824dfec86d1879498e934bc24af2c/e4a3f/mbconv.png 295w,\n/static/ca0824dfec86d1879498e934bc24af2c/fcda8/mbconv.png 590w,\n/static/ca0824dfec86d1879498e934bc24af2c/efc66/mbconv.png 885w,\n/static/ca0824dfec86d1879498e934bc24af2c/c83ae/mbconv.png 1180w,\n/static/ca0824dfec86d1879498e934bc24af2c/eb1d2/mbconv.png 1622w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>EfficientNet-B4 에 gradually <em>Fused-MBConv</em> 를 적용해 봤는데, <strong>early layers</strong> (1 ~ 3 stages) 에만 적용하는게,\n속도도 빠르면서 성능도 제일 좋게 가져갈 수 있었다고 합니다.</p>\n<h3 id=\"equally-scaling-up-every-stage-is-sub-optimal\" style=\"position:relative;\"><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\" aria-label=\"equally scaling up every stage is sub optimal permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Equally scaling up every stage is sub-optimal</h3>\n<p>EfficinetNet 에선 compound scaling rule 에 따라서 scaling 하는데, 만약 depth coef 가 2라면, 모든 stages 에서 2로 scaling 합니다.\n그런데, 각 stages 에서 훈련 시간과 파라메터 수는 equally contributed 안하는 문제점을 들면서, <em>non-uniform</em> 한 scaling 전략을 선택하겠다고 합니다.</p>\n<p>이미지 사이즈 같은 경우도 훈련 시간과 memory 에 큰 영향을 주기 때문에, (image size에 대한) scaling rule 도 변견했다고 합니다.</p>\n<h2 id=\"training-aware-nas-and-scaling\" style=\"position:relative;\"><a href=\"#training-aware-nas-and-scaling\" aria-label=\"training aware nas and scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training-Aware NAS and Scaling</h2>\n<h3 id=\"nas-search\" style=\"position:relative;\"><a href=\"#nas-search\" aria-label=\"nas search permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NAS Search</h3>\n<p>모델 훈련 속도를 위한 best combination 을 찾기위해, training-aware NAS 을 제안합니다.</p>\n<p>EfficientNet에서 사용한 NAS 기반을 했는데, 아래와 같은 목표를 joinly optimize 했다고 합니다.</p>\n<ol>\n<li>accuracy</li>\n<li>parameter-efficiency</li>\n<li>training-efficiency (on modern accelerators)</li>\n</ol>\n<p>구체적인 settings 은 논문에</p>\n<h3 id=\"efficientnetv2-architecture\" style=\"position:relative;\"><a href=\"#efficientnetv2-architecture\" aria-label=\"efficientnetv2 architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Architecture</h3>\n<p>NAS를 사용해서 찾은 architecture (EfficientNetV2-S, baseline) 가 아래와 같은 구조라 합니다. EfficientNet 과 크게 4가지 차이점이 있다 하는데,</p>\n<ol>\n<li><em>MBConv</em> 와 <em>Fused-MBConv</em> 를 섞어서 씀</li>\n<li>더 작은 expansion ratio (for <em>MBConv</em>) 를 사용 -> 더 적은 overhead 를 가지기 때문</li>\n<li>3x3 kernel sizes 를 선호. (하지만 작은 receptive field를 사용하는 만큼 layer를 더 쌓게 됨)</li>\n<li>EfficientNet 에 있던 맨 마지막 stride-1 stage 를 제거. -> 이것도 메모리 때문에</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABqElEQVR42k3R65LiIBQEYN//2Ryt1VxAuQRCGF1zgwTIIWYyG53a3f7/VXV17ygljNLHo8YYSVkyRsuyVKqqHw9KqZBCVRUAXC4YY3y732+3m5RlURRa612WZUlylqJIsyzPc0YpoZSQa5pmOcrTV273Jkw/8d4DQAgBAHYxxiRJEMJt25reSCmPxyNCqK5rY4zW+mO/54xjfEmSMyGEc56lqRBiWZbduq5919V1AwDrunZtgzAulZrn+fl8js41bWOtCd5XlTJ2GIah7/thGNZ13XCM0XvfdS3jHCF0Op2UUpXWldYhBP/K19fy+34HiO/mMcYfPE2Tc67rWoRQmiaHw0GW23JSltM0WWvf+FNrgAgA3rv/cNjwp65+nc6UXDHGvBAYIcqYD8FaG0J4PmellA8hAozjAH9xCGF0TnC2/zgIUUhR8EIkSXK9XJ334ziGEOYYGeOjczGCMeYf9t7bYSilOJ3PjDFCCMYXlOfkSoZx9N5veI5FIZzbfur7/r3ublkWY0zXdZ9aK1VqXSmlKKWc86rSdd2857XWKKW2O1+x1v656hvCMSYo2bOYPgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"EfficinetNetV2-S\"\n        title=\"\"\n        src=\"/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png\"\n        srcset=\"/static/25eda3ffc1b887abae2f1aaa5829a298/12f09/efficientnetv2-s.png 148w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4a3f/efficientnetv2-s.png 295w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/fcda8/efficientnetv2-s.png 590w,\n/static/25eda3ffc1b887abae2f1aaa5829a298/e4c9a/efficientnetv2-s.png 591w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"efficientnetv2-scaling\" style=\"position:relative;\"><a href=\"#efficientnetv2-scaling\" aria-label=\"efficientnetv2 scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNetV2 Scaling</h3>\n<p>위에서 만든 EfficientNetV2-S 기반으로 M/L 버전도 만들었는데, 몇 가지 제한을 두고 scaling 했다고 합니다.</p>\n<ol>\n<li>maximum inference image size to 480</li>\n<li>add more layers to later stages (stage 5, 6)</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACGklEQVR42m1Si46jMAzs///g7rWEBRJCKSXksXFeEJ9Cer3T6oYoKLLssWd8QcR932OMiIi5nIy5XAkFEXziw2OQUpVgzu+74oKI3ydK4Di/mHHFOEUppQ76OI6c834i53wcx9/knHMIoTLnI+OG8R43samgUk6V6o2fzMF7SikAlIDMlts1rG53pfGU5vlOKeu+unmeH48FAJxzAFBLXJxzjLH92DHj8rVorzPmGCIAxBhDCN47pdQmhJQyhKC1UVr9YQ6BUuqjjypufEPEWhsRYwzOOc6fnHNjiigAdp7ndV2rhIV5WZbjOL7Zt3MupeScQ8RwIucc4wEASikplRBCa62UthZeyaQlTrntvoU9gC3z+BPWWnF265zz3v9HMAAYxxECSCe98/u+V/GttVLKd8Ib1bmXYIhojDHfJqTgC4EPIRhjlHotxvEPfvqMWCzx3tdpU0rGlJ354e3/fa4bppQuYigtpRRCnA55d+ruvQcAa63WGgCMMbWFV3Kt58DVgZflQUg7jmwYBkop55wx5s96KcXqYk25jIwxNgqxMsb6vh85H0dGCKGMdV3X933TNJTR6X6f+MSnaRUrIeT5fKaULkPfd303DAMh5Hq98ok3zW0Yhr7r27ZtmlvXdaRpPj4+vsqz+XW9kqb5/PxcV3GZpolSerteCSFt2xLSdF1HKSWkud1u4MosKaUUY0r1H/c9eVcE/g05+pRKMiheIwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"acc_vs_step\"\n        title=\"\"\n        src=\"/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png\"\n        srcset=\"/static/dfccbfac89e8eb29126044fee0fdf98a/12f09/accuracy_vs_training_step.png 148w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/e4a3f/accuracy_vs_training_step.png 295w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/fcda8/accuracy_vs_training_step.png 590w,\n/static/dfccbfac89e8eb29126044fee0fdf98a/50383/accuracy_vs_training_step.png 740w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"progressive-learning\" style=\"position:relative;\"><a href=\"#progressive-learning\" aria-label=\"progressive learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Progressive Learning</h2>\n<p>훈련 시 image size 를 dynamic 하게 사용하는데, 이전 연구들은 accuracy drop 이 발생했다고 합니다.\n이번 논문에선 그런 accuracy drop이 <em>imbalanced regularization</em> (다른 이미지 크기로 학습하면 거기에 맞는 regularization strength를 사용해야 한다)에서 오지 않을까 추측합니다.</p>\n<p>아래와 같이 regularization strength를 실험해 본 결과, 추측한 대로 image size 가 작을 땐 weak augmentations, 클 땐 strong augmentations이 성능향상에 더 도움됐다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABEElEQVR42jXL2XLCIBQA0Pz/11ULhjVsSYgpXASytY7asTM976dRylAmvfeMMUKIUkprMwy9lNJa65yzf4SUztlxnGx/G3y5zrXWo9E24FZ3ncSXCyGEMcYoJZQi9IkxppQKITFC3nsAyDkDpBgjADwej+Z+/1nXZdu2/d+yLPu+D30vhFBK5VyMMUIIY0xKib8xa+3r9Wq+j71tCWNcKyUEp4xJKY/jsNYopSJArdVaCwAl51LKMAy5lHVd3/nYt4/TGSHEOW/bFmFMGUsA4zg653LJS62n88lPEwDEGBmlAHBL6Z33bRNCSClV1wnBZSe11jHGlNJXCCGGnPN1uoYQUkrz/MU5936a5/n5fP4CG5JD1ccxK58AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"regularization_strength\"\n        title=\"\"\n        src=\"/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png\"\n        srcset=\"/static/5411322c7d21461bb1aafd0796cd7304/12f09/regularization_strength.png 148w,\n/static/5411322c7d21461bb1aafd0796cd7304/e4a3f/regularization_strength.png 295w,\n/static/5411322c7d21461bb1aafd0796cd7304/fcda8/regularization_strength.png 590w,\n/static/5411322c7d21461bb1aafd0796cd7304/baaa6/regularization_strength.png 813w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Progressive Learning 을 하기 위해서 fomulation을 세웠는데, 아래와 같습니다.</p>\n<p>전체 훈련을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> steps를 하고 훈련 과정을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> stages로 나눴고, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span> 는 regularizations 종류 (e.g. RandAugment, MixUp, Dropout, ...)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABj0lEQVR42lXO247aMBAGYN7/jbhhdyuh3Yo2JODYhthJStanHIDgkINtSEWUXvS7Go30zz+LzWbz/va+Wq0+Pn6EYQgAAGF4mPi+jxBGEEIYbrc+QuiAceD7u93O8zwhxKJpmhDCU5YxxjjjUkrOeVVVRZEXRcH5vGGMSSmVUoyxvu+7rrXWLpxzb6vV5tdvSsjxGEXHI0L4dDpxzrMsI4S8esMQYxwnSRzHSqnxn4W1drlcfn79pJQCACCEGB8QQmmagP0eQQhASCmFEJIoghBlWda27Rwe+n69Xqs854wFQaDyvCwLz/Patq3r+j4xxkyvdlo3Xd9b5+bwOI5N0xBCkjhhjJ/PZ61vZVnWda2UattWKaVvNyFE13XP53Mcx+dkDmutMcYkiiiNkziWQlZVdblc8jzX+sY5L8tSSnm9XjnnutFSvg7N4fu92e0BpZR9fxNC0jRVKhdCpGnKGEuSpCjKP0nMuHDOPR4PY8zcPAyDv90Gwe5yuTrnzGCMNcMwdF03TMx/rJtY+xr+AuaY6OfBhtIwAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"progressive_learning\"\n        title=\"\"\n        src=\"/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png\"\n        srcset=\"/static/88a8f124fae964c40aa0bce818b45469/12f09/progressive_learning.png 148w,\n/static/88a8f124fae964c40aa0bce818b45469/e4a3f/progressive_learning.png 295w,\n/static/88a8f124fae964c40aa0bce818b45469/fcda8/progressive_learning.png 590w,\n/static/88a8f124fae964c40aa0bce818b45469/9128f/progressive_learning.png 603w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>각 모델은 아래와 같은 recipes 로 훈련했다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 526px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABdUlEQVR42jWP3XajIBgAff8n617UBhVDEhSVCPL3EVFCm5rY7Z6zczWXM1ld4zzP+2E41hgVJSHkfDpdLpeiLBFClDZ5/k4IaRtaYfyef+CqQgjhqtLGZB7AGgPgnXNa667rAECp6Z84IYT3vj4eRzF675XSAI5fr/M8xxiz/T+v12vfd++91grAL0uw1oYQpJyWJRhrAbwHMFrHe5qkfHx/ZwCglXLOKa1TSs65YegHzgEcIcQYSykFAM45Y0ypiVIaQiCEeH/Lhr7/8/ZWllVdY2OsMVaraRQipYQxXpaFMZbu96HvW8bC7Nu227atrvG6rpkQoiiKC6UFOtCmkdNv5Brj87lZa1P6tNbu+661MtbGGOcQ9v0lpfx6PDIhRnQ4tG1bIDQKOc+zlHJd18fja5pUjFGM4/Z8Xq/cWHubb9rYbdsYY/d7+s0uy5KcyPl86rpeCNE0jZRSTRNjnZSSMWaM6bqOc37lvO8H51zbtn+ffwDok7EshQEq9wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"recipe\"\n        title=\"\"\n        src=\"/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png\"\n        srcset=\"/static/8fa0df98e27bce0117600108343ea1d2/12f09/progressive_learning_recipes.png 148w,\n/static/8fa0df98e27bce0117600108343ea1d2/e4a3f/progressive_learning_recipes.png 295w,\n/static/8fa0df98e27bce0117600108343ea1d2/2d7ab/progressive_learning_recipes.png 526w\"\n        sizes=\"(max-width: 526px) 100vw, 526px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"benchmark\" style=\"position:relative;\"><a href=\"#benchmark\" aria-label=\"benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Benchmark</h2>\n<h3 id=\"imagenet\" style=\"position:relative;\"><a href=\"#imagenet\" aria-label=\"imagenet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet</h3>\n<p>accuracy, training speed 측면에서 EfficientNet 대비 다 좋네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.24324324324323%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADCklEQVR42l2U2ZKbOBSGef93ylymM51eHFaBzWojoV1INhgE1hSmk1SNLo/qk/7zn8UriiKKIpAkx+MxDMM0BVEUxckpik9FiVokKFOXlkCEEYSo65q6yfP8cjkfs8xL4tj3/SAINgSAMAqjKPD9II5BEASfnx9hEBwOn4fD5+fHx+vrz8Ph8P725vtBURSeMUZr3TRNEIZVVdUNqs8dhFjrHkGYZVlV1UVR5Hle1RVs27puIIRt20IIvcfj4ZyTUkZxBBKQlzSvOETSOTcMA8adEIJSKoRQSjnntNbrujrnrtfrF6yUzLL0dMrrM29Rj7CyyzoOAyVESEkpVUr1fe+c6/v+/zAhJPB/pWmeZDgvWX3mSg33+whhSwiFEDHGhODOOWPMDj8eD2+e7TRNlFLfPySgjFMMMkSZ3mW37QYjBKWUWm9BIcSyLF+w1hpjrOQmu6xgc5HNRWgz7DAhRErFGLvf79baPfj351224DyO4rzY4PrM7PPtYRhQhxjnhBDOuZRyT3WHjTFfMGPU939FSZsVvKypXbbrYRxg23Ydruu66zpGmXOu67odllL+dTtNkzSr86KrGtyb0S7rMAwbwzhCSEppjHHOzfO8I33fe/M8L8tCCQEAHI9ZdiyyEzoVGCIphEQIYkIghIwxKaW11hgzTdM8z5vsLw1CAJDmJcpLUp9ZC8X9PvV9jzHe3aaUcs7XdeWMT9O0LNZa6w23m5RSCp4koCzrqkbVWbRIaXO/3W6EYMZ512HO+d5hfwzb3B6GQWsthEiSODtWcYrSE2HcjKN91vmCMXm2CuZ8M2wcxx1elsVb1q0qlNI4jhOQ1g1GWMFOqP5qjG7bFmMMIey6jlJqrWWMjeM4TZNSyrter0Lwy+Xyz7dvP/59lVI459Z1fTwe19sVQUgpQwhtpWbsWRe1G6619uyyTNNECP7x8j0IwyRJbsPWXr+nCkspGedSiP6Zs/md8zYYEMKqqqIoenn5/vb28/39I03T4/MkYNsHAIAwDAHYNkye50kCqqrinGOM/wM/txaiEgGivwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"performance\"\n        title=\"\"\n        src=\"/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png\"\n        srcset=\"/static/e901cbbd3e0d1a76a8c62b654920a072/12f09/performance.png 148w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/e4a3f/performance.png 295w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/fcda8/performance.png 590w,\n/static/e901cbbd3e0d1a76a8c62b654920a072/5a6dd/performance.png 802w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABI0lEQVR42j3K22rrMBCFYb//89kEAqkJVhVHkm2dRjOj4yZt2etiXfx8EwB0pOyiD771RoVCDDlh9sFHX2pFZsD0YcRknQuRM1PBnPNkrfXqUvoIKdhgOWdrrTOX2KTx0XoIAROicy4a+9x2aU5IqdZaSpnO4zycNfYaY9TaxhgJYNfK2Jhz6b239onXdUp9nD721v7HyRhFFGsdY/Te+xgDAI5DtdZ/yviNxmiA8If6n5yYKUZPRN5/PufcWgOIzIxIMcZaS62t1hKCI6LgPTP/somZU0pKvdd1NcYARERMKRHhvu/v9xsRASAlVEp/CyGl3PddKYWI0+PxWJZlnufb7Xa/35d5XuZlXb+klOJnr5fctk2+PhNCKKWFEM/nU2v9D23/jZs9f5BLAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"efficiency\"\n        title=\"\"\n        src=\"/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png\"\n        srcset=\"/static/e5c498ab87386813999ea34af6583e5b/12f09/efficiency.png 148w,\n/static/e5c498ab87386813999ea34af6583e5b/e4a3f/efficiency.png 295w,\n/static/e5c498ab87386813999ea34af6583e5b/fcda8/efficiency.png 590w,\n/static/e5c498ab87386813999ea34af6583e5b/efc66/efficiency.png 885w,\n/static/e5c498ab87386813999ea34af6583e5b/c83ae/efficiency.png 1180w,\n/static/e5c498ab87386813999ea34af6583e5b/68638/efficiency.png 1510w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"transfer-learning-performance-comparison\" style=\"position:relative;\"><a href=\"#transfer-learning-performance-comparison\" aria-label=\"transfer learning performance comparison permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transfer Learning Performance Comparison</h3>\n<p>다른 datasets 에 transfer learning 했는데, 성능이 comparable 하네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.72972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABEUlEQVR42iXN63KDIBBAYd//8TJRU6soGkApuywIwVu0k/b8/+ZkXduVZZnneVEWRZ4Xef54PO73O2OsaRrGGiHlOI5KKSGEkJ+01lJK51yWUkLEcRyHpzRAaH2IcV2XlJL6MPWjNRijtQYARDBgiIicu64rO45j2zYEKIqv72ZqOlCTPd/v67r0NAFijBERrbXOuWVZzI9Z/jrPM1vXlYgATF2zqlZVLfonxNe6bm+lFACkVyIi7z0RLcuCAPu+H8fxf95jjMaYqqr6vtdaGyCwwc9pnCYiCiEQEVqLiCEEROu9t4gfHELo+34YnrfbjXMeQpi9997Nsx+GQQip9SSFYG3bcS6EaNuWc84Y27btF4TzS02LWazDAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transfer_learning\"\n        title=\"\"\n        src=\"/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png\"\n        srcset=\"/static/814e99c33821f04e492f5dee4c8b8c87/12f09/transfer_learning.png 148w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4a3f/transfer_learning.png 295w,\n/static/814e99c33821f04e492f5dee4c8b8c87/fcda8/transfer_learning.png 590w,\n/static/814e99c33821f04e492f5dee4c8b8c87/efc66/transfer_learning.png 885w,\n/static/814e99c33821f04e492f5dee4c8b8c87/c83ae/transfer_learning.png 1180w,\n/static/814e99c33821f04e492f5dee4c8b8c87/e4ee8/transfer_learning.png 1496w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>새로운 아이디어보단 여러 가지를 조합하고 training recipe 실험에 가까웠지만, 갠적으로 이런 튜닝 성격의 연구도 좋아하고, 엄청난 개선이 있어서 재밌게 읽었습니다.</p>\n<p>결론 : 굳굳</p>","excerpt":"TL;DR EfficientNet 의 2번째 논문이 나왔네요. 저자는 EfficientNet 을 쓴 두 분이 쓰셨네요. 이번에 나온 논문은 효율성을 목표로 한 연구인데, NAS로 모델 훈련 속도와 파라메터 수를 엄청나게 줄이면서 성능도 compara…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#training-efficiency\">Training Efficiency</a></p>\n<ul>\n<li><a href=\"#training-with-very-large-image-sizes-is-slow\">Training with very large image sizes is slow</a></li>\n<li><a href=\"#depthwise-convolutions-are-slow-in-early-layers\">Depthwise Convolutions are slow in early layers</a></li>\n<li><a href=\"#equally-scaling-up-every-stage-is-sub-optimal\">Equally scaling up every stage is sub-optimal</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#training-aware-nas-and-scaling\">Training-Aware NAS and Scaling</a></p>\n<ul>\n<li><a href=\"#nas-search\">NAS Search</a></li>\n<li><a href=\"#efficientnetv2-architecture\">EfficientNetV2 Architecture</a></li>\n<li><a href=\"#efficientnetv2-scaling\">EfficientNetV2 Scaling</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#progressive-learning\">Progressive Learning</a></p>\n</li>\n<li>\n<p><a href=\"#benchmark\">Benchmark</a></p>\n<ul>\n<li><a href=\"#imagenet\">ImageNet</a></li>\n<li><a href=\"#transfer-learning-performance-comparison\">Transfer Learning Performance Comparison</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/EfficientNetV2/"},"frontmatter":{"title":"EfficientNetV2 - Smaller Models and Faster Training","date":"Apr 02, 2021","tags":["Deep-Learning"],"keywords":["Classification","EfficientNet","NAS"],"update":"Apr 02, 2021"},"timeToRead":2}},"pageContext":{"slug":"/EfficientNetV2/","series":[],"lastmod":"2021-04-02"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}