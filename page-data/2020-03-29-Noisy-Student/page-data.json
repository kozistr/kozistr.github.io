{"componentChunkName":"component---src-templates-post-tsx","path":"/2020-03-29-Noisy-Student/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각나서 리뷰하게 됐어요.</p>\n<p>아래는 이번 approach 가 달성한 성능인데, 이전 SOTA 에 비해서 Accuracy 가 약 2% 정도 올라갔네요.</p>\n<p>해당 이미지에는 <em>L2</em> performance 가 안올라와 있는데, <em>Noisy Student + Random Augment</em> 로 훈련한 <em>L2</em> 모델 top-1 accuracy 가 <em>88.4%</em> 입니다.\n올해에는 90% 가 넘는 architecture 가 나오지 않을까 생각이 드네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACLElEQVR42m1Ti5KiMBD0//9s73RL95QVFUFeCiSQQEJIALmOeJ5XdVNb2WRIT/f0xMU0TYyxuq6Hvu+0brUhUWhYmR7c0N0HoY3oEU3TGGO6PyGlXACMlFJqGIaurlXom2thCh5+B8H5SinR2twfMf0bFows51xp3dVcX8KqGHhzF2rqzDT+DwMmIURZlhDyYNamkULdCL12ZpwegPnP7iHY8zxoR2sAFEVxOh7jOEaJRdu2Nisa6kSdGmfAY53wCbfTNPXOXlVV6Au04zhCKY5WNgTUQnY4nckwg6ZJa41vMAJXdde1qoVD2L+6wN6CKaVcNDwtmqBAFp6LRlSsQl5IyTiH24QSiHy1PZewYDsnUZdpVQZVxcs0SZFFYUhBCeulUjPg5d9fMGRLJSs/SYKsrCghFJqthGF4UaFz9A8MksBABY4WfLvdGGexTxhr6prjHr5l2S2JEynbWeZms1mv18fTMUlTkDmOk2U5FC26TuMfekMlgAkh8L7v+/ubSN/3HWf36+vrcDhgNK7rzu3YOUMDOgchBGOMKISe3x/Gfr9fLper1co7ebi53W5hyBMMDAjzR8ClYejfpm3Xb8dZ/lx+/PgAJ5g/Pz9pWWJ4T+YMTeR5kibgh+Z32ln2brdz3f3lEkAwNuB4MuPdQAZKXIIgSWDK1brA2LwijxeKaZ89L4qjIs9xxCsAZDE/F+DBACU4YgU/bsPYKIwIpeHF7s0z8DMzUCdl+xuso4v4qG/jGwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png\"\n        srcset=\"/static/83339124bc869908ad6f92e63006f4a1/12f09/performance.png 148w,\n/static/83339124bc869908ad6f92e63006f4a1/e4a3f/performance.png 295w,\n/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png 590w,\n/static/83339124bc869908ad6f92e63006f4a1/efc66/performance.png 885w,\n/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png 1039w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1911.04252.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이전 Image Classification 연구 동향들에서도 간단하게 설명은 다음에 다른 포스트로 다뤄볼께요. 양이 좀 많이 질 듯 해서 (<del>귀찮네요</del>)</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<h3 id=\"self-training-w-noisy-student\" style=\"position:relative;\"><a href=\"#self-training-w-noisy-student\" aria-label=\"self training w noisy student permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Training w/ Noisy Student</h3>\n<p>학습은 다음과 같은 process 로 이뤄지는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACaElEQVR42oWU53LbMBCE+f7P5nEm9IhNYi9gk2SxiJ3KB9JWYiczuR8QiCu7WNxJMU3z5eVF07Q4jg+aZtu2Y0tLEnG73fI8v1wuZVmez+e+7zm5Xq/nskyFuFyuyro+yqI4HA5UcRyb1XacMAyplUgTrueSU2/WNG3TNJW0el1WZVkWkSSmaRwt601VTcvSdN3Q9aIsojgGea+SpmmeZUWeF0UhhIiiaJ5n5fF4UA0y0MuyDCcMszzH9/ifKX8fjeNALRhO09i2LZzv9zsXZo93/bQvyc+jpq6PxyPcYNi0bd/17+/vyAO1PeAL8vrV5nlCkGEYdte3gGWzj2SCuGoQBkHgn44nMMMw+Hwv27IsBPM81/M8NGMvROJ5/rJ80i7LAqmJsKyj7/sQtngw2yaBx6PqOM2g/QP5yW23aZrQZuiHW1U1m6ETJyQM0vqP+K2Q8k0tFK6rqrvfaQPZCnWN1F3XkSyLbkL8Foxvuo8LcyX7dPqpvrGapqUbBtdzXa4qYB5FsmF83wuC0HUdx3WLspTINCrCcGIYxuvrD13XDNNiT6upqoocqvpGuIUSpnk4yP6HzjiOMpmWIDTLUtd1mRBd09iAH8UJUOCgNUIS0/WS/7P5ZDJfqDKMI3sclGRFgF1VPpFtnpfdi6JP2WVy1925A/1f5Bks0HmfQbiQxsyBniRxmuW0Kt0mxyPPCVC2ehNi1HLKlnVD+7OhELzIi+21Pl50f3MWBVag0WQb2JkeusjfM+MFfiIEewDxilTuGUy44OWQP4M1iWNEIhYQEqqtPVhxMwzV9k8AWzA4ZHZ38m3b/gL9cit7gJJOrwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png\"\n        srcset=\"/static/db781720920abfda90bbcb8c902c548d/12f09/teacher-student.png 148w,\n/static/db781720920abfda90bbcb8c902c548d/e4a3f/teacher-student.png 295w,\n/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png 590w,\n/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png 654w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li>Labelled dataset 인 ImageNet 으로 <strong>Teacher Model</strong> 을 학습</li>\n<li>un-labelled dataset 인 JFT-300M 를 <strong>Teacher Model</strong> 로 pseudo labelling 하기</li>\n<li><code class=\"language-text\">2</code> 에서 생성된 data + ImageNet 으로 <strong>Student Model</strong> 학습 w/ noise</li>\n<li>다시 <code class=\"language-text\">2</code> 으로 가서 반복 (iterative training)</li>\n</ol>\n<p>위 과정이 knowledge (self) distillation 과 비슷한 과정인데, 주로 요 목적은 compression 으로 사용되는데, 여기는\n해당 목적 (not compression) 으로 사용하지 않는다는게 차이점 입니다.</p>\n<h3 id=\"training\" style=\"position:relative;\"><a href=\"#training\" aria-label=\"training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training</h3>\n<p>데이터 셋이나 구체적인 training recipe 들이 있지만, 적용한 technique 가 있어서 이걸 설명 해 보면,</p>\n<h4 id=\"fix-train-test-resolution-discrepancy\" style=\"position:relative;\"><a href=\"#fix-train-test-resolution-discrepancy\" aria-label=\"fix train test resolution discrepancy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>fix train-test resolution discrepancy</h4>\n<p>처음 몇 epoch 은 low resolution image 로 훈련을 하고 후에 high resolution image 로 fine-tuning 하는 기법입니다.</p>\n<p>논문 실험에서는 처음 350 epochs 는 낮은 해상도 이미지로 훈련하고, 1.5 epochs 는 더 큰 해상도로 unlabelled image 에 대해서 훈련했다고 하네요.</p>\n<p>unlabelled image data 는 학습할 때 labelled image 보다 14 배 큰 batch size 를 사용했다고도 하네요.</p>\n<h4 id=\"iterative-training\" style=\"position:relative;\"><a href=\"#iterative-training\" aria-label=\"iterative training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Iterative Training</h4>\n<p>논문에서는 총 3 steps 의 iterative training 을 했다고 소개합니다.</p>\n<ol>\n<li><code class=\"language-text\">EfficientNet-b7</code> 을 ImageNet 으로 훈련 <code class=\"language-text\">(as Teacher)</code></li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 JFT-300M + ImageNet 으로 훈련 <code class=\"language-text\">(as Student)</code> (batch size 비율은 labelled : unlabelled = 1 : 14)</li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 새롭게 훈련 <code class=\"language-text\">2</code> 에서 만든 모델을 <code class=\"language-text\">Teacher</code> 로 사용 <code class=\"language-text\">(as Student)</code></li>\n<li><code class=\"language-text\">3</code> 과 비슷한 scheme 으로 진행하는데, (batch size 비율은 labelled : unlabelled = 1 : 28) 로 훈련</li>\n</ol>\n<h4 id=\"noisy\" style=\"position:relative;\"><a href=\"#noisy\" aria-label=\"noisy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Noisy</h4>\n<p>Student Model 을 학습할 때 논문에서 <code class=\"language-text\">Noisy</code> 하게 훈련한다고 했는데, 이 때 <code class=\"language-text\">Noisy</code> 에 해당하는 부분은 크게 3 부분 입니다.</p>\n<ol>\n<li>Data Augmentation w/ RandAugment</li>\n<li>Dropout</li>\n<li>Stochastic Depth</li>\n<li>other techniques (data filtering, balancing)</li>\n</ol>\n<ul>\n<li>OOD (Out-Of-Distribution)</li>\n<li>unlabelled data 에 대해선 class 별 samples 수가 biased 돼있으니, 적은 sample 들 duplicate 하기</li>\n<li>pseudo label 시, soft or hard pseudo 한다고 했었는데, soft, hard 둘다 좋은 결과를 보였지만, soft 가 더 좋았다</li>\n</ul>\n<p>요런 기법들은 이전에 소개된 기법들이니 설명은 pass</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"imagenet-benchmark\" style=\"position:relative;\"><a href=\"#imagenet-benchmark\" aria-label=\"imagenet benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet Benchmark</h3>\n<p>역대 ImageNet architecture 들 정확도를 benchmark 한 table 인데, 다른 구조보다</p>\n<ol>\n<li>더 적은 params 수</li>\n<li>상대적으로 적은 extra data 수</li>\n<li>더 높은 성능</li>\n</ol>\n<p>을 달성했다는 점에서 의미가 있을 것 같네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB7UlEQVR42m1T2ZKjMAzk/39uk6lhuI05bbCJjSFcIWQbqMnDZvUkCrW61ZItzhmlSX9E13f3+71pJCFESjkMw00p5IyxPb/d0jSt61rUNSBKKYtzTqKY1tTMZlu3dV1RRylFl+fz+Xg8xnFclgX5PM9lWQoh8ImCYRytsijsbzuMg9RkryNQRKJoHKfzE7B3UnEGMPIwCud5stDserlGnJjFvLa9aFnmMAxB+A9427YkSTACcs91p2myqoq7jkvTJBMZlJ/gOI7x75MZMwsp0QVGzPNiccZc38tkrge9PtdTNhjeYJS+wZgRtm2vDd2XebZqXrmBK+5i/WUAM6Xxp2x4Cc1aa7SDYbANbrPrn6uXuLznv4ZNYRD+17A0TYSQSIIg2MFFUVy+LoQTNartcAyyPc/F2j/BSUKxZzC7p2FQ4vhOwYqqq8Z1OsFRFA2HbNRB7XYEwGBqmgYMMAVlAJee54dBQFLCatb0EucBP7quW494HIEEVABL2QAG23FLVpHn37YNME6FZCRr837sfd8HGMr1EW1r2raFhRnO8zgSnAr67XtGKaiSNCl5IYzQfYsLA3QY7liMUgdca1DleS6bBkIiQuDobtjPj+04ThQRrbTpjNL7YwAC49E4xhuASBwihAS+XzJmjDml/QWKYxJf0CLY0QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png\"\n        srcset=\"/static/4a5d59c7ec213c223b48d78af30834c5/12f09/overall-imagenet-performance.png 148w,\n/static/4a5d59c7ec213c223b48d78af30834c5/e4a3f/overall-imagenet-performance.png 295w,\n/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png 590w,\n/static/4a5d59c7ec213c223b48d78af30834c5/efc66/overall-imagenet-performance.png 885w,\n/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" style=\"position:relative;\"><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" aria-label=\"robustness results on imagenet a imagenetc and imagenet p permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</h3>\n<p>해당 dataset 은 이 모델이 정말로 noise 에 robust 한지 체크하는 task 입니다.</p>\n<p>예를 들어서, (일반적인 corruptions, perturbations)</p>\n<ol>\n<li>blur 섞인 이미지</li>\n<li>fogged 이미지</li>\n<li>rotated 이미지</li>\n<li>scaled 이미지</li>\n</ol>\n<p>등등이 데이터에 섞여 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.1891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB80lEQVR42m1SXa+bMAzl//+qXVXqnnZXut4WAgkQAoSP0lJIgNLSnYAqbdPOQ2Ilto/tYyuX0qc0iqLqfL5er5e/Udd1FIVCiCRJ8jyvqioMQ9gxj5vmZlVVefj6oj693W6617pXQz8M4zgsUErFMaeM4ZR5rrUmrpukKVIorRFceYREnPuEBjmjivWqHxfM84ws9/tdyizL5HOen08cr8fjMS+wiqI4HA6EkCBgSEFakpQJD9FHBM/7ApmmPI7h/Xq9cK7BsK2yLE+nUxAEnHE/oVRTrfR4N2Xjew1OEgHqxd8A/KsB5tze265LKKexFEIJnvEoiISIHwvzNE3wQZ//CS6L3HVd6lGSkajhQzt0Y6cmpbseJWBgKBLMpot39J/MxfF4PDlOyAJG6e7HzkkdphiJPRL4zbWZJlO251OU8G8w1EPPlNIwCmIhYHAeZV0uWlF3F9Xpuj5zMz5utNSq6zoYOPu+NwND2Ri1SzzO+bk6P5+P2chixBiN4D1j1H8zQ1rb3tWXi2HO0nSz2ez39rePj+/b7W5nO84Ja4LE65KUZfHz8/PX4QhCFAya7XabpAlmYeH7uuzhuoy4m6Zp37gtWK227bC/uPECHwRaq9xSSowN/YcBcxzH8wilLJOZMVggsxSPDvQkRMQxsqzb8hsNYs/VOFJ/XgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png\"\n        srcset=\"/static/bd4580870392b73ee63cd51263ff1380/12f09/imagenet-a-benchmark.png 148w,\n/static/bd4580870392b73ee63cd51263ff1380/e4a3f/imagenet-a-benchmark.png 295w,\n/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png 590w,\n/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png 598w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>ImageNet-A dataset benchmark table 만 하나 보면, 확실히 <code class=\"language-text\">The Noise</code> 가 robustness 에 큰 도움을 주고 있네요.</p>\n<h3 id=\"adversarial-robustness\" style=\"position:relative;\"><a href=\"#adversarial-robustness\" aria-label=\"adversarial robustness permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial Robustness</h3>\n<p>이번 실험은 adversarial attack 에도 robust 한지 확인하는 겁니다. 주로 FGSM Attack 을 해서 테스트를 하는데, 성능은 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 523px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABm0lEQVR42n2T65KiMBCFff/H059ahRaCXIQohIDktl8Ii7M7znRRVKfTp0/ndLLz3lvn1POphXhp/Zqm1782KaXLsqnrsqru93ue53Vd3243pdQOsDamF0Inif9kZp6H8zk9nw+Hw/F43O/31+uVKsMw7JxzWZZVbdtmmZfSke7cV7C1Vkg5jmPXdc/Fpmma55nIDpRc9mTX2ap6I3Hi98ne4NPp1Pc9BXRd+zT9DnBfzdpw0giuFhkQhoUaBj+Onv7zPDgspfxIrrUO4KZpEJDTG2s5U+ShLd91AVwUvix92/qmCZ8Qodww6KoaUVsIATO6A0MMa4z/xbT2ff8Gg2RuMLOFnmYD/yrY2jY9p2mKYIT4o9+m0GfyJb4KxpBhRrBwH4yhEaI/qf0/M2yXy4WGV6k8oghGEG/CTy2sc46EUbAtlUiYnFJFUTwej+hjxGMCzFQPYI4N28bsvl3P8DyWByP/Gq2tD4N6tM3Ao2D41MJhcvHykkoQafjTC2lJkqxtb+Xh5DD4ejG2SSrLEhKWse1wnYyhonPuD0FxoHT3nxQJAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n        srcset=\"/static/3a179011ae1230e5213e3dcc7fa3a416/12f09/fgsm-benchmark.png 148w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/e4a3f/fgsm-benchmark.png 295w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png 523w\"\n        sizes=\"(max-width: 523px) 100vw, 523px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>FGSM 보다 더 강력한 attack 인 FGD Attack 시에도 꽤괜 성능을 보였다고 캅니다.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>요즘 경향들은 이전처럼 deep 한 architecture 를 설계하거나 AutoML 을 이용한 NAS 를 만드는 것 보다는,\ntraining recipe (~ techniques) 에 집중을 하고 있는데, 이런 trend 에서 재미있는 approach 들이 많이 나오고 있는 것 같네요.</p>\n<p>또 현재 상태에서 CutMix 등등 여러 또 다른 기술들이 적용되면 최고 performance 가 어느 정도 될지도 궁금해 지네요.</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#architecture\">Architecture</a></p>\n<ul>\n<li>\n<p><a href=\"#self-training-w-noisy-student\">Self-Training w/ Noisy Student</a></p>\n</li>\n<li>\n<p><a href=\"#training\">Training</a></p>\n<ul>\n<li><a href=\"#fix-train-test-resolution-discrepancy\">fix train-test resolution discrepancy</a></li>\n<li><a href=\"#iterative-training\">Iterative Training</a></li>\n<li><a href=\"#noisy\">Noisy</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"#imagenet-benchmark\">ImageNet Benchmark</a></li>\n<li><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\">Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</a></li>\n<li><a href=\"#adversarial-robustness\">Adversarial Robustness</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/2020-03-29-Noisy-Student/"},"frontmatter":{"title":"Self-training with Noisy Student improves ImageNet classification","date":"Apr 11, 2020","tags":["Deep-Learning"],"keywords":["EfficientNet","Noisy-Student"],"update":"Apr 11, 2020"},"timeToRead":2}},"pageContext":{"slug":"/2020-03-29-Noisy-Student/","series":[],"lastmod":"2020-04-11"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}