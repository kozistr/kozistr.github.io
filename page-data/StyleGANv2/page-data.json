{"componentChunkName":"component---src-templates-post-tsx","path":"/StyleGANv2/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 리뷰할 논문은 지난 19년 11월에 나온 <strong>StyleGAN v2</strong>를 리뷰 해 보겠습니다</p>\n<p>StyleGAN 에 이어서 2 번째 논문인데, 이번 버전에서는 어떤 문제점들을 어떻게 해결했는지를 한번 보려고 합니다!</p>\n<p>아래는 StyleGAN v2 로 생성한 이미지들 입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5113e9adaf42d411c678dbce0d576441/2bef9/stylegan2-teaser-1024x256.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAAsTAAALEwEAmpwYAAABPElEQVR42gExAc7+AIx+UxkRDSEZEz4yJaucZUA9DT8tGj0nIyEKAiESAmFLJ4ZxV4N1c3BjTrWwd8HNmrSxoaqjkczNreTmxwCtllR5TCq3cUmecECem0JPPwSGb3KhfHovFAw1FwlUSTibgHPkyMGBZVxjXyyntIveysHgwLG3rIqzy4QAvqBRsHVGyXhPy5pYvaxHtHkNjW1nb1JSNxoTaTILal41nISA2r+4sZCIdGVJtqaDz62buYx8nYxefJ9LAG5dNYNVM7ZtSoRoRJWBSK94IqJ7coJdYS8VDlMvCGJZKnZlY964rotuZWZZN8e9mLiSfZp1a4V7UWuNSQAgICQrIyBALiUbGh41NDuRiJdsVlZzU1ERBQATDg1cNy9+XljStKVtU0lbTSaDiISMjZp+bmN6jFhNXjoxOH5GGhPyKAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/5113e9adaf42d411c678dbce0d576441/fcda8/stylegan2-teaser-1024x256.png\"\n        srcset=\"/static/5113e9adaf42d411c678dbce0d576441/12f09/stylegan2-teaser-1024x256.png 148w,\n/static/5113e9adaf42d411c678dbce0d576441/e4a3f/stylegan2-teaser-1024x256.png 295w,\n/static/5113e9adaf42d411c678dbce0d576441/fcda8/stylegan2-teaser-1024x256.png 590w,\n/static/5113e9adaf42d411c678dbce0d576441/efc66/stylegan2-teaser-1024x256.png 885w,\n/static/5113e9adaf42d411c678dbce0d576441/2bef9/stylegan2-teaser-1024x256.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1912.04958.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>official implementation : <a href=\"https://github.com/NVlabs/stylegan2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요건 이전 버전 StyleGAN paper 입니다.</p>\n<ul>\n<li>StyleGAN : <a href=\"https://arxiv.org/pdf/1812.04948.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n</ul>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이번 논문에서는 이전에 발표한 StyleGAN 의 artifacts 들에 대해 지적하면서 시작합니다.</p>\n<p>요약 해 보면 크게 3개의 문제점을 지적 / 개선 / 해결 했는데,</p>\n<ol>\n<li>blob-like artifacts</li>\n<li>artifacts related to progressive growing</li>\n<li>metrics for evaluating GAN performance</li>\n</ol>\n<h3 id=\"blob-like-artifacts\" style=\"position:relative;\"><a href=\"#blob-like-artifacts\" aria-label=\"blob like artifacts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>blob-like artifacts</h3>\n<p>StyleGAN 에서 <em>Instance Normalization</em> 이 아래 이미지 처럼 water droplet 과 같은 artifacts 를 발생한다고 합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/18408e2b9433f5d9ea020c0b9c0a5937/280a1/droplet-llike-artifacts.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.243243243243242%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA/UlEQVR42gXBX0uDQAAAcD9bQdDjoFFB2ZqNNRlYyz9T725qJ9M2ogUF5zYx0vPutF56iKAY1UPQ0176Pv1+0nuRUDKN7kLxmNMkYVzEE1/tH7qWlvFa1M80TZ8Ym0bRNR4NOi2zdVBTWnJOFwvpax5/kvF+u+EHAYTwMghOOrIsN7WeenYxBAiNDAND1FWOyQSHg26vue1YlgvA0DCk35n3gc3B0S7yPeCCMMRKW9nc2NrbaZjaKYTAdhyAUABdz9Bv4PkLXUIIbdvWdV36W95/X43X4LYUnBZFJURCSF9V2QNZ/6wopXmeM8Z4kWPHTGfx6u21qqqyLLMs+wdVW4Dcg6iphQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/18408e2b9433f5d9ea020c0b9c0a5937/fcda8/droplet-llike-artifacts.png\"\n        srcset=\"/static/18408e2b9433f5d9ea020c0b9c0a5937/12f09/droplet-llike-artifacts.png 148w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/e4a3f/droplet-llike-artifacts.png 295w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/fcda8/droplet-llike-artifacts.png 590w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/efc66/droplet-llike-artifacts.png 885w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/c83ae/droplet-llike-artifacts.png 1180w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/280a1/droplet-llike-artifacts.png 1545w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>generator</em> 의 activation map 을 보면 (오른쪽 사진들) 자국 같은 것 들이 보일텐데, 설계상 때문에 요게 문제가 됐다는 겁니다.</p>\n<p>그래서 StyleGANv2 에서는 새로운 normalization method 를 사용해서 이 문제를 해결합니다.</p>\n<h3 id=\"artifacts-related-to-progressive-growing\" style=\"position:relative;\"><a href=\"#artifacts-related-to-progressive-growing\" aria-label=\"artifacts related to progressive growing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>artifacts related to progressive growing</h3>\n<p>high-resolution GAN 에서 stable 한 훈련을 위해 low resolution 부터 훈련을 시작하는 progressive 한 훈련 방식을 해 왔었는데,\n이전에는 각 resolution 마다 같은 network 구조를 사용했는데, 다른 resolution 을 훈련할 때는 다른 network topology 를 사용해야 한다는 말 입니다.</p>\n<p>매 resolution 마다 같은 구조가 아닌 다른 구조로 학습을 하게 되면, 각 resolution 에 맞게 더 효율적으로 훈련할 수 있다는 논문피셜 입니다.\n(당연한 이야기긴 하지만)</p>\n<h3 id=\"metrics-for-evaluating-gan-performance\" style=\"position:relative;\"><a href=\"#metrics-for-evaluating-gan-performance\" aria-label=\"metrics for evaluating gan performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>metrics for evaluating GAN performance</h3>\n<p>GAN 생성 이미지 quality 를 측정하기 위해 여러 metric 들을 사용하는데 (e.g. FID, precision, recall),\n이런 metric 들의 문제점을 제기하고 새로운 gan metric 을 사용해서 performance 를 측정했다고 합니다.</p>\n<p>간단하게 설명 해 보면, 위에 소개된 이전 metric 들은 inception v3 같은 base network 에 기반해서, 전반적인 texture 보다 shape 같은 것에 집중을 하는데,\n결론적으로 이미지 quality 전반적인 면을 capture 하지 못한다는 말 입니다.</p>\n<p>그래서 이런 문제가 어느 정도 해결 한 perceptual path length (PPL) metric 을 사용했다고 합니다.</p>\n<h3 id=\"etc\" style=\"position:relative;\"><a href=\"#etc\" aria-label=\"etc permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>etc</h3>\n<p>마지막으론 latent space <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span> 가 더 잘된다고 하네요</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<p>결론적으로 아래와 같은 변경 사항들로</p>\n<ol>\n<li>characteristic artifacts 들을 제거하고</li>\n<li>full controllability 를 유지한다.</li>\n</ol>\n<h3 id=\"removing-normalization-artifacts\" style=\"position:relative;\"><a href=\"#removing-normalization-artifacts\" aria-label=\"removing normalization artifacts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>removing normalization artifacts</h3>\n<p>위에 introduction 에서 <em>instance normalization</em> 때문에 blob-like 한 artifacts 가 생긴다고 했는데,\n최종 이미지 (1024x1024) 에서는 안 보일 수 있어도, 중간 이미지 (64x64) 쯤 부터 발생하는 걸 볼 수 있는데,\n이런 부분을 <em>discriminator</em> 에서 잡을 수 있어야 하겠죠?</p>\n<p><em>AdaIN</em> 같은 operation 이 각 features 별 <em>mean</em>, <em>variance</em> 를 따로따로 normalize 하기 떄문이라 원인을 밝힙니다.\n(논문에선 generator 가 의도적으로 signal strength 정보를 이전 <em>IN</em> 으로부터 sneak 한다고 표현돼 있어요)</p>\n<p>결론은 이런 normalization step 이 없어지면 이런 droplet artifacts 가 없어질 거라 합니다.</p>\n<h3 id=\"generator-architecture-revisited\" style=\"position:relative;\"><a href=\"#generator-architecture-revisited\" aria-label=\"generator architecture revisited permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>generator architecture revisited</h3>\n<p>아래가 과거 (StyleGAN) / 현재 (StyleGANv2) <em>generator</em> architecture 비교 샷 인데,\n<em>AdaIN</em> operation 을 normalization, modulation step 으로 분리해서 보여줬네요.</p>\n<p>과거에는 <em>bias</em> 하고 <em>noise</em> 를 <em>style block</em> 에 적용 해 줬는데, 이런 상대적인 영향이 게 style magnitude 에 반비례하게 적용된다고 합니다.</p>\n<p>그래서 이런 operation 을 <em>style block</em> 밖으로 빼면서 조금 더 predictable 한 결과를 가져갈 수 있었다고 해요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dcd29a1c419acffd55110ffd0c4a35d3/af756/stylegan2-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.891891891891895%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABmUlEQVR42h2PXXOiMAAA+f8/5WbuoZ25mfNqe3O2HUVtq4AEw4dWrKAEMJCQhCSkN+7Tvu2s1bHWj52FZ1/QUWutlKpqvAK7Myo71gw3OO9lr83NjTGDHqSUxhiLdtiDrr2e47YSPStQ5sHgx+N4E0OlBKouyT4cL563n9CYoSizN5DkRZZfUqWkRegVJGDhvaMqk6o/nY5O4N89T9wIME4IaY9f6Z+ZDZKIdLUQAsRpuPfDndOQ2uKCO956Op82TaO0RNV5A4Kfo0cfRmbQZX1JkvjX018H+Er11bUASRrGobdxGO8sLqgXfry+T/LioLUqURlE3ujltws/GCe9kOlpP54/bWJH9IxxtgqSpTNdrv+15Hp7jtYL125p3fc8y1MPbu8mryCJpeKoPMe7aDSb+jFkvK0xgrtjVddFUTBOLUJxEAf2aplfDlL1eZa7gXs/eXChw1jLGPvKDg/2C0gCLijtWtuN3O2bB2eEYktKXlb4lJdaaWOMEAI3ZH8845YaY5RSlFJUlPiKGafDoDnr8BUXBfpf/gaqZLARoyH3LQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/dcd29a1c419acffd55110ffd0c4a35d3/fcda8/stylegan2-architecture.png\"\n        srcset=\"/static/dcd29a1c419acffd55110ffd0c4a35d3/12f09/stylegan2-architecture.png 148w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/e4a3f/stylegan2-architecture.png 295w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/fcda8/stylegan2-architecture.png 590w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/efc66/stylegan2-architecture.png 885w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/c83ae/stylegan2-architecture.png 1180w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/af756/stylegan2-architecture.png 1639w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>그리고 한 가지 더, <em>AdaIN</em> (normalize &#x26; mod) 를 <em>mean</em>, <em>std</em> 둘 다가 아닌 <em>std</em> 에만 적용 하는 것만으로도 충분 하다고 합니다.</p>\n<h3 id=\"instance-normalization-revisited\" style=\"position:relative;\"><a href=\"#instance-normalization-revisited\" aria-label=\"instance normalization revisited permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>instance normalization revisited</h3>\n<p>이전 <em>instance normalization</em> 은 style 에 너무 strong 한 영향을 끼쳤었고, 이를 그럼 어떻게 scale-specific 영향을 style 에 그래도 주면서, 좀 relaxing 할 수 있을까 했는데,</p>\n<ol>\n<li>일단 <em>batch normalization</em> 은 안됨. (small mini-batches 에서 high-resolution synthesis 엔 부적합)</li>\n<li>그냥 <em>instance normalization</em> 제거. -> 실제로 성능 증가 (효과있음)</li>\n</ol>\n<p>그런데, 이렇게 제거 해 버리는 것은, scale-specific 보다 style cumulative 한 거에 영향을 주었고,\nStyleGAN 의 <strong>controllability</strong> 를 잃어 버리게 됐답니다.</p>\n<p>그래서 새로운 방법을 제안했는데, <strong>controllability</strong> 를 유지하면서, <em>instance normalization</em> 으로 인한 artifacts 는 제거하는 방법.</p>\n<p><em>modulation</em>, <em>convolution</em>, <em>normalization</em> 에서 <em>modulation</em> 부분을 생각 해 보면,</p>\n<p>들어오는 style 에 의해서 (위 그림에서 A) <em>modulation</em> 각 convolution 의 feature map 을 scale 하는데,\n이 부분을 아래처럼 구현이 가능 합니다.</p>\n<blockquote>\n<p><em><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><msub><mi mathvariant=\"normal\">‘</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w`_{i,j,k} = s_i \\cdot w_{i,j,k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9805em;vertical-align:-0.2861em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\"><span class=\"mord\">‘</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5945em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span></span></em></p>\n</blockquote>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span> 는 일반 weight 이고, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mi mathvariant=\"normal\">‘</mi></mrow><annotation encoding=\"application/x-tex\">w`</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\">‘</span></span></span></span></span> 이 modulated weight, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span> 는 scale 인데, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span></span> 번째 input feature map, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">j,k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span> 는 output feature map 에 해당.</p>\n<p>이렇게 하게 되면, 위 처럼 제안된 <em>instance normalization</em> 은 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span> effect 를 output feature maps distribution 에서 제거할 수 있어요.</p>\n<p>결론적으로 위에 (d) 그럼처럼, 이제 <em>style block</em> 은 하나의 <em>convolution layer</em> 로 구성될 수 있네요. (conv weight 는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">s</span></span></span></span></span> 에 의해 adjust 됨)</p>\n<p>(논문 뒤엔 이거에 대한 이야기가 더 있는데 skip)</p>\n<p>그래서 아래와 같이 blob-like artifacts 들을 해결 했답니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/386f1f8ead1bc4bc789a111ab0dcf307/7de01/stylegan2-resolve-artifacts.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 103.37837837837837%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEeklEQVR42j2S3XMTVRjG9y/QCy8cLxxRGUAGrAoVdEDUOlJsK4XCFNRYIwilQGsy2e9zds/Z3XxvNwlp2aQfSZoUQ2AoSgvqIKgDMzp64YVOm5TmYze0tI1wgZe0jtOu+lycORfnnfc5v+chsl//FBn7Wcv96GQl6I2Gzn8/ev235PDg8HBieHg4nU4nEolkMpnJZKxLMplMpVLxeHxiYoJw0czBYw5b275Q+5v6p7tt+1s5LcExNEmSNE2zLGudgiDQNM1xnMfjCQaDFEX19fURRzoONbW819m8Y4K33/B0HX/npfb2A1iSOI6DEGKMBUEQRRFjDCFUFCUajeq6LklSLBYjfKEzrQfbD7+9TT3cGLQ1hT5o8J76jBNEjmVFUZRlGUIoCALGmOd5jHEkElFVVdO0bDZLaOlxV/9XHW37++0t57sP8C1bqZPHoYhYlgEAyLIMViVJEgAAITQwMBCLxVRVPXfuHCF7gydo+WBbW0/DFrLl9eY3tjc2NWKEKIriOE4QBI7jrDGWZSGE4XBY0zSE0NmzZwmK7W5+v6H9wN6d29Z1nrANjOQA43C7PQghWZYlSVIUxe12BwIBj8cjy7LX6+3t7fX5fENDQ8RHXS3Prn/8lfq6ul0bTlJ7FKHr2BGb2+1WFEVVVUEQZFn2+/3hcDgUCgEAEomErusMw+i6Tuw99O5ruzYCxcH1Rji/+vTzazdt3hwMBkVRVBQFY2xlE41GVVW1ECKEaJoeHBwkICY3vVy//+Ouhn22J5565rl16zhA+nw+653f77dsu91udpW/oigMw7hcrng8ToyNjdXVvbh2zZrTJ47GIt5ffrhy+8Y1hFYWWp8EAFg5MwzD8zxCCEJIUdSK7Zs3v00kB/v6tS/PJ369ff3Gd+Pp5IAsK7IsY4wRQpYFr9eLV4UQwhgDAFaAXbpyNX1x3KX4Wf2MwlNQlGhRFgWBZVkrYatqiqIAACiKEkVRkiSn07nSsG9ujlwcC41divgvXew43d3x1pZe4XOWB06nk/lPGGPLP4TQ4XAoipLL5S5cuECcidHHmzZIfqHtSGfzq3WHW/fQrlMcx4uiyLKsLMs+nw9CGAgEbt26NTIy4nK5Ll++nEqlMpkM0d198oUnH9tZv3HH1k2t9etPs8AhBUQIpdVWa5qGMaZp2ufzjY+P67qOMc5mswCAcDhMjOYE+yfbXT27Ozt3NTVutnXZqUg/EiAAgOdX9lvdFASBJEmGYUiS5Hm+p6cnFAoR167Ko+kPcyNHR+P2TMo++oV9KOmAEK/O8gzDWJWWJAlC+D8/kiRXgN2v3flz7o/7c5MP5qYe3Ju8v/D7vbnJQmF6amoqn88X8vmpyal8vlAsFosz/6pUKk1PT5umSZjmbLlSnZ9fvFOs3Jkplyt3DaNarVbL5VK1Wl1cXDQMwzSMSqVSLJZmZ2drtdrC/PzCwkKtViPuVs1icaZSqRiGMXu3WijkTdM0DNM0zUKhYJrVcrlsGEapVCrOzDx8+Nffy8uPHj1aWlpaXlr+BxkosjsvzN2VAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/386f1f8ead1bc4bc789a111ab0dcf307/fcda8/stylegan2-resolve-artifacts.png\"\n        srcset=\"/static/386f1f8ead1bc4bc789a111ab0dcf307/12f09/stylegan2-resolve-artifacts.png 148w,\n/static/386f1f8ead1bc4bc789a111ab0dcf307/e4a3f/stylegan2-resolve-artifacts.png 295w,\n/static/386f1f8ead1bc4bc789a111ab0dcf307/fcda8/stylegan2-resolve-artifacts.png 590w,\n/static/386f1f8ead1bc4bc789a111ab0dcf307/7de01/stylegan2-resolve-artifacts.png 794w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"etc-1\" style=\"position:relative;\"><a href=\"#etc-1\" aria-label=\"etc 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>etc</h3>\n<p>training technique 들도 몇 개 소개되었는데, 그렇게 여러 기술들이 덕지덕지 적용되지 않고 깔-끔 합니다.</p>\n<p>다른 거 붙이는 거 비해 computation cost 가 적게들고, memory 사용량 down 등의 이유를 들었네요.</p>\n<ol>\n<li>Loss : logistic loss</li>\n<li>Lazy Regularization : <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">R_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> regularization (every 16 mini-batches 마다 1 번)</li>\n<li>Path Length Regularization : poor local conditioning 을 피해기 위한 method 인데, Jacobian Matric 연산의 heaviness 를 해결하기 위해 identity 등을 사용 하는 등 이야기가 논문에 나옵니다.</li>\n</ol>\n<p>결론적으로, 조금 더 reliable 하고 consistently 동작하는 모델이 됐다고 캅니다.</p>\n<blockquote>\n<p><em><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>E</mi><mrow><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mtext> </mtext><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"normal\">∥</mi><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mi mathvariant=\"normal\">∥</mi><msub><mi mathvariant=\"normal\">∥</mi><mn>2</mn></msub><mo>−</mo><mi>α</mi><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">E_{x,y~N(0, I)}(\\|\\|J^T_wy\\|\\|_2 - \\alpha)^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1965em;vertical-align:-0.3552em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace nobreak mtight\"><span class=\"mtight\"> </span></span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">0</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">I</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\">∥∥</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-2.453em;margin-left:-0.0962em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∥</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span></em></p>\n</blockquote>\n<p><em><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>J</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">J_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.0962em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></em> 가 orthogonal matrix 니, 이런 matrix 는 lengths 를 보존하겠죠? (어떤 dimension 에 대해서 squeezing 없고)</p>\n<p>constant <em><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span></em> 가 training 중 path lengths 의 exponential moving average 값을 optimize 되게 해 줍니다</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<p>FFHQ / LSUN Car dataset 에서 퍼포먼스는 아래와 같아요. 총 4가지 metrics 을 사용해서 evaluate 했습니다.</p>\n<ol>\n<li>Frechet Inception Distance (FID)</li>\n<li>Perceptual Path Length (PPL)</li>\n<li>Precision</li>\n<li>Recall</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/213052fe97cf0f4fbd9233da3ed26bcd/61583/stylegan2-results.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 27.7027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABDklEQVR42l2M23KDIBRF/f+/i44tHBBBUFBo5JITMZlJO+n0qfthvaw1u+GcCyFijnhHvKH3vpTStW3f95fLxXu/bVuMMYSQUuq6DgDatl3XNQTfMIC+73vygRW/Xy9r7XEczrnjOLQxN0SjdSk377fzrPNscilSjoiotW6klISQcRzXdTvPc9/3x+Npra21huBrrc45RAxfX4/nM4SAeHe/1jnXaD0BpcuyeO+dc6N4v2itrbVSjtZazvk8z0opa5dRynk2SqllWaSUDQAoo4QWAx8opYwxIQRjDAA+yXucc8YYpXQY/gIAIIQA0EYpZYxWk5ymKedUSsk5x7inlPb9P3POKcWccynler3+AG9yRMhLjb2YAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/213052fe97cf0f4fbd9233da3ed26bcd/fcda8/stylegan2-results.png\"\n        srcset=\"/static/213052fe97cf0f4fbd9233da3ed26bcd/12f09/stylegan2-results.png 148w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/e4a3f/stylegan2-results.png 295w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/fcda8/stylegan2-results.png 590w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/efc66/stylegan2-results.png 885w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/c83ae/stylegan2-results.png 1180w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/61583/stylegan2-results.png 1616w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>이 외에도 이전 StyleGAN 과 PPL distribution 차이 여러 가지 analysis 가 있습니당</p>\n<p>나머지는 논문 참고하세요~ (<del>귀찮</del>)</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>이번 논문도 엄청 재미있는 approach 들이 많아서 재밌었는데,\n매번 느끼지만 nvidia labs 는 이전 연구들의 root cause 를 잘 잡고 좋은 결과들을\n매번 보여주네요.</p>\n<p>결론 : 마음에 드는 논문</p>","excerpt":"TL;DR 이번 포스팅에서는 리뷰할 논문은 지난 19년 11월에 나온 StyleGAN v2를 리뷰 해 보겠습니다 StyleGAN 에 이어서 2 번째 논문인데, 이번 버전에서는 어떤 문제점들을 어떻게 해결했는지를 한번 보려고 합니다! 아래는 Style…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n<ul>\n<li><a href=\"#blob-like-artifacts\">blob-like artifacts</a></li>\n<li><a href=\"#artifacts-related-to-progressive-growing\">artifacts related to progressive growing</a></li>\n<li><a href=\"#metrics-for-evaluating-gan-performance\">metrics for evaluating GAN performance</a></li>\n<li><a href=\"#etc\">etc</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"#removing-normalization-artifacts\">removing normalization artifacts</a></li>\n<li><a href=\"#generator-architecture-revisited\">generator architecture revisited</a></li>\n<li><a href=\"#instance-normalization-revisited\">instance normalization revisited</a></li>\n<li><a href=\"#etc-1\">etc</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiment-result\">Experiment Result</a></p>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/StyleGANv2/"},"frontmatter":{"title":"StyleGAN-v2 - Analyzing and Improving the Image Quality of StyleGAN","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["StyleGAN","GAN"],"update":"Mar 14, 2020"},"timeToRead":4}},"pageContext":{"slug":"/StyleGANv2/","series":[],"lastmod":"2020-03-14"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}