{"componentChunkName":"component---src-templates-post-tsx","path":"/StyleGANv2/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서는 리뷰할 논문은 지난 19년 11월에 나온 <strong>StyleGAN v2</strong>를 리뷰 해 보겠습니다</p>\n<p>StyleGAN 에 이어서 2 번째 논문인데, 이번 버전에서는 어떤 문제점들을 어떻게 해결했는지를 한번 보려고 합니다!</p>\n<p>아래는 StyleGAN v2 로 생성한 이미지들 입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5113e9adaf42d411c678dbce0d576441/2bef9/stylegan2-teaser-1024x256.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAAsSAAALEgHS3X78AAABPElEQVQY0wExAc7+AI6BVRgRDR8YEjsvI6udZkE+DT0sGDwmIiEKAyASAmFLJ4VxV4J0cnBiTravd8LNmbOxoKmjkMvMrOTmxwCumFV3Sim2cUmebj+emkNNPwSEb3Ckf3wvFQwzFQlTSDeagHLlycGAZV1hXSmmtIney8LiwrS4roy0zIUAv6FSsHVFyHdOy5lXva1JtHkLjWxmb1NSOBsTaDEKal41m4N/2r64sZGJdGVKtKWBzq6cuIx8nYxffZ9LAHBfNoNVM7dtSodpRJaCSLB4IaJ7cYJdYTAVDlQvCGRZKnZmY9+5r45wZ2RXNsW7lbmUf5p1bIZ6UWyNSQAfICQqIR8/LSMaGh41NDySipluWFh3VlUQBAAQDAxZOjB4XFXRsqNqUEdbTiOKkYuLi5Z6aFx1h1ZOXzwY534exOOUiAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/5113e9adaf42d411c678dbce0d576441/fcda8/stylegan2-teaser-1024x256.png\"\n        srcset=\"/static/5113e9adaf42d411c678dbce0d576441/12f09/stylegan2-teaser-1024x256.png 148w,\n/static/5113e9adaf42d411c678dbce0d576441/e4a3f/stylegan2-teaser-1024x256.png 295w,\n/static/5113e9adaf42d411c678dbce0d576441/fcda8/stylegan2-teaser-1024x256.png 590w,\n/static/5113e9adaf42d411c678dbce0d576441/efc66/stylegan2-teaser-1024x256.png 885w,\n/static/5113e9adaf42d411c678dbce0d576441/2bef9/stylegan2-teaser-1024x256.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1912.04958.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>official implementation : <a href=\"https://github.com/NVlabs/stylegan2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>요건 이전 버전 StyleGAN paper 입니다.</p>\n<ul>\n<li>StyleGAN : <a href=\"https://arxiv.org/pdf/1812.04948.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></li>\n</ul>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이번 논문에서는 이전에 발표한 StyleGAN 의 artifacts 들에 대해 지적하면서 시작합니다.</p>\n<p>요약 해 보면 크게 3개의 문제점을 지적 / 개선 / 해결 했는데, </p>\n<ol>\n<li>blob-like artifacts</li>\n<li>artifacts related to progressive growing</li>\n<li>metrics for evaluating GAN performance</li>\n</ol>\n<h3 id=\"blob-like-artifacts\" style=\"position:relative;\"><a href=\"#blob-like-artifacts\" aria-label=\"blob like artifacts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>blob-like artifacts</h3>\n<p>StyleGAN 에서 <em>Instance Normalization</em> 이 아래 이미지 처럼 water droplet 과 같은 artifacts 를 발생한다고 합니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/18408e2b9433f5d9ea020c0b9c0a5937/280a1/droplet-llike-artifacts.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.243243243243242%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA+0lEQVQI1w3IX0uDQAAAcL9fBVEbrZEIW2BNcG7Q1rk7Tz2tFrOsUSrd7Zqmlkp7aEHQQ/TnO/TSh2m/x5/wlbGpb11Rv+TzlNLHotCxpnZbBhoWy7csy584L/LcNfGlg+XGttvrVmX5kKaLJBH+qjtmq5IiEptACAmxd5tbh7KkyJ0jgEarGQwgAKrcZhcnUG7K9XWEjGMAXMsSfumkBIrW2bcdByF0Pj6r13Y21jb3GjU47BsYQ4RM0zy1MBn0555ZJVTXRys2IcLPtftxoH0CL7hnURDEcYwgbEnia8G/319uboMwDDmfTb1Jry3O/PFy8UwpjaKIMfYPSRd+22dO3n4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/18408e2b9433f5d9ea020c0b9c0a5937/fcda8/droplet-llike-artifacts.png\"\n        srcset=\"/static/18408e2b9433f5d9ea020c0b9c0a5937/12f09/droplet-llike-artifacts.png 148w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/e4a3f/droplet-llike-artifacts.png 295w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/fcda8/droplet-llike-artifacts.png 590w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/efc66/droplet-llike-artifacts.png 885w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/c83ae/droplet-llike-artifacts.png 1180w,\n/static/18408e2b9433f5d9ea020c0b9c0a5937/280a1/droplet-llike-artifacts.png 1545w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>generator</em> 의 activation map 을 보면 (오른쪽 사진들) 자국 같은 것 들이 보일텐데, 설계상 때문에 요게 문제가 됐다는 겁니다.</p>\n<p>그래서 StyleGANv2 에서는 새로운 normalization method 를 사용해서 이 문제를 해결합니다.</p>\n<h3 id=\"artifacts-related-to-progressive-growing\" style=\"position:relative;\"><a href=\"#artifacts-related-to-progressive-growing\" aria-label=\"artifacts related to progressive growing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>artifacts related to progressive growing</h3>\n<p>high-resolution GAN 에서 stable 한 훈련을 위해 low resolution 부터 훈련을 시작하는 progressive 한 훈련 방식을 해 왔었는데,\n이전에는 각 resolution 마다 같은 network 구조를 사용했는데, 다른 resolution 을 훈련할 때는 다른 network topology 를 사용해야 한다는 말 입니다.</p>\n<p>매 resolution 마다 같은 구조가 아닌 다른 구조로 학습을 하게 되면, 각 resolution 에 맞게 더 효율적으로 훈련할 수 있다는 논문피셜 입니다.\n(당연한 이야기긴 하지만)</p>\n<h3 id=\"metrics-for-evaluating-gan-performance\" style=\"position:relative;\"><a href=\"#metrics-for-evaluating-gan-performance\" aria-label=\"metrics for evaluating gan performance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>metrics for evaluating GAN performance</h3>\n<p>GAN 생성 이미지 quality 를 측정하기 위해 여러 metric 들을 사용하는데 (e.g. FID, precision, recall),\n이런 metric 들의 문제점을 제기하고 새로운 gan metric 을 사용해서 performance 를 측정했다고 합니다.</p>\n<p>간단하게 설명 해 보면, 위에 소개된 이전 metric 들은 inception v3 같은 base network 에 기반해서, 전반적인 texture 보다 shape 같은 것에 집중을 하는데,\n결론적으로 이미지 quality 전반적인 면을 capture 하지 못한다는 말 입니다.</p>\n<p>그래서 이런 문제가 어느 정도 해결 한 perceptual path length (PPL) metric 을 사용했다고 합니다.</p>\n<h3 id=\"etc\" style=\"position:relative;\"><a href=\"#etc\" aria-label=\"etc permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>etc</h3>\n<p>마지막으론 latent space <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 가 더 잘된다고 하네요</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<p>결론적으로 아래와 같은 변경 사항들로 </p>\n<ol>\n<li>characteristic artifacts 들을 제거하고 </li>\n<li>full controllability 를 유지한다.</li>\n</ol>\n<h3 id=\"removing-normalization-artifacts\" style=\"position:relative;\"><a href=\"#removing-normalization-artifacts\" aria-label=\"removing normalization artifacts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>removing normalization artifacts</h3>\n<p>위에 introduction 에서 <em>instance normalization</em> 때문에 blob-like 한 artifacts 가 생긴다고 했는데,\n최종 이미지 (1024x1024) 에서는 안 보일 수 있어도, 중간 이미지 (64x64) 쯤 부터 발생하는 걸 볼 수 있는데,\n이런 부분을 <em>discriminator</em> 에서 잡을 수 있어야 하겠죠?</p>\n<p><em>AdaIN</em> 같은 operation 이 각 features 별 <em>mean</em>, <em>variance</em> 를 따로따로 normalize 하기 떄문이라 원인을 밝힙니다.\n(논문에선 generator 가 의도적으로 signal strength 정보를 이전 <em>IN</em> 으로부터 sneak 한다고 표현돼 있어요)</p>\n<p>결론은 이런 normalization step 이 없어지면 이런 droplet artifacts 가 없어질 거라 합니다.</p>\n<h3 id=\"generator-architecture-revisited\" style=\"position:relative;\"><a href=\"#generator-architecture-revisited\" aria-label=\"generator architecture revisited permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>generator architecture revisited</h3>\n<p>아래가 과거 (StyleGAN) / 현재 (StyleGANv2) <em>generator</em> architecture 비교 샷 인데,\n<em>AdaIN</em> operation 을 normalization, modulation step 으로 분리해서 보여줬네요.</p>\n<p>과거에는 <em>bias</em> 하고 <em>noise</em> 를 <em>style block</em> 에 적용 해 줬는데, 이런 상대적인 영향이 게 style magnitude 에 반비례하게 적용된다고 합니다.</p>\n<p>그래서 이런 operation 을 <em>style block</em> 밖으로 빼면서 조금 더 predictable 한 결과를 가져갈 수 있었다고 해요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dcd29a1c419acffd55110ffd0c4a35d3/af756/stylegan2-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.891891891891895%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABlklEQVQY0z1R2ZLaMBD0//9J8pRUqCJJUUvYWhxgFwOWjWW8xpIB37ctycIH0e5D+mmmZrqrZ1q6d/zd1VVzE6W3cRz7vi/L6oyueVEQWj0+MQ7jf3y04zgMgyikmmTA1NeqUpOivbMsi0wHfXmSIcLDcE+z6OZd5cPufMWCVDUF8sKyqrIsEXyJtrWJtTWQg8Tt+y7wfc1Up39nwDqIEWX0ckOzrXxEkHNS1cUKWCes265KGZEILcERrt72dV33wz1OfMO0v/560U/O4zGkeeg4zs+FDAyTMqFFPD9EGEMIqyaXuo7v9NVcnnohFrekSaoau8n8297YtJxw3jnYmiwmO2M79F1eJM+KIW8Xy9fflDVSQwrowB0EhFVCyA8u+sn6sdyYGPU9T7IIYefp9c1EZ+GRceZHSZbncRw1pJSEwNE+LJVnL0SCHAQ+gIfvfyZ7qAifhBD35kzXc2BrXdemeTRba+CkHN83rKUSa2s/SG0ccM7F91vGsrzULDdK88/getKQy9WPk7ShhVhglMbJRwQinX+VKbAsvWiGdAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/dcd29a1c419acffd55110ffd0c4a35d3/fcda8/stylegan2-architecture.png\"\n        srcset=\"/static/dcd29a1c419acffd55110ffd0c4a35d3/12f09/stylegan2-architecture.png 148w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/e4a3f/stylegan2-architecture.png 295w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/fcda8/stylegan2-architecture.png 590w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/efc66/stylegan2-architecture.png 885w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/c83ae/stylegan2-architecture.png 1180w,\n/static/dcd29a1c419acffd55110ffd0c4a35d3/af756/stylegan2-architecture.png 1639w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>그리고 한 가지 더, <em>AdaIN</em> (normalize &#x26; mod) 를 <em>mean</em>, <em>std</em> 둘 다가 아닌 <em>std</em> 에만 적용 하는 것만으로도 충분 하다고 합니다.</p>\n<h3 id=\"instance-normalization-revisited\" style=\"position:relative;\"><a href=\"#instance-normalization-revisited\" aria-label=\"instance normalization revisited permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>instance normalization revisited</h3>\n<p>이전 <em>instance normalization</em> 은 style 에 너무 strong 한 영향을 끼쳤었고, 이를 그럼 어떻게 scale-specific 영향을 style 에 그래도 주면서, 좀 relaxing 할 수 있을까 했는데,</p>\n<ol>\n<li>일단 <em>batch normalization</em> 은 안됨. (small mini-batches 에서 high-resolution synthesis 엔 부적합)</li>\n<li>그냥 <em>instance normalization</em> 제거. -> 실제로 성능 증가 (효과있음)</li>\n</ol>\n<p>그런데, 이렇게 제거 해 버리는 것은, scale-specific 보다 style cumulative 한 거에 영향을 주었고,\nStyleGAN 의 <strong>controllability</strong> 를 잃어 버리게 됐답니다.</p>\n<p>그래서 새로운 방법을 제안했는데, <strong>controllability</strong> 를 유지하면서, <em>instance normalization</em> 으로 인한 artifacts 는 제거하는 방법.</p>\n<p><em>modulation</em>, <em>convolution</em>, <em>normalization</em> 에서 <em>modulation</em> 부분을 생각 해 보면,</p>\n<p>들어오는 style 에 의해서 (위 그림에서 A) <em>modulation</em> 각 convolution 의 feature map 을 scale 하는데,\n이 부분을 아래처럼 구현이 가능 합니다.</p>\n<blockquote>\n<p><em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi><msub><mi mathvariant=\"normal\">‘</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w`_{i,j,k} = s_i \\cdot w_{i,j,k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.980548em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\"><span class=\"mord\">‘</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.59445em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span></em></p>\n</blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 는 일반 weight 이고, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi><mi mathvariant=\"normal\">‘</mi></mrow><annotation encoding=\"application/x-tex\">w`</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\">‘</span></span></span></span> 이 modulated weight, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span> 는 scale 인데, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> 번째 input feature map, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">j,k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 는 output feature map 에 해당. </p>\n<p>이렇게 하게 되면, 위 처럼 제안된 <em>instance normalization</em> 은 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span> effect 를 output feature maps distribution 에서 제거할 수 있어요.</p>\n<p>결론적으로 위에 (d) 그럼처럼, 이제 <em>style block</em> 은 하나의 <em>convolution layer</em> 로 구성될 수 있네요. (conv weight 는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span></span></span></span> 에 의해 adjust 됨)</p>\n<p>(논문 뒤엔 이거에 대한 이야기가 더 있는데 skip)</p>\n<p>그래서 아래와 같이 blob-like artifacts 들을 해결 했답니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/386f1f8ead1bc4bc789a111ab0dcf307/7de01/stylegan2-resolve-artifacts.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 103.37837837837837%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEDElEQVQ4yyVT309TZxg+f8FudjONRnZlNnVBJBnClqJjU1aDY24OCgwoATEK2J9QetrT055C21PojzGBlmJbSigWrC6bSrI5IM4Lk+2iWJZaKra0xdrK7vmR7Ok5T5o37/f1e973ed7vfMQvK//YQ0/H7j9XUCxptFtnHgf+WPf7vNMcZmZmvN5iPjs7i8Tn83k5eDyeUChEMMOmb8U3m0UtTP1n7k5he90FhXmKJNVyuby/v1+lUg1woCgKEUuGYViWVavVQ0NDhLTnWq3wYptQsCAXLZt7++s+vXzpIk3rBwcHNRoNTdNaDnq9HktEh8MxPj4+PDyMEsRtt6e+QXTl/FnT1a/YFqFNVDPU1UxqKbQEx2AwIPJVSJJEf7vdbrVabTbb1NQU4Qotycbui1vF5oYv78mbdHVnlF2tGkoHhdAGMphI0BNkFEJbl8s1MjJSJDt+nuiSU40/tncLSlWXq747V1l9XkBpNbxhnFZz0Ol0MIJkdHQUbeHcYrEQJjN9qa5G1HCl+uyJ5oavXf675IDMaGRwWs+B4WA2mzEhCEEEDcphnpBoO0o+eq/09Imy6k9E16tsFmlnWyNjNOIcTqAE3IKJbk6nE8onJyfdbje8YJPo6m0+VV6iILuVZotmxH66ouLQ4cMmkwmzMRqNYCKiFZgQjE0UhRw4gm1ibMJysrTsG1Fn7Q8dh0qOf3D0WJ/0BstaKKooG1VAhmzcDTxjBMjBVCgURfLq6kplZeWxI0faWxrtLL38cP7vZ79DNgB7OMoPGYX4gcEIlvhgigP769myZ9pld7Dz/onnq49Xlh8FZ++ACHnQTHFAgs6IKAEydsDHFIh7v/4WDD9UMOzA7Z90ZL9eS8u0BvzP3zN/yQAk8A2xg7qQXez859NQKGxbWLA7H4TaVGSToMykvKahKKlEwl81ImhwgRJIpFIp+MFgEG+G8M1buutPGmhJ0y2lsKL8+9pzcsl1tbo4G5iESHTAafhfWlpaXFyUSCRzc3Mg48IIHa09dfT9z88cF1SVC0s/vCnpu8VYtRpSz32Y+PrRU6lUQnY4HMZjxGYgEOBHQDx4ZOsQV/TdqOnt+aL2wsci8VXJqJOmdRgydPKzASABPvlLgiKZTIYd4skTp+9O47y/MzDe5p1u9QfE0/4ezJWfE/+GUYh/kmqSBBNLDKI4sHeFRD4bKWSi7zLRnWy0sB3Jbb+IxWLr6+v/cohGi+kGhziHRCKB+CqRILLbuc3X6Xz+v81keiORTG1tp7ayQCqZTKczOzs7mUwmnU5vbaVeJ5OZTLZQKOTevMnnC7lcjkilktHoi3j8JX7JzVdra2tokEhs4BeJROLxjZexGPogRqPRt2/zBwcHu7u7e3t7+/v7/wMJU7Ffy3IWAgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/386f1f8ead1bc4bc789a111ab0dcf307/fcda8/stylegan2-resolve-artifacts.png\"\n        srcset=\"/static/386f1f8ead1bc4bc789a111ab0dcf307/12f09/stylegan2-resolve-artifacts.png 148w,\n/static/386f1f8ead1bc4bc789a111ab0dcf307/e4a3f/stylegan2-resolve-artifacts.png 295w,\n/static/386f1f8ead1bc4bc789a111ab0dcf307/fcda8/stylegan2-resolve-artifacts.png 590w,\n/static/386f1f8ead1bc4bc789a111ab0dcf307/7de01/stylegan2-resolve-artifacts.png 794w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"etc-1\" style=\"position:relative;\"><a href=\"#etc-1\" aria-label=\"etc 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>etc</h3>\n<p>training technique 들도 몇 개 소개되었는데, 그렇게 여러 기술들이 덕지덕지 적용되지 않고 깔-끔 합니다.</p>\n<p>다른 거 붙이는 거 비해 computation cost 가 적게들고, memory 사용량 down 등의 이유를 들었네요.</p>\n<ol>\n<li>Loss : logistic loss</li>\n<li>Lazy Regularization : <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">R_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> regularization (every 16 mini-batches 마다 1 번)</li>\n<li>Path Length Regularization : poor local conditioning 을 피해기 위한 method 인데, Jacobian Matric 연산의 heaviness 를 해결하기 위해 identity 등을 사용 하는 등 이야기가 논문에 나옵니다.\n결론적으로, 조금 더 reliable 하고 consistently 동작하는 모델이 됐다고 캅니다.</li>\n</ol>\n<blockquote>\n<p><em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>E</mi><mrow><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mtext> </mtext><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∥</mi><mi mathvariant=\"normal\">∥</mi><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mi mathvariant=\"normal\">∥</mi><msub><mi mathvariant=\"normal\">∥</mi><mn>2</mn></msub><mo>−</mo><mi>α</mi><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">E_{x,y~N(0, I)}(\\|\\|J^T_wy\\|\\|_2 - \\alpha)^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1965309999999998em;vertical-align:-0.3551999999999999em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace nobreak mtight\"><span class=\"mtight\"> </span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">0</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07847em;\">I</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\">∥</span><span class=\"mord\">∥</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.09618em;\">J</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.09618em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∥</span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></em></p>\n</blockquote>\n<p><em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>J</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">J_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.09618em;\">J</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></em> 가 orthogonal matrix 니, 이런 matrix 는 lengths 를 보존하겠죠? (어떤 dimension 에 대해서 squeezing 없고)</p>\n<p>constant <em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span></em> 가 training 중 path lengths 의 exponential moving average 값을 optimize 되게 해 줍니다</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<p>FFHQ / LSUN Car dataset 에서 퍼포먼스는 아래와 같아요. 총 4가지 metrics 을 사용해서 evaluate 했습니다.</p>\n<ol>\n<li>Frechet Inception Distance (FID)</li>\n<li>Perceptual Path Length (PPL)</li>\n<li>Precision</li>\n<li>Recall</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/213052fe97cf0f4fbd9233da3ed26bcd/61583/stylegan2-results.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 27.7027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA+ElEQVQY0zVQ7W6EIBDk/R/ORHP1AAEBGwsFPCVG41fTuZruj2HZmZ3sLuGcW2PznJdlyVMOMY7jWBTFk9KyLJF778fxFWMchgGVRoiqqtIwfHtPKKUfjweT7LzOfd/7vt+27UZjNFAbDV9YgO2snee5bdtlXY3WBE6MM611GhJo2J/niebjOOANvI1Sij/XFUJY19U5B6X7cgT2nHHvfAwBulaplBK80KmUgq5pOOrGWuSov3NjkIMlUirbW91pKWRd1ygpKYUQlD4pewP+nDcNJsSMjOHFmTAsWNIqrKBUq7ruc5qmnDPw9R842I1Y56bGPw0ipvQLoK5E4mEDCI4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/213052fe97cf0f4fbd9233da3ed26bcd/fcda8/stylegan2-results.png\"\n        srcset=\"/static/213052fe97cf0f4fbd9233da3ed26bcd/12f09/stylegan2-results.png 148w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/e4a3f/stylegan2-results.png 295w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/fcda8/stylegan2-results.png 590w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/efc66/stylegan2-results.png 885w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/c83ae/stylegan2-results.png 1180w,\n/static/213052fe97cf0f4fbd9233da3ed26bcd/61583/stylegan2-results.png 1616w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>이 외에도 이전 StyleGAN 과 PPL distribution 차이 여러 가지 analysis 가 있습니당</p>\n<p>나머지는 논문 참고하세요~ (<del>귀찮</del>)</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>이번 논문도 엄청 재미있는 approach 들이 많아서 재밌었는데,\n매번 느끼지만 nvidia labs 는 이전 연구들의 root cause 를 잘 잡고 좋은 결과들을\n매번 보여주네요.</p>\n<p>결론 : 마음에 드는 논문</p>","excerpt":"TL;DR 이번 포스팅에서는 리뷰할 논문은 지난 19년 11월에 나온 StyleGAN v2를 리뷰 해 보겠습니다 StyleGAN 에 이어서 2 번째 논문인데, 이번 버전에서는 어떤 문제점들을 어떻게 해결했는지를 한번 보려고 합니다! 아래는 Style…","tableOfContents":"<ul>\n<li><a href=\"/StyleGANv2/#tldr\">TL;DR</a></li>\n<li><a href=\"/StyleGANv2/#related-work\">Related Work</a></li>\n<li>\n<p><a href=\"/StyleGANv2/#introduction\">Introduction</a></p>\n<ul>\n<li><a href=\"/StyleGANv2/#blob-like-artifacts\">blob-like artifacts</a></li>\n<li><a href=\"/StyleGANv2/#artifacts-related-to-progressive-growing\">artifacts related to progressive growing</a></li>\n<li><a href=\"/StyleGANv2/#metrics-for-evaluating-gan-performance\">metrics for evaluating GAN performance</a></li>\n<li><a href=\"/StyleGANv2/#etc\">etc</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/StyleGANv2/#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"/StyleGANv2/#removing-normalization-artifacts\">removing normalization artifacts</a></li>\n<li><a href=\"/StyleGANv2/#generator-architecture-revisited\">generator architecture revisited</a></li>\n<li><a href=\"/StyleGANv2/#instance-normalization-revisited\">instance normalization revisited</a></li>\n<li><a href=\"/StyleGANv2/#etc-1\">etc</a></li>\n</ul>\n</li>\n<li><a href=\"/StyleGANv2/#experiment-result\">Experiment Result</a></li>\n<li><a href=\"/StyleGANv2/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/StyleGANv2/"},"frontmatter":{"title":"StyleGAN-v2 - Analyzing and Improving the Image Quality of StyleGAN","date":"Mar 14, 2020","tags":["Deep-Learning"],"keywords":["StyleGAN","GAN"],"update":"Mar 14, 2020"},"timeToRead":6}},"pageContext":{"slug":"/StyleGANv2/","series":[],"lastmod":"2020-03-14"}},"staticQueryHashes":["2027115977","694178885"]}