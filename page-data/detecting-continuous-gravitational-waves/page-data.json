{"componentChunkName":"component---src-templates-post-tsx","path":"/detecting-continuous-gravitational-waves/","result":{"data":{"markdownRemark":{"html":"<ul>\n<li>Original Post : <a href=\"https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves/discussion/375927\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves/discussion/375927</a></li>\n</ul>\n<h2 id=\"data\" style=\"position:relative;\"><a href=\"#data\" aria-label=\"data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data</h2>\n<h3 id=\"pre-processing\" style=\"position:relative;\"><a href=\"#pre-processing\" aria-label=\"pre processing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pre-Processing</h3>\n<p>In my experiment, <a href=\"https://www.kaggle.com/code/laeyoung/g2net-large-kernel-inference\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">preprocessing</a> (<code class=\"language-text\">normalize</code> function) works better than the power spectrogram. It improves the score by about +0.02 on CV/LB. After normalizing the signal, take a mean over the time axis. The final shape is (360, 360).</p>\n<h3 id=\"simulation\" style=\"position:relative;\"><a href=\"#simulation\" aria-label=\"simulation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Simulation</h3>\n<p>Generating samples is the most crucial part of boosting the score. I can get 0.761 on the LB with a single model.</p>\n<p>In short, signal depth (<code class=\"language-text\">sqrtSX / h0</code>) takes a huge impact. I generated 100K samples (50K positives, 50K negatives) and uniformly sampled the signal depth between 10 and 100. <code class=\"language-text\">cosi</code> parameter is uniformly sampled (-1, 1).</p>\n<table>\n<thead>\n<tr>\n<th>signal depth</th>\n<th>LB score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10 ~ 50</td>\n<td>0.73x ~ 0.74x</td>\n</tr>\n<tr>\n<td>10 ~ 80</td>\n<td>0.75x</td>\n</tr>\n<tr>\n<td>10 ~ 100</td>\n<td>0.761</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"augmentations\" style=\"position:relative;\"><a href=\"#augmentations\" aria-label=\"augmentations permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Augmentations</h3>\n<p>Also, I've worked on the augmentations for much time. Here's a list.</p>\n<ol>\n<li>v/hflip</li>\n<li>shuffle channel</li>\n<li>shift on freq-axis</li>\n<li>denoise a signal (subtract corresponding noise from the signal)</li>\n<li>add noises\n<ul>\n<li>Guassian N(0, 1e-2)</li>\n<li>mixed (add or concatenate) with another (stationary) noise(s)</li>\n</ul>\n</li>\n<li>add vertical line artifact(s).</li>\n<li>SpecAugment</li>\n<li>mixup (alpha 5.0)\n<ul>\n<li>perform <code class=\"language-text\">or</code> mixup</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"model\" style=\"position:relative;\"><a href=\"#model\" aria-label=\"model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model</h2>\n<p>First, I tried to search for the backbones (effnet, nfnet, resnest, convnext, vit-based) and found <code class=\"language-text\">convnext</code> works best on CV &#x26; LB score. After selecting a baseline backbone, I experimented with customizing a stem layer (e.g. large kernel &#x26; pool sizes, multiple convolutions stem with various kernel sizes) to detect the long-lasting signal effectively, but they didn't affect the performance positively.</p>\n<h2 id=\"ensemble\" style=\"position:relative;\"><a href=\"#ensemble\" aria-label=\"ensemble permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ensemble</h2>\n<p>Most of the models used at the ensemble are <code class=\"language-text\">convnext-xlarge</code> but each model trained with different variances (e.g. augmentations, simulated samples, ...) and <code class=\"language-text\">eca-nfnet-l2</code>, <code class=\"language-text\">efficientnetv2-xl</code> for one model. Every model trained on various datasets and LB score seems reliable, so I adjusted the ensemble weights by LB score.</p>\n<p>I selected the two best LB submissions (LB 0.768 PB 0.771). And the best PB that I didn't select is 0.778 (LB 0.766) (mixing all my experiments).</p>\n<h2 id=\"works\" style=\"position:relative;\"><a href=\"#works\" aria-label=\"works permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Works</h2>\n<ul>\n<li><code class=\"language-text\">convnext</code> family backbone</li>\n<li>signal depth 10 ~ 100</li>\n<li>hard augmentation</li>\n<li>pair stratified k fold\n<ul>\n<li>8 folds</li>\n<li>stratified on the target</li>\n<li><code class=\"language-text\">pair</code> means the pair (corresponding noise &#x26; signal) must be in the same fold.</li>\n</ul>\n</li>\n<li>pseudo label (smooth label)</li>\n<li>segmentation (but hard to converge on my experiment)</li>\n<li>TTA</li>\n</ul>\n<h2 id=\"not-works\" style=\"position:relative;\"><a href=\"#not-works\" aria-label=\"not works permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Not Works</h2>\n<ul>\n<li>segmentation with classification head (0.6 * bce + 0.4 * dice)\n<ul>\n<li>Actually, seg with cls works slightly better than only cls, but hard to train without loss divergence. So, I just did only cls.</li>\n</ul>\n</li>\n<li><code class=\"language-text\">cosi == 0</code>\n<ul>\n<li><code class=\"language-text\">cosi</code> is also a critical parameter to determine an SNR. I generated more samples where <code class=\"language-text\">cosi</code> is 0, but there's a score drop.</li>\n</ul>\n</li>\n<li>augmentations (not worked)\n<ul>\n<li>swap with random negatives (proposed at the past competition)</li>\n<li>random resized crop</li>\n</ul>\n</li>\n<li>Customize a stem layer with large kernel &#x26; pool sizes.</li>\n</ul>\n<p>I hope this could help you :)</p>\n<p>Happy new year!</p>","excerpt":"Original Post : https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves/discussion/375927 Data Pre-Processing Inâ€¦","tableOfContents":"<ul>\n<li>\n<p><a href=\"#data\">Data</a></p>\n<ul>\n<li><a href=\"#pre-processing\">Pre-Processing</a></li>\n<li><a href=\"#simulation\">Simulation</a></li>\n<li><a href=\"#augmentations\">Augmentations</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#model\">Model</a></p>\n</li>\n<li>\n<p><a href=\"#ensemble\">Ensemble</a></p>\n</li>\n<li>\n<p><a href=\"#works\">Works</a></p>\n</li>\n<li>\n<p><a href=\"#not-works\">Not Works</a></p>\n</li>\n</ul>","fields":{"slug":"/detecting-continuous-gravitational-waves/"},"frontmatter":{"title":"(Kaggle) Detecting Continuous Gravitational Waves - 22th (top 1%) place solution","date":"Jan 03, 2023","tags":["Deep-Learning","Kaggle"],"keywords":["european-gravitational-observatory","simulate-gravitational-waves","cnn"],"update":"Jan 19, 2023"},"timeToRead":2}},"pageContext":{"slug":"/detecting-continuous-gravitational-waves/","series":[],"lastmod":"2023-01-19"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}