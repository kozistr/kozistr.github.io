{"componentChunkName":"component---src-templates-post-tsx","path":"/default-prediction/","result":{"data":{"markdownRemark":{"html":"<ul>\n<li>Original Post : <a href=\"https://www.kaggle.com/competitions/amex-default-prediction/discussion/347996\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.kaggle.com/competitions/amex-default-prediction/discussion/347996</a></li>\n</ul>\n<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>I couldn't spend lots of time on the competition (only made 30 submissions :(). In the meantime, the competition metric is kinda noisy and we also expected a shake-up/down (not a planet-scale, but for some cases). So, my strategy is focused on protecting a shake-down as possible i can (instead of bulding new features).</p>\n<h2 id=\"overview\" style=\"position:relative;\"><a href=\"#overview\" aria-label=\"overview permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Overview</h2>\n<p>My strategy is <code class=\"language-text\">building various datasets, folds, seeds, models</code>. I'll explain them one by one.</p>\n<h3 id=\"data-pre-processing\" style=\"position:relative;\"><a href=\"#data-pre-processing\" aria-label=\"data pre processing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data (Pre-Processing)</h3>\n<p>My base dataset is based on the raddar's dataset (huge thanks to @raddar). Also, most of the pre-processing logic can be found in the <code class=\"language-text\">Code</code> section.</p>\n<p>The differences are...</p>\n<ol>\n<li>\n<p>using more lagging features (to 3 months)</p>\n</li>\n<li>\n<p>not just using a single dataset, but multiple datasets (I just added features incrementally) for the variousity.</p>\n<ul>\n<li>A dataset</li>\n<li>B dataset = A dataset + (features)</li>\n<li>C dataset = B dataset + (another features)</li>\n</ul>\n</li>\n</ol>\n<p>I didn't check the exact effectiveness of using the datasets on multiple models, however, it seems that positive effects when ensembling in my experiments.</p>\n<h3 id=\"model\" style=\"position:relative;\"><a href=\"#model\" aria-label=\"model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model</h3>\n<p>I built 6 models (3 gbtm, 3 nn) to secure the variousity and roboustness. Also, a few models (LightGBM, CatBoost) are trained on multiple seeds (1, 42, 1337) with the same training recipe. Lastly, some models are trained with 10, 20 folds.</p>\n<ul>\n<li>Xgboost</li>\n<li>CatBoost</li>\n<li>LightGBM (w/ dart, w/o dart)</li>\n<li>5-layers NN</li>\n<li>stacked bi-GRU</li>\n<li>Transformer</li>\n</ul>\n<p>Here's the best CV by the model (sorry for the LB, PB scores, I rarely submitted a single model)</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Model</th>\n<th align=\"center\">CV</th>\n<th align=\"center\">Note</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">bi-GRU</td>\n<td align=\"center\">0.787006</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">Transformer</td>\n<td align=\"center\">0.785647</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">NN</td>\n<td align=\"center\">0.789874</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\">Xgboost</td>\n<td align=\"center\">0.795940</td>\n<td align=\"center\">only using the given(?) cat features as <code class=\"language-text\">cat_features</code></td>\n</tr>\n<tr>\n<td align=\"center\">CatBoost</td>\n<td align=\"center\">0.797058</td>\n<td align=\"center\">using all <code class=\"language-text\">np.int8</code> features as <code class=\"language-text\">cat_features</code></td>\n</tr>\n<tr>\n<td align=\"center\">LighGBM</td>\n<td align=\"center\">0.798410</td>\n<td align=\"center\">w/ dart</td>\n</tr>\n</tbody>\n</table>\n<p>The CV score of the single neural network model isn't good. Nevertheless, when ensembling, It works good with the tree-based models.</p>\n<h3 id=\"blend-ensemble\" style=\"position:relative;\"><a href=\"#blend-ensemble\" aria-label=\"blend ensemble permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Blend (Ensemble)</h3>\n<p>Inspired by the discussion <a href=\"https://www.kaggle.com/competitions/amex-default-prediction/discussion/329103\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">log-odds</a>, I found weighted ensemble with log-odds probability is better than a normal weighted ensemble (I tuned the weights with <code class=\"language-text\">Optuna</code> library based on the OOF). But, one difference is not <code class=\"language-text\">ln</code>, but <code class=\"language-text\">log10</code>. In my experiments, It's better to optimize the weights with <code class=\"language-text\">log10</code>. However, It brings little boost (4th digit difference).</p>\n<p>I ensembled about 50 models, and there's no post-processing logic.</p>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h2>\n<p>The final score is</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Model</th>\n<th align=\"center\">CV</th>\n<th align=\"center\">Public LB</th>\n<th align=\"center\">Private LB</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Ensemble</td>\n<td align=\"center\"><code class=\"language-text\">0.8009</code></td>\n<td align=\"center\"><code class=\"language-text\">0.7992</code></td>\n<td align=\"center\"><code class=\"language-text\">0.8075</code></td>\n</tr>\n</tbody>\n</table>\n<p>Last day of the competition, I selected about 1600th Public LB solution (my best CV solution). Luckily, <code class=\"language-text\">Trust CV score</code> wins again :) (Actually, my best CV is also my best LB, and when the cv score increases, lb score increases, so there's little difference between best CV &#x26; LB for my cases)</p>\n<p>After the competition, I checked the correlation among the scores (CV vs Private LB, CV vs Public LB). then, I found the CV score is more correlated with Private LB than Public LB in my case.</p>\n<h3 id=\"works\" style=\"position:relative;\"><a href=\"#works\" aria-label=\"works permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Works</h3>\n<ul>\n<li>blending various models (gbtm + nn), even if there're huge CV gaps\n<ul>\n<li>e.g. nn 0.790, lgbm 0.798</li>\n</ul>\n</li>\n<li>(maybe) various datasets, models, seeds bring a robust prediction I guess</li>\n</ul>\n<h3 id=\"didnt-work\" style=\"position:relative;\"><a href=\"#didnt-work\" aria-label=\"didnt work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Didn't work</h3>\n<ul>\n<li>pseudo labeling (w/ hard label)\n<ul>\n<li>maybe <code class=\"language-text\">soft-label</code> or <code class=\"language-text\">hard label</code> with a more strict threshold could be worked i guess.</li>\n</ul>\n</li>\n<li>deeper NN models\n<ul>\n<li>5-layers nn is enough</li>\n</ul>\n</li>\n<li>num of folds doesn't matter (5 folds are enough)\n<ul>\n<li>there's no significant difference between 5 folds vs 20 folds</li>\n</ul>\n</li>\n<li>rank weighted ensemble</li>\n</ul>\n<p>I hope this you could help :) Thank you!</p>","excerpt":"Original Post : https://www.kaggle.com/competitions/amex-default-prediction/discussion/347996 TL;DR I couldn't spend lots of time on the co…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#overview\">Overview</a></p>\n<ul>\n<li><a href=\"#data-pre-processing\">Data (Pre-Processing)</a></li>\n<li><a href=\"#model\">Model</a></li>\n<li><a href=\"#blend-ensemble\">Blend (Ensemble)</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#summary\">Summary</a></p>\n<ul>\n<li><a href=\"#works\">Works</a></li>\n<li><a href=\"#didnt-work\">Didn't work</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/default-prediction/"},"frontmatter":{"title":"Default Prediction (Kaggle) - 135th place solution","date":"Aug 17, 2022","tags":["Deep-Learning"],"keywords":["kaggle","america-express","default-prediction","gbm","transformer","cnn","nn"],"update":"Jan 19, 2023"},"timeToRead":2}},"pageContext":{"slug":"/default-prediction/","series":[],"lastmod":"2023-01-19"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}