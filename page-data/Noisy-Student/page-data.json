{"componentChunkName":"component---src-templates-post-tsx","path":"/Noisy-Student/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각나서 리뷰하게 됐어요.</p>\n<p>아래는 이번 approach 가 달성한 성능인데, 이전 SOTA 에 비해서 Accuracy 가 약 2% 정도 올라갔네요.</p>\n<p>해당 이미지에는 <em>L2</em> performance 가 안올라와 있는데, <em>Noisy Student + Random Augment</em> 로 훈련한 <em>L2</em> 모델 top-1 accuracy 가 <em>88.4%</em> 입니다.\n올해에는 90% 가 넘는 architecture 가 나오지 않을까 생각이 드네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACLElEQVQoz3WTi3KbMBBF/f9/1jit7diB2OAxyGDeEhIggXj1CtokTaY7HmEJ7e7Zu8tmmiZKaVVVwzB0Wsu2LQjpWfE428HVJff7nZglDIKmabTWbdt2XYcV2808z+tG971irCO+jos2E97rLfBjyugwjPM3Q0op5QYPXpZdP3SUahKyfOTVLLu51fM4ffeaZdOwxeq6NplV24pK1GFWJLo3aaa/vxm1XN3r5XIJwxCcQogsy67XK7YmMwJwzpu6YnbQDyvRBxjepklKCIEuqA6xcF6WJbbGuSiKWikwMyKmxQkLJICbUsqo2HX9YtNi/9ScZzmvReklMmQ41Z0GG0oCDkJUlfB9P0kSxPqs1jiOxpmXXHaKBVSEZUbzOIqREO/yPAcboxSt+a/aaZo2sqEOiYOiLBlAgPrlHlgAv+qHiZCLmT7HcYx94NGmkSVj1WJJHD8eD2CvDvvd7nQ6ua4DfmhkvVrgAuBGLVZyrpQ01VLTQMgD8lWeYejPl/Ph5XA47B3HRWgEWuU0fcY/qC8l+t1mWcoFf1cVNg6DbVu/drvDfu95PrS0LQsh/ji3SgEjW4wLgZwf7V6wj8eX7Xb79OPJdV2k+fn8jGvte+Y0QZlR+HhAiS+CoQTHcU7H09ub7fsEzpZlCVEBc7OOxIItb7dbEASQCkjAW1ece96N3AlCBGGIJBg4kEJd4wxOZMOKL279yMASRTHmJIoirGawpex7vQ7cehMpfwMLXYzReaRx/AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png\"\n        srcset=\"/static/83339124bc869908ad6f92e63006f4a1/12f09/performance.png 148w,\n/static/83339124bc869908ad6f92e63006f4a1/e4a3f/performance.png 295w,\n/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png 590w,\n/static/83339124bc869908ad6f92e63006f4a1/efc66/performance.png 885w,\n/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png 1039w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1911.04252.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이전 Image Classification 연구 동향들에서도 간단하게 설명은 다음에 다른 포스트로 다뤄볼께요. 양이 좀 많이 질 듯 해서 (<del>귀찮네요</del>)</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<h3 id=\"self-training-w-noisy-student\" style=\"position:relative;\"><a href=\"#self-training-w-noisy-student\" aria-label=\"self training w noisy student permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Training w/ Noisy Student</h3>\n<p>학습은 다음과 같은 process 로 이뤄지는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACaElEQVQ4y5WTiY6bQBBE/f8/lkTa7AK2N1w+uC9jM9zgNeQN3isrJVJaAo+Z7p7qqppVWZa2ZamqalqWYRi/dH232x2PThhGQog4idM0beqmbduXl+sw9Cy6rmuapu+H1e12y7KUMoot07Rsm8XhsA+DkIXneaZp+r4fhTLiOCF833M9n8LVPM/0dlzXcRxd1/f7/eFwsG17GIbr9eUerMdxvF6v8uE9jn3fT9Mki+d5ks9bkN117W36+Dj/JVafy6abzKzrarffR1F0yrL8fM7zPMtOBACB+p77WvwlYIVKKAHhPWl6K/p6MgOUoqzKsijEckgmRMGBl8ulKIpTdmKPM9MkuZzPmcSQxXH8ejJNfc9XNfV5u3l8fNqsN6ZlPvx8UFRVW68VRYFtaumLqIgnOOOcf8CGuqZFtx4F+0EGmOu6RlIQsfgXYZ+DKaQBum6BfakqWdw2DVtIwAF/FMPK4piaN8DyXNKby58zXURZYq6SHkLQhQQMVlUVdJC/QtUgCBgMc6zX2vdvP563W01bK0+KoeuapuHW7XaLYbHRhgyY2Gxcz4NSCTuJI7Yd52iYpqI8UWCY9uJVC6vTgmz+8T1NJd2o8zEzDOEBSVjTIhiQqqqGKkx552xBKylEmq+EjcOQpRkM9T3DMyEzVvRCFhhhcjyDRgzPG7RLN5lzl6pj4CRNyaaG9tBbLcGWvDaOgzcKIejF7n2LptIk0mQSGDf2RvG4xLBcI/7CKoDHt8C89+sF09KeXFrD0AOufxjBbZJwa+MgCIPA53rGUYRyKOIcj6xd12XteT4fJWyUBRj2mv8zfgNpeC/7ZdD6rQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png\"\n        srcset=\"/static/db781720920abfda90bbcb8c902c548d/12f09/teacher-student.png 148w,\n/static/db781720920abfda90bbcb8c902c548d/e4a3f/teacher-student.png 295w,\n/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png 590w,\n/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png 654w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li>Labelled dataset 인 ImageNet 으로 <strong>Teacher Model</strong> 을 학습</li>\n<li>un-labelled dataset 인 JFT-300M 를 <strong>Teacher Model</strong> 로 pseudo labelling 하기</li>\n<li><code class=\"language-text\">2</code> 에서 생성된 data + ImageNet 으로 <strong>Student Model</strong> 학습 w/ noise</li>\n<li>다시 <code class=\"language-text\">2</code> 으로 가서 반복 (iterative training)</li>\n</ol>\n<p>위 과정이 knowledge (self) distillation 과 비슷한 과정인데, 주로 요 목적은 compression 으로 사용되는데, 여기는\n해당 목적 (not compression) 으로 사용하지 않는다는게 차이점 입니다.</p>\n<h3 id=\"training\" style=\"position:relative;\"><a href=\"#training\" aria-label=\"training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training</h3>\n<p>데이터 셋이나 구체적인 training recipe 들이 있지만, 적용한 technique 가 있어서 이걸 설명 해 보면,</p>\n<h4 id=\"fix-train-test-resolution-discrepancy\" style=\"position:relative;\"><a href=\"#fix-train-test-resolution-discrepancy\" aria-label=\"fix train test resolution discrepancy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>fix train-test resolution discrepancy</h4>\n<p>처음 몇 epoch 은 low resolution image 로 훈련을 하고 후에 high resolution image 로 fine-tuning 하는 기법입니다.</p>\n<p>논문 실험에서는 처음 350 epochs 는 낮은 해상도 이미지로 훈련하고, 1.5 epochs 는 더 큰 해상도로 unlabelled image 에 대해서 훈련했다고 하네요.</p>\n<p>unlabelled image data 는 학습할 때 labelled image 보다 14 배 큰 batch size 를 사용했다고도 하네요.</p>\n<h4 id=\"iterative-training\" style=\"position:relative;\"><a href=\"#iterative-training\" aria-label=\"iterative training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Iterative Training</h4>\n<p>논문에서는 총 3 steps 의 iterative training 을 했다고 소개합니다.</p>\n<ol>\n<li><code class=\"language-text\">EfficientNet-b7</code> 을 ImageNet 으로 훈련 <code class=\"language-text\">(as Teacher)</code></li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 JFT-300M + ImageNet 으로 훈련 <code class=\"language-text\">(as Student)</code> (batch size 비율은 labelled : unlabelled = 1 : 14)</li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 새롭게 훈련 <code class=\"language-text\">2</code> 에서 만든 모델을 <code class=\"language-text\">Teacher</code> 로 사용 <code class=\"language-text\">(as Student)</code></li>\n<li><code class=\"language-text\">3</code> 과 비슷한 scheme 으로 진행하는데, (batch size 비율은 labelled : unlabelled = 1 : 28) 로 훈련</li>\n</ol>\n<h4 id=\"noisy\" style=\"position:relative;\"><a href=\"#noisy\" aria-label=\"noisy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Noisy</h4>\n<p>Student Model 을 학습할 때 논문에서 <code class=\"language-text\">Noisy</code> 하게 훈련한다고 했는데, 이 때 <code class=\"language-text\">Noisy</code> 에 해당하는 부분은 크게 3 부분 입니다.</p>\n<ol>\n<li>Data Augmentation w/ RandAugment</li>\n<li>Dropout</li>\n<li>Stochastic Depth</li>\n<li>other techniques (data filtering, balancing)</li>\n<li>OOD (Out-Of-Distribution)</li>\n<li>unlabelled data 에 대해선 class 별 samples 수가 biased 돼있으니, 적은 sample 들 duplicate 하기</li>\n<li>pseudo label 시, soft or hard pseudo 한다고 했었는데, soft, hard 둘다 좋은 결과를 보였지만, soft 가 더 좋았다</li>\n</ol>\n<p>요런 기법들은 이전에 소개된 기법들이니 설명은 pass</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"imagenet-benchmark\" style=\"position:relative;\"><a href=\"#imagenet-benchmark\" aria-label=\"imagenet benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet Benchmark</h3>\n<p>역대 ImageNet architecture 들 정확도를 benchmark 한 table 인데, 다른 구조보다 </p>\n<ol>\n<li>더 적은 params 수</li>\n<li>상대적으로 적은 extra data 수</li>\n<li>더 높은 성능</li>\n</ol>\n<p>을 달성했다는 점에서 의미가 있을 것 같네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB+0lEQVQoz21T2ZKbMBDk/38u69RiLiOELSQhwNYiLnElLVjnIcm4oGbM9HSrGTwpBaV5d4TpTN/3TV2nhNR1PQzD8/kkhAghzvx+vyulKqUopS+tPSklSTOqaGvNvu7ruqIPz/ph2LZtWZZxHK21yHHnnKuqmq1FwzCOHi+4/+knWfJo2a8j5nnOSDZNE/J933E7/0delrKqKuS32w2zPCH45eOSStIu5mybZ3tLbyA8Afu+HSNcnuc5FwJZFIYOrEoZBiG9U1YzY40DW5tlYLZvwm8wlOPMVV2jhBEQ6EkhwjhiDdPD17qtaMJIMEzWvsWeWAfmRQHbUGO6AytZhklUjdW2bycVwDSj/8qGl7Bda40SDQ4spLj8uER5JDv5Bk/w4zTsP7KrGkkcJw5cFPzj5weRRI96PxwDcxRFXdf/xQxMnlO8Z5ThaZjgPIiDQnDVq2lz5wRnmqbDIXtzseICALKLoqjrxsmm9PtVRVF8SxLyIEKJpmv6roMfxph1Xf4EkBh6gic7QT92CbLZp3/FIX3fJ4ywlg1TH8exge6ugz1fLlpc4zixx0MdS4JVwTivlBKtoMrvOS+LytTaaMgG+Nxn7Qbgp1Ey9miaZpkXNEAImIvr1Q+CAB+DfunWtC/9wg4Agj43NM8hEluNbyZJYs5F27ZOmjG/AcPrEoRUJCDlAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png\"\n        srcset=\"/static/4a5d59c7ec213c223b48d78af30834c5/12f09/overall-imagenet-performance.png 148w,\n/static/4a5d59c7ec213c223b48d78af30834c5/e4a3f/overall-imagenet-performance.png 295w,\n/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png 590w,\n/static/4a5d59c7ec213c223b48d78af30834c5/efc66/overall-imagenet-performance.png 885w,\n/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" style=\"position:relative;\"><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" aria-label=\"robustness results on imagenet a imagenetc and imagenet p permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</h3>\n<p>해당 dataset 은 이 모델이 정말로 noise 에 robust 한지 체크하는 task 입니다.</p>\n<p>예를 들어서, (일반적인 corruptions, perturbations)</p>\n<ol>\n<li>blur 섞인 이미지</li>\n<li>fogged 이미지</li>\n<li>rotated 이미지 </li>\n<li>scaled 이미지</li>\n</ol>\n<p>등등이 데이터에 섞여 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.1891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB9ElEQVQoz2VSyZbbIBD0///WxAfHb15sJ9aGWATakBdtYC3eUrLiOUz6wAOa6iqqe5HneRAEkYjqujatadu2eQf2uBSCx3GcJElRHM7nE6MUJymltXZRFIXrukKIqqwa27S27bu+H4b+FcaYJIk5F0rK4/ForaE0BJ9Squu6RaH1xCyjMCD8wKllF3NBIPd8PodhuF6veZrmWuP4eDyw3u/3eb/QOt/v9zSkgjMRR8SQ/KiVVJCKh+MwjOOYJpPsCffC3G63ucoEdhyHcSZZFGY0tNS0FrQgn5kBhkg8m8FfzBM4y9Ltdhd4JIxJfEh0r+MsVkJmWfZ4y9Z5nmX5838wEn4QUEJJRmQpbWXNaOxgbGPrqobhEAlvIylfSifZKPdPNqzDnz3Pi7hglH2uP93EYYb6sU8jBrfHcVBKUsZQ5TszBPmeL7iIogi2oGdpmmhTJE1SmYn5dD4hJaVC51ELa1mWdd1MrYITrufSMHQ9D0g08/YujIDCvutI4FNKZ0Kt9WbzC8MzMcdKLZfL/Z/fHx8/VqvVZrcLAh9WG2vsxYIZpmy3G8/3AYDzoFmvfyZpCi8XU1esrV6BdFlBUv01nrjExr7UzoJxidKYzUn2LC/NUsfZH4oDRgX6SRAwztEgqOBC5Mjib65HCMHwWHuZbf8LQozQY+SLdxsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png\"\n        srcset=\"/static/bd4580870392b73ee63cd51263ff1380/12f09/imagenet-a-benchmark.png 148w,\n/static/bd4580870392b73ee63cd51263ff1380/e4a3f/imagenet-a-benchmark.png 295w,\n/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png 590w,\n/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png 598w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>ImageNet-A dataset benchmark table 만 하나 보면, 확실히 <code class=\"language-text\">The Noise</code> 가 robustness 에 큰 도움을 주고 있네요.</p>\n<h3 id=\"adversarial-robustness\" style=\"position:relative;\"><a href=\"#adversarial-robustness\" aria-label=\"adversarial robustness permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial Robustness</h3>\n<p>이번 실험은 adversarial attack 에도 robust 한지 확인하는 겁니다. 주로 FGSM Attack 을 해서 테스트를 하는데, 성능은 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 523px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABmElEQVQoz42TbXOCMBCE+f//zvajTlVEIICAECAG8fqEWKu1bzdMBpLb3WPvEojIRaRrmrEsT9aejDk9xjAMo1JlnqdKFUURx3Ge50mSdF0XAD5PU1vX49ubXOD5GtP5rDebOAxfXl+Xy+VisdjtdrBorQMIkjiGs4xjaRqHfqSYpqnSejCmbdvj8dg0jTFmHMe+7wPPQW19101KkXsFQeGf7wIwkCAMw9VqBWVnjC0KCcNPvNx4HsKBrXXgKIowwHuDuAyDUH8UuRc+tX7m+lTe7/cYSOVQou91xFppW4fkR9JUylIOB/dUlWPs+7EoBtzOsgxlwMCwBG/ll+B0Jh2zzIGRBX8DWzT/b9h6vaZvNIAt3D9Qm6/8p7g3DDDKuOVbSuegkL/iqpym6Xa7rev6Jsi7UsrMc/oL2A2JF2TObv2cfTkD5hjqqqqGOfhk/9pnr0wSGfe/enkaT1ykClb9ETTIXQxS4cNnP6ewYF41B/WzckQqI0ACK0ok4NT1Vnl6SoAIei9FYahhB9kUbGd72QSDGCvJ72KQn41hq/jdAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n        srcset=\"/static/3a179011ae1230e5213e3dcc7fa3a416/12f09/fgsm-benchmark.png 148w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/e4a3f/fgsm-benchmark.png 295w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png 523w\"\n        sizes=\"(max-width: 523px) 100vw, 523px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>FGSM 보다 더 강력한 attack 인 FGD Attack 시에도 꽤괜 성능을 보였다고 캅니다.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>요즘 경향들은 이전처럼 deep 한 architecture 를 설계하거나 AutoML 을 이용한 NAS 를 만드는 것 보다는,\ntraining recipe (~ techniques) 에 집중을 하고 있는데, 이런 trend 에서 재미있는 approach 들이 많이 나오고 있는 것 같네요.</p>\n<p>또 현재 상태에서 CutMix 등등 여러 또 다른 기술들이 적용되면 최고 performance 가 어느 정도 될지도 궁금해 지네요.</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각…","tableOfContents":"<ul>\n<li><a href=\"/Noisy-Student/#tldr\">TL;DR</a></li>\n<li><a href=\"/Noisy-Student/#related-work\">Related Work</a></li>\n<li><a href=\"/Noisy-Student/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/Noisy-Student/#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"/Noisy-Student/#self-training-w-noisy-student\">Self-Training w/ Noisy Student</a></li>\n<li><a href=\"/Noisy-Student/#training\">Training</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/Noisy-Student/#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"/Noisy-Student/#imagenet-benchmark\">ImageNet Benchmark</a></li>\n<li><a href=\"/Noisy-Student/#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\">Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</a></li>\n<li><a href=\"/Noisy-Student/#adversarial-robustness\">Adversarial Robustness</a></li>\n</ul>\n</li>\n<li><a href=\"/Noisy-Student/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/Noisy-Student/"},"frontmatter":{"title":"Self-training with Noisy Student improves ImageNet classification","date":"Apr 11, 2020","tags":["Deep-Learning"],"keywords":["EfficientNet","Noisy-Student"],"update":"Apr 11, 2020"},"timeToRead":4}},"pageContext":{"slug":"/Noisy-Student/","series":[],"lastmod":"2020-04-11"}},"staticQueryHashes":["2027115977","694178885"]}