{"componentChunkName":"component---src-templates-post-tsx","path":"/Noisy-Student/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각나서 리뷰하게 됐어요.</p>\n<p>아래는 이번 approach 가 달성한 성능인데, 이전 SOTA 에 비해서 Accuracy 가 약 2% 정도 올라갔네요.</p>\n<p>해당 이미지에는 <em>L2</em> performance 가 안올라와 있는데, <em>Noisy Student + Random Augment</em> 로 훈련한 <em>L2</em> 모델 top-1 accuracy 가 <em>88.4%</em> 입니다.\n올해에는 90% 가 넘는 architecture 가 나오지 않을까 생각이 드네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACJklEQVQoz3VTi3KbMBD0//9Z47S2YxeHQg3IwjwlkACBeHYFadom0xvPYUns7d6e2M3zzDmv6nocR933qusYIUPBHvaV/nTJ/X4nJoWUNk3T933XdVprZCx3y7LoddUPY1sWOvD7hLVp5V08GsS84NM0LZ8ClEqpHR6iLPUwas56EhZsEtWi9NL1yzR/Ri2qaYo16ro2zNAga1nTjCXDYADz79+CXlzHsW07DEPorKoqz7Lb7YalYUYBIURTV9yiw7ghTUyTEYbTLEvRM6jQ3NYClGJpwIyxum015wWRK+u8aQGsbVvjotbDGvMa//QMGaKRpZ+oR4HdXmspJfxHNu1IGQRBkiSo9bdbkGDAohRKtwXlghYpy+I4BgZneZ6jRME5mP/rdpqmjWq4Q2LK0Eue5ZD64T3wbyVwBM/UGmbOoMKa+rxpFHyo1sDm4/GAhA1wPBwul4vrOtAPj6zvFnTBkV27RikEioGh4GaAsAfKN3vGcbB/2KeX0+l0dBwXpVFos9PMGf/KsgQYVBiMkHKa/9yPaRyvV+vb4XA6Hn0/wJCuloUSb2A8csayNQxyHeb7VCD7fH7Z7/dPX55c1wXN1+dnvNa9M6dJEsUR+gT/B8PQguM4l/Pl9fUaBARgy7KkrCDzjRlbcM/zPEopSmAH8raMcr7vkTtBCRqGSRwTQmAY3N2tNxF9jcj44raPDFqiKMa1jaII2VxspYah3y7c9iaYfwFAiIzdQa3AUQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png\"\n        srcset=\"/static/83339124bc869908ad6f92e63006f4a1/12f09/performance.png 148w,\n/static/83339124bc869908ad6f92e63006f4a1/e4a3f/performance.png 295w,\n/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png 590w,\n/static/83339124bc869908ad6f92e63006f4a1/efc66/performance.png 885w,\n/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png 1039w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1911.04252.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이전 Image Classification 연구 동향들에서도 간단하게 설명은 다음에 다른 포스트로 다뤄볼께요. 양이 좀 많이 질 듯 해서 (<del>귀찮네요</del>)</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<h3 id=\"self-training-w-noisy-student\" style=\"position:relative;\"><a href=\"#self-training-w-noisy-student\" aria-label=\"self training w noisy student permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Training w/ Noisy Student</h3>\n<p>학습은 다음과 같은 process 로 이뤄지는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACaklEQVQ4y5WT6XKbQBCE9f4vlqTKLkscDkLo4JKABYnlBgmRb5FdTlTJj3SV8BpmZnt6ehZFUTibzWq1sjeb9Xr907K22+3h4B6PRyllFEdJktR13TTN7Xbr+45D27a86ft+MY6jEAlpJG9se+M4HPb73TEMOfi+b9t2EATHGVEUgyDwPT8gcTFNE7Vdz3Ndl2t3u91+v3cch8LX6+0BzsMwXK9X9eM5DF3X3e93lTxNd/X7BNFwG8fHSy4Y+TP9DYvf00ggrKrK7W53Op1SIbLzOcsyIVIgkmScIx74SH4CqpCJJDB8BN0/k55vpoFCFmVR5LmcLxFS5lx4vlwu+SUVKd8QJYljXgjFQURR9HEzRUM/YFSmaby+vhm6gbwvLy/L5VLTdJ4MIhEC+mVZSgYrJZW/aCNd3dQ8mWDXK8C5qipkgxGH6R947pkuyOzaVrHOL2Wpkpu65hPe4II/klFldkzFE1JZpuSdcaYKPDFXQQ0pq7oiAIPBP89z4hdMNcRMto05dF37/u3Hu2mqbt+Wa8vSNA23mqaJYbGRQYSm64aBw6CmaMdxZJrvrnuw1zYK6bpu2w5WxayWZSGYYZpYlzxUR248/9UzCmVp2s2mlzPosypL+m/bBsrwbJumqmvltifBhr4XiYBGp5qnwzm66xg4itB5dDpRsZ5LM/9iBjGPUbU0DCWi0ZPyyFvO4JNaG9fFG1IW1OIr7+HFNcokymRKRjZ2JHmY0c9rxL+oShPDJzDvY71QesG2+Z6HMGGoNpZlxH1xFAXMIAhYULUhaRYE4eFw4IzmrLfn+QxT0cZuzAF7Tf+JX3jrL/7FMnMhAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png\"\n        srcset=\"/static/db781720920abfda90bbcb8c902c548d/12f09/teacher-student.png 148w,\n/static/db781720920abfda90bbcb8c902c548d/e4a3f/teacher-student.png 295w,\n/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png 590w,\n/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png 654w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li>Labelled dataset 인 ImageNet 으로 <strong>Teacher Model</strong> 을 학습</li>\n<li>un-labelled dataset 인 JFT-300M 를 <strong>Teacher Model</strong> 로 pseudo labelling 하기</li>\n<li><code class=\"language-text\">2</code> 에서 생성된 data + ImageNet 으로 <strong>Student Model</strong> 학습 w/ noise</li>\n<li>다시 <code class=\"language-text\">2</code> 으로 가서 반복 (iterative training)</li>\n</ol>\n<p>위 과정이 knowledge (self) distillation 과 비슷한 과정인데, 주로 요 목적은 compression 으로 사용되는데, 여기는\n해당 목적 (not compression) 으로 사용하지 않는다는게 차이점 입니다.</p>\n<h3 id=\"training\" style=\"position:relative;\"><a href=\"#training\" aria-label=\"training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training</h3>\n<p>데이터 셋이나 구체적인 training recipe 들이 있지만, 적용한 technique 가 있어서 이걸 설명 해 보면,</p>\n<h4 id=\"fix-train-test-resolution-discrepancy\" style=\"position:relative;\"><a href=\"#fix-train-test-resolution-discrepancy\" aria-label=\"fix train test resolution discrepancy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>fix train-test resolution discrepancy</h4>\n<p>처음 몇 epoch 은 low resolution image 로 훈련을 하고 후에 high resolution image 로 fine-tuning 하는 기법입니다.</p>\n<p>논문 실험에서는 처음 350 epochs 는 낮은 해상도 이미지로 훈련하고, 1.5 epochs 는 더 큰 해상도로 unlabelled image 에 대해서 훈련했다고 하네요.</p>\n<p>unlabelled image data 는 학습할 때 labelled image 보다 14 배 큰 batch size 를 사용했다고도 하네요.</p>\n<h4 id=\"iterative-training\" style=\"position:relative;\"><a href=\"#iterative-training\" aria-label=\"iterative training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Iterative Training</h4>\n<p>논문에서는 총 3 steps 의 iterative training 을 했다고 소개합니다.</p>\n<ol>\n<li><code class=\"language-text\">EfficientNet-b7</code> 을 ImageNet 으로 훈련 <code class=\"language-text\">(as Teacher)</code></li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 JFT-300M + ImageNet 으로 훈련 <code class=\"language-text\">(as Student)</code> (batch size 비율은 labelled : unlabelled = 1 : 14)</li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 새롭게 훈련 <code class=\"language-text\">2</code> 에서 만든 모델을 <code class=\"language-text\">Teacher</code> 로 사용 <code class=\"language-text\">(as Student)</code></li>\n<li><code class=\"language-text\">3</code> 과 비슷한 scheme 으로 진행하는데, (batch size 비율은 labelled : unlabelled = 1 : 28) 로 훈련</li>\n</ol>\n<h4 id=\"noisy\" style=\"position:relative;\"><a href=\"#noisy\" aria-label=\"noisy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Noisy</h4>\n<p>Student Model 을 학습할 때 논문에서 <code class=\"language-text\">Noisy</code> 하게 훈련한다고 했는데, 이 때 <code class=\"language-text\">Noisy</code> 에 해당하는 부분은 크게 3 부분 입니다.</p>\n<ol>\n<li>Data Augmentation w/ RandAugment</li>\n<li>Dropout</li>\n<li>Stochastic Depth</li>\n<li>other techniques (data filtering, balancing)</li>\n<li>OOD (Out-Of-Distribution)</li>\n<li>unlabelled data 에 대해선 class 별 samples 수가 biased 돼있으니, 적은 sample 들 duplicate 하기</li>\n<li>pseudo label 시, soft or hard pseudo 한다고 했었는데, soft, hard 둘다 좋은 결과를 보였지만, soft 가 더 좋았다</li>\n</ol>\n<p>요런 기법들은 이전에 소개된 기법들이니 설명은 pass</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"imagenet-benchmark\" style=\"position:relative;\"><a href=\"#imagenet-benchmark\" aria-label=\"imagenet benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet Benchmark</h3>\n<p>역대 ImageNet architecture 들 정확도를 benchmark 한 table 인데, 다른 구조보다 </p>\n<ol>\n<li>더 적은 params 수</li>\n<li>상대적으로 적은 extra data 수</li>\n<li>더 높은 성능</li>\n</ol>\n<p>을 달성했다는 점에서 의미가 있을 것 같네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB80lEQVQoz2VTCY6jMBDk/49bTUZDuI0JBjtccTCXuWbLMIlWOy0BLeiqri43Fhc8SZLuCNWpvu/rqooIqapqGIamaQghnPMzv91uRVGURUEpfUhpCSHiKKYFbbXa131dV9ThWz8M27YtyzKOo9YaOe55npdlNWuNgmEcrTzL7C87iIO0Zd9HzPMck3iapu+f2H8e+36/i7IskYdhCC4LZJePSyRIu6izbJ51GIVo+MJsb3CSUM4FMs91DRhkruPSG2UVU1oZsNZxjM76PzCUY2Z4ARYYAYGWyLnre6xmcniu24oiUMLCSb/B+xuMGZvmgTdgN+C7EG7glWO5vToATGP6Wza8hO3y+QQYBQaMo7r8uXiJKzrxAk/w423Yb9lIfD8w4CzLPj4/iCBylPvhGDp7ntd1/an5X3BCaVGWeOWehsFtx3dynhd9MW1mTvSMomg4ZBvoth4UZgUyllV1bWRTeoB57nl+GAQkJbzgdVf3XQc/lFLrurwDSJBCJsCTnqAfu2RljH3ZVwxp2zZhhLVsmHrf9xV0d52UMAjR4hrHKU3Tc0lwB51xG6VYqeSW5Pe8VJVUErIBPvdZmnjiGoaRsbRu6mWeUTBOk+l8vdqO4+BnkA/ZqvYhH9gBVNd1Df04c4jEdPhnAt/HabVta6Qp9RcE0RKlo4IsFQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png\"\n        srcset=\"/static/4a5d59c7ec213c223b48d78af30834c5/12f09/overall-imagenet-performance.png 148w,\n/static/4a5d59c7ec213c223b48d78af30834c5/e4a3f/overall-imagenet-performance.png 295w,\n/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png 590w,\n/static/4a5d59c7ec213c223b48d78af30834c5/efc66/overall-imagenet-performance.png 885w,\n/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" style=\"position:relative;\"><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" aria-label=\"robustness results on imagenet a imagenetc and imagenet p permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</h3>\n<p>해당 dataset 은 이 모델이 정말로 noise 에 robust 한지 체크하는 task 입니다.</p>\n<p>예를 들어서, (일반적인 corruptions, perturbations)</p>\n<ol>\n<li>blur 섞인 이미지</li>\n<li>fogged 이미지</li>\n<li>rotated 이미지 </li>\n<li>scaled 이미지</li>\n</ol>\n<p>등등이 데이터에 섞여 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.1891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB9UlEQVQoz2VSyZKbMBT0///WxAfHNRV7YrMJCSEQCPDCIrF5SwPjHJJ3UPEk+nWrW6s0TT3PC3lYVZVudNM09bvwjU3OgziOpZR5XpzPZ0YpOiGEMWaV55lt25zz8lrWpm5M03d9Pwz9XFprKeMg4JEQp9MJLaW+UiqKoq7rVlmWTcwi9D0SFAE1rNUtCmev12sYhtvtppJEZRna5/OJ9fF4LN8rpdLD4eD7Pg8Yj0OiSXbK4iiCTvw4DsM4jnKWPeFmzP1+X6asMqUsy2KcCRb6KfUN1Y0BLcgXZoAhMsvUAv7LPIGTJNl/fXku8WMiC5n3uVQyFhEu9nzLBkGaqtf/YOy7nkcJJSkRV2FKo0dtBm1qU5UV7IdIeCsiMSudZGPct+xUpcfj0XGcMOCMss/tpy0tpqkbuzRksHcchygSlDFM+ZcZObuOywMehiFsQWZJInNdyFqWukLU58sZR6BG8piF9Xq9VlU9R6WU7djU923HARJh3t+DUVDYdx3xXErZQohod7tfeDwTM5xcr9fHw++Pjx+bzWYH8zy37VpttGkNmDF9v985rgsAkgPNdvtTJgm8XHVzQUlZlm9J3/Xd1pWBWnRVfblcMA4p4m1OshffkjSxrGORFzyg0E+IxwKOIKAi4FzhFHezsU/wzo1pF9v/AC9X0FBz/fHqAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png\"\n        srcset=\"/static/bd4580870392b73ee63cd51263ff1380/12f09/imagenet-a-benchmark.png 148w,\n/static/bd4580870392b73ee63cd51263ff1380/e4a3f/imagenet-a-benchmark.png 295w,\n/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png 590w,\n/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png 598w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>ImageNet-A dataset benchmark table 만 하나 보면, 확실히 <code class=\"language-text\">The Noise</code> 가 robustness 에 큰 도움을 주고 있네요.</p>\n<h3 id=\"adversarial-robustness\" style=\"position:relative;\"><a href=\"#adversarial-robustness\" aria-label=\"adversarial robustness permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial Robustness</h3>\n<p>이번 실험은 adversarial attack 에도 robust 한지 확인하는 겁니다. 주로 FGSM Attack 을 해서 테스트를 하는데, 성능은 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 523px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABkElEQVQoz5WT6ZKCMBCEef+3c/1plQcqEo6AQCCGI/uFsB7lHrVTVCom092TnjGw1k7TpKqql/JmzE3r22t0XdcLIZMkFiLLsiiK0jS9XC5KqQDwMAx1WfbbLTT2LcZhUPv9OQw/1uvNZrNarY7HIyxN0zgwZHDKKLJVNc2VvIDHsWiaTuu6rq/Xa1VVWuu+79u2DYqi4JTaWqVGIchdQFD477sADCQoy5JioFRamyyzYfjA2zvPSziwMQ6czuG9Qdx2Hc+wp5Pb8LNp3rkeylLK8/lM5VCi73W4tHXtwDwkjq2UNs/dVxSOTqk+TTvcBoB1+A4MCry1vwS3z+A8z5MkwXeucJJ6/mFYHMf0DRhHuA/XUvlP8WwYsyKEwC3fUsBQ2L9iUeadh8OBht0F2UPHJBhjfgG7IfGC3rB7GxlYwFzzKKbIjdAcnC999srsyHh+6vQ2npTAu1ibr2A0nv4Yde1nHRbMK+agfj+8pPqOsqJEwm63W8CenjrRhN5LURhq2EE2BZvZXg7BIMZK8ieH/aCb+TtlngAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n        srcset=\"/static/3a179011ae1230e5213e3dcc7fa3a416/12f09/fgsm-benchmark.png 148w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/e4a3f/fgsm-benchmark.png 295w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png 523w\"\n        sizes=\"(max-width: 523px) 100vw, 523px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>FGSM 보다 더 강력한 attack 인 FGD Attack 시에도 꽤괜 성능을 보였다고 캅니다.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>요즘 경향들은 이전처럼 deep 한 architecture 를 설계하거나 AutoML 을 이용한 NAS 를 만드는 것 보다는,\ntraining recipe (~ techniques) 에 집중을 하고 있는데, 이런 trend 에서 재미있는 approach 들이 많이 나오고 있는 것 같네요.</p>\n<p>또 현재 상태에서 CutMix 등등 여러 또 다른 기술들이 적용되면 최고 performance 가 어느 정도 될지도 궁금해 지네요.</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각…","tableOfContents":"<ul>\n<li><a href=\"/Noisy-Student/#tldr\">TL;DR</a></li>\n<li><a href=\"/Noisy-Student/#related-work\">Related Work</a></li>\n<li><a href=\"/Noisy-Student/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/Noisy-Student/#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"/Noisy-Student/#self-training-w-noisy-student\">Self-Training w/ Noisy Student</a></li>\n<li><a href=\"/Noisy-Student/#training\">Training</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/Noisy-Student/#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"/Noisy-Student/#imagenet-benchmark\">ImageNet Benchmark</a></li>\n<li><a href=\"/Noisy-Student/#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\">Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</a></li>\n<li><a href=\"/Noisy-Student/#adversarial-robustness\">Adversarial Robustness</a></li>\n</ul>\n</li>\n<li><a href=\"/Noisy-Student/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/Noisy-Student/"},"frontmatter":{"title":"Self-training with Noisy Student improves ImageNet classification","date":"Apr 11, 2020","tags":["Deep-Learning"],"keywords":["EfficientNet","Noisy-Student"],"update":"Apr 11, 2020"},"timeToRead":4}},"pageContext":{"slug":"/Noisy-Student/","series":[],"lastmod":"2020-04-11"}},"staticQueryHashes":["2027115977","694178885"]}