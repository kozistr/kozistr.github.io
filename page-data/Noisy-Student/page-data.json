{"componentChunkName":"component---src-templates-post-tsx","path":"/Noisy-Student/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각나서 리뷰하게 됐어요.</p>\n<p>아래는 이번 approach 가 달성한 성능인데, 이전 SOTA 에 비해서 Accuracy 가 약 2% 정도 올라갔네요.</p>\n<p>해당 이미지에는 <em>L2</em> performance 가 안올라와 있는데, <em>Noisy Student + Random Augment</em> 로 훈련한 <em>L2</em> 모델 top-1 accuracy 가 <em>88.4%</em> 입니다.\n올해에는 90% 가 넘는 architecture 가 나오지 않을까 생각이 드네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACH0lEQVQoz22T23KiQBCGef/nsipbFdebKIJLOGgEOQzDcB5F2Q8xta6VvohTsf/+D90a4ziWZSml7LquadtG6/gYNjKPfO/r09kfDr7vB0HgeV6e5y0N96rvZQDu+x7k7XbTXddHR30Mh1R+rXe+7cdJDOByuZzPZxrGp2KEwb/46LTulDoHfnmqlbqVzVjWY9dfqfH/Gi4XxkFWVdXE3DZt1dRaFOLQNOfxdoXhH0kURcg+nU4zIEnTT8fBywRGMz7KppLboK2HZxKkIupwONiWzYhZP1oIKM2ypm2MgqdSui5zVw7flHQAI0jw13sNw4DB2cX8mDwLIaCtoqz20snSMMBAsKiCB137/R7NDHoxP4FLVapKFZEsA1l3TZZm7ABYHMdJkmRZxpTb+EM9mNu+U/sodONUJGEYaq1f+hDPuNkOWs73msAwSJmHbqJkPS+fU8mFOEVTvDPY2lq2PWUmBN+IP44DCncG3VDJomIS4eEWe7y5m+HORoFcrVbvy3fTNOn/WH9IWSBh2jOuiqJgEmKyNFVKEduzbNd11+v1dmtyqHTudru5fwLjRxaFeFTOiBfPIN/efi0Wi81mw+jfy2UmxIMZkSzjFEWk1fX9yw1T/Co2G9OyrMAPMGVZW2Rr3X8zy7yuK5q4J4IhKprme0Se73u2vTPNLfrZn+M4HCnKjXkBZAYhfRwPAGBI4UGqJMSbTeh7oZa/WGP6X3UvjEm8sYrxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png\"\n        srcset=\"/static/83339124bc869908ad6f92e63006f4a1/12f09/performance.png 148w,\n/static/83339124bc869908ad6f92e63006f4a1/e4a3f/performance.png 295w,\n/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png 590w,\n/static/83339124bc869908ad6f92e63006f4a1/efc66/performance.png 885w,\n/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png 1039w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1911.04252.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이전 Image Classification 연구 동향들에서도 간단하게 설명은 다음에 다른 포스트로 다뤄볼께요. 양이 좀 많이 질 듯 해서 (<del>귀찮네요</del>)</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<h3 id=\"self-training-w-noisy-student\" style=\"position:relative;\"><a href=\"#self-training-w-noisy-student\" aria-label=\"self training w noisy student permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Training w/ Noisy Student</h3>\n<p>학습은 다음과 같은 process 로 이뤄지는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACX0lEQVQ4y42T53KjQBCEef9X8/2wkUwQCCRyEDlIJOm+BYc721V3UyUxsLPbM9290jRNvu/Jsqzrummaqqodj0fHcXzfz7IsjEKSruv6vh+GG3G9Xvs15nmW5nlJ4vig64ZhqMqrYZiaphsHI4qi49Fip2VZrusGvh8EfhjyOfJc93Q+i82Px6Msy5AIwtPJFnVBADIdPb7F/X7/M5G+VyzL0nXtOIrN4zTRKiD3ZeH/S6W0HbOwuAZ52zYMD36SJEVR1nV9WSNNk63gA/8H5HmasjwH8PGvkIZhyLMsTdM4TqCHmQGEl0CQEPIKpuu4sACvkBZF4dlxaOENOQrD3U7WNfVF3qmaZtv28/Pzbr8nf93vUYHOUWu4ATSgFfknYR/TErACz+M4oiSdb5JuOFvZB9vE15nZ1rYtxxeliKapu7bt+w4g7HH9mwiJ6pXPlGrmsU9n38MFIoqiiJO4KAsYSdKUs9IkgUtBShjyKpChBksxqqoqT0+/NIymqHtmVmFBNk2D3MZAlqUoigwvqgaLbdeKzWQsAwc3mFx+kTEphrVs+3A4CNtq2tE0Wamqqu962v/UGZKausZScACdMDSusaz8QVvXdxN3AJ+92+NzM8ue56NzVZaczTB5nlV1zaiglFXpug5z5jkkFFmGLRg8I1+Rp5HNfBE2XoSBEQtJAUe4Is8xD6TebsOGzNqE6adJ4kefoAHFreU8pGqatq4bEiRoeGtEXlcVT1rbPjA593n2PE9VFO7q5ZJtfszzPE0vtAomzRebPEHAEvc5SeIoit+kquvK873+ncP/j98UTTEG7aMbCgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png\"\n        srcset=\"/static/db781720920abfda90bbcb8c902c548d/12f09/teacher-student.png 148w,\n/static/db781720920abfda90bbcb8c902c548d/e4a3f/teacher-student.png 295w,\n/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png 590w,\n/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png 654w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li>Labelled dataset 인 ImageNet 으로 <strong>Teacher Model</strong> 을 학습</li>\n<li>un-labelled dataset 인 JFT-300M 를 <strong>Teacher Model</strong> 로 pseudo labelling 하기</li>\n<li><code class=\"language-text\">2</code> 에서 생성된 data + ImageNet 으로 <strong>Student Model</strong> 학습 w/ noise</li>\n<li>다시 <code class=\"language-text\">2</code> 으로 가서 반복 (iterative training)</li>\n</ol>\n<p>위 과정이 knowledge (self) distillation 과 비슷한 과정인데, 주로 요 목적은 compression 으로 사용되는데, 여기는\n해당 목적 (not compression) 으로 사용하지 않는다는게 차이점 입니다.</p>\n<h3 id=\"training\" style=\"position:relative;\"><a href=\"#training\" aria-label=\"training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training</h3>\n<p>데이터 셋이나 구체적인 training recipe 들이 있지만, 적용한 technique 가 있어서 이걸 설명 해 보면,</p>\n<h4 id=\"fix-train-test-resolution-discrepancy\" style=\"position:relative;\"><a href=\"#fix-train-test-resolution-discrepancy\" aria-label=\"fix train test resolution discrepancy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>fix train-test resolution discrepancy</h4>\n<p>처음 몇 epoch 은 low resolution image 로 훈련을 하고 후에 high resolution image 로 fine-tuning 하는 기법입니다.</p>\n<p>논문 실험에서는 처음 350 epochs 는 낮은 해상도 이미지로 훈련하고, 1.5 epochs 는 더 큰 해상도로 unlabelled image 에 대해서 훈련했다고 하네요.</p>\n<p>unlabelled image data 는 학습할 때 labelled image 보다 14 배 큰 batch size 를 사용했다고도 하네요.</p>\n<h4 id=\"iterative-training\" style=\"position:relative;\"><a href=\"#iterative-training\" aria-label=\"iterative training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Iterative Training</h4>\n<p>논문에서는 총 3 steps 의 iterative training 을 했다고 소개합니다.</p>\n<ol>\n<li><code class=\"language-text\">EfficientNet-b7</code> 을 ImageNet 으로 훈련 <code class=\"language-text\">(as Teacher)</code></li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 JFT-300M + ImageNet 으로 훈련 <code class=\"language-text\">(as Student)</code> (batch size 비율은 labelled : unlabelled = 1 : 14)</li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 새롭게 훈련 <code class=\"language-text\">2</code> 에서 만든 모델을 <code class=\"language-text\">Teacher</code> 로 사용 <code class=\"language-text\">(as Student)</code></li>\n<li><code class=\"language-text\">3</code> 과 비슷한 scheme 으로 진행하는데, (batch size 비율은 labelled : unlabelled = 1 : 28) 로 훈련</li>\n</ol>\n<h4 id=\"noisy\" style=\"position:relative;\"><a href=\"#noisy\" aria-label=\"noisy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Noisy</h4>\n<p>Student Model 을 학습할 때 논문에서 <code class=\"language-text\">Noisy</code> 하게 훈련한다고 했는데, 이 때 <code class=\"language-text\">Noisy</code> 에 해당하는 부분은 크게 3 부분 입니다.</p>\n<ol>\n<li>Data Augmentation w/ RandAugment</li>\n<li>Dropout</li>\n<li>Stochastic Depth</li>\n<li>other techniques (data filtering, balancing)</li>\n<li>OOD (Out-Of-Distribution)</li>\n<li>unlabelled data 에 대해선 class 별 samples 수가 biased 돼있으니, 적은 sample 들 duplicate 하기</li>\n<li>pseudo label 시, soft or hard pseudo 한다고 했었는데, soft, hard 둘다 좋은 결과를 보였지만, soft 가 더 좋았다</li>\n</ol>\n<p>요런 기법들은 이전에 소개된 기법들이니 설명은 pass</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"imagenet-benchmark\" style=\"position:relative;\"><a href=\"#imagenet-benchmark\" aria-label=\"imagenet benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet Benchmark</h3>\n<p>역대 ImageNet architecture 들 정확도를 benchmark 한 table 인데, 다른 구조보다 </p>\n<ol>\n<li>더 적은 params 수</li>\n<li>상대적으로 적은 extra data 수</li>\n<li>더 높은 성능</li>\n</ol>\n<p>을 달성했다는 점에서 의미가 있을 것 같네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB/klEQVQoz21T25KjIBT0/39uJlsm4w1vBERRgzcEje62cTIPU8sD1YX0OX2a1hFCZFk2juPQD1gATVPHSSKlnKapbds4JpzzAzdNnucVVllmWd62D6csRUKSXNLeDtu6reuq9ZSmKaps22atBZjn+fl8GmMYZ5WUOEySBOWcgvObewuTkPb072uBj2+zMcD7vmM7z4GPtlICExKjllMU/PLxSQqi1u68tixLRCKt9UnY9+2HTGnOiwLY97yDjAm821dKM9bwwY4nOY7jeTZvzjcZU1DMjM77TuIY4h0Y5vk+lbQZm+W5HmRrYaEx5t1v/yFzxpqmBcZcB7kSpRf60tT7ezZ0hmFaz79kP7dNiOKhFI5x4dW5FJfPP0EeiFF8k63FzLP5n2xKpaxxEoYhejiM84/LR8SjVrdn82Wx8AMv/qszyFmWlmWFU+80rODFLfjiBa9GaTaLS9BDSDRN+iRgnWMDMHaXdQ38li0KPwhIRBKWlE2ltMLrww/kDcHAm0Me9jMkiFpd1/bl6BESFHPdK2a4um5M47zLtZnCIERU8dRd1/X9K7fDAPKd5mV1hERWFSo6CGoQBJCR5ikTd9nLblSEkJOMbCulUAK7nibGWN00UJEmKRx17oy5aH29RhFRD9WPveoOMm6DCf1QCJNhCmoFgc8Yx08Ew1D9H0XbEnVrmtmBAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png\"\n        srcset=\"/static/4a5d59c7ec213c223b48d78af30834c5/12f09/overall-imagenet-performance.png 148w,\n/static/4a5d59c7ec213c223b48d78af30834c5/e4a3f/overall-imagenet-performance.png 295w,\n/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png 590w,\n/static/4a5d59c7ec213c223b48d78af30834c5/efc66/overall-imagenet-performance.png 885w,\n/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" style=\"position:relative;\"><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" aria-label=\"robustness results on imagenet a imagenetc and imagenet p permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</h3>\n<p>해당 dataset 은 이 모델이 정말로 noise 에 robust 한지 체크하는 task 입니다.</p>\n<p>예를 들어서, (일반적인 corruptions, perturbations)</p>\n<ol>\n<li>blur 섞인 이미지</li>\n<li>fogged 이미지</li>\n<li>rotated 이미지 </li>\n<li>scaled 이미지</li>\n</ol>\n<p>등등이 데이터에 섞여 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.1891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAB50lEQVQoz21S2XKjMBD0/39WHOd1fREOnSDh5QrI3DY43hZsUqmtnQfVMKjV0z2zyfOcUiqlzLKsLMuPH1EuEYahilQc68vlUhSFEEJpjSJ+bYypHOedBIEx12Ec+rG/jTfEuMQwDHEcU8qUAj7GH9zUsRaC101jwYSSKIo45eIiaM+6rrsv8Xg8gEdSFDmQ84zC4/l84vy08dhUVXk+n33f55yFURTUgS7RloKQ232a5xlgKMLnNFskQCh+Lq9YZtd1gVQyYhEjA2m7FgBw4t40TciTJNFKzwvgm9mCwXw8HnzPZ5KqRMWtjvIoRBMqvE9/mWEjNP8HbKrS8zxGONGEX/m9nwawQmwHu0boBz5NEi44khVs2/5iRtue63lSiFDI/a+DG7p85FRTj/uYB65maRoQCiH/MqMlaGaMSWkHSDBzLuL6t25VVmdDP0CXUpGUoTGm73v00jRN27bIYZiB1aGUQUAgLC9yaynULeOAYMxcYQqErMx1XZ9OR/hvmbE0r9vt6bB/edm+7V73h6PjOF3fr0uC56uyPEDM8QQJeA4D3+122DDYuWm7rqzsVn5vJlyov+KKMGZN6rpBm9frWrPFzSod/O+Ok6Qp5ME8SqCdYryMUsJYmiZBEKCOE7sI2LotfwDUFM/MTxmLlwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png\"\n        srcset=\"/static/bd4580870392b73ee63cd51263ff1380/12f09/imagenet-a-benchmark.png 148w,\n/static/bd4580870392b73ee63cd51263ff1380/e4a3f/imagenet-a-benchmark.png 295w,\n/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png 590w,\n/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png 598w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>ImageNet-A dataset benchmark table 만 하나 보면, 확실히 <code class=\"language-text\">The Noise</code> 가 robustness 에 큰 도움을 주고 있네요.</p>\n<h3 id=\"adversarial-robustness\" style=\"position:relative;\"><a href=\"#adversarial-robustness\" aria-label=\"adversarial robustness permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial Robustness</h3>\n<p>이번 실험은 adversarial attack 에도 robust 한지 확인하는 겁니다. 주로 FGSM Attack 을 해서 테스트를 하는데, 성능은 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 523px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABjUlEQVQoz32T6Y6DMAyE+/5vV6k/W1EBorRAw51wLN4PzPbeWijK4bFnJmEjIj/TVJdlUxRN2zZvUVdVm6bmOocxJk1TJkmSWGs3gMdpaq5X8X3mMk3yHGPfD54XeN5ut9vv99vtNgiC0+lE3RnM7JJlJgjE2lco4HFMjCmr6vIXwPq+b9t2k+d5BTEIl6Ukydp8ei/yFIBn2gg4Ho8Ia5wb4lj4XkJrTfdgbxiGGQwNaDOzznVtK4iPIjmfORfnaPGx8wrGPbUObSXM6dR1UlVijOS5hOFciDlFs0xI4BQLy3IGo5nOdV0DQ/yrWpZQuH3WatEhCCyG0Rk8hpFZFAV8XqR+ox3HMYYBY4v+c/MZ+L/bapi6HYZhlmX9YgxjFEWIf8j82pmrOhwOapVmJ0tw/FjlM1jfkGq+3aRzDv6oQBSOwKjrOsYbkTuYq+bFv5NkSV3Nozoj5fRvIX8Fc5AvwZm+1mIJbU4SqQA4YpNOaPQ8b/0xlAZ5yl/frXbzfR/9wFiChDyFSGCE1y/VLp+wJ5GhmQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n        srcset=\"/static/3a179011ae1230e5213e3dcc7fa3a416/12f09/fgsm-benchmark.png 148w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/e4a3f/fgsm-benchmark.png 295w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png 523w\"\n        sizes=\"(max-width: 523px) 100vw, 523px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>FGSM 보다 더 강력한 attack 인 FGD Attack 시에도 꽤괜 성능을 보였다고 캅니다.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>요즘 경향들은 이전처럼 deep 한 architecture 를 설계하거나 AutoML 을 이용한 NAS 를 만드는 것 보다는,\ntraining recipe (~ techniques) 에 집중을 하고 있는데, 이런 trend 에서 재미있는 approach 들이 많이 나오고 있는 것 같네요.</p>\n<p>또 현재 상태에서 CutMix 등등 여러 또 다른 기술들이 적용되면 최고 performance 가 어느 정도 될지도 궁금해 지네요.</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각…","tableOfContents":"<ul>\n<li><a href=\"/Noisy-Student/#tldr\">TL;DR</a></li>\n<li><a href=\"/Noisy-Student/#related-work\">Related Work</a></li>\n<li><a href=\"/Noisy-Student/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/Noisy-Student/#architecture\">Architecture</a></p>\n<ul>\n<li><a href=\"/Noisy-Student/#self-training-w-noisy-student\">Self-Training w/ Noisy Student</a></li>\n<li><a href=\"/Noisy-Student/#training\">Training</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/Noisy-Student/#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"/Noisy-Student/#imagenet-benchmark\">ImageNet Benchmark</a></li>\n<li><a href=\"/Noisy-Student/#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\">Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</a></li>\n<li><a href=\"/Noisy-Student/#adversarial-robustness\">Adversarial Robustness</a></li>\n</ul>\n</li>\n<li><a href=\"/Noisy-Student/#conclusion\">Conclusion</a></li>\n</ul>","fields":{"slug":"/Noisy-Student/"},"frontmatter":{"title":"Self-training with Noisy Student improves ImageNet classification","date":"Apr 11, 2020","tags":["Deep-Learning"],"keywords":["EfficientNet","Noisy-Student"],"update":"Apr 11, 2020"},"timeToRead":4}},"pageContext":{"slug":"/Noisy-Student/","series":[],"lastmod":"2020-04-11"}},"staticQueryHashes":["2027115977","694178885"]}