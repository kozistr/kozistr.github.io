{"componentChunkName":"component---src-templates-post-tsx","path":"/Noisy-Student/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"tldr\" style=\"position:relative;\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<p>이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각나서 리뷰하게 됐어요.</p>\n<p>아래는 이번 approach 가 달성한 성능인데, 이전 SOTA 에 비해서 Accuracy 가 약 2% 정도 올라갔네요.</p>\n<p>해당 이미지에는 <em>L2</em> performance 가 안올라와 있는데, <em>Noisy Student + Random Augment</em> 로 훈련한 <em>L2</em> 모델 top-1 accuracy 가 <em>88.4%</em> 입니다.\n올해에는 90% 가 넘는 architecture 가 나오지 않을까 생각이 드네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACTUlEQVR42m2Ti07jMBBF+/9/xm5BwEJpgNDUbkLbxB4/xs/Y8SoNy3alvYpGVqSTe2fGWZVShBBKqTSOPgQb4kBJFLx7rUi1bcgsepHWOsbo/8gYsyqlxBidcyklr5Qju/jZx16Sl6b5+GRsCCFOF5V/NcPTNEkpXQheybAn0CepJ3TFx5L/x8QYEZFzrrW+OIeoDbrjwD59zOUCLM98ppTWdU0IVUpprfu+f397OxwOiLiy1s5vUbMN9S4vwKUWa63Wuuu6+qMGgJRSjDHnLKUEgDk251yh8QDwMaQFKiWEYIxxzuWcg/fWWe99zvm7i5zzDDPGJGrZ9brpSylpHFEjCGCMoTFCSkLIwAZE/G57+cQMz3tCxTvgDYDkXdsZY3LOAAAC5lk6twDf8/sLc86NM7Br2+bEgQ0DCyHMEVL6ttJaW2unaUopGWMQ0Vo7w8fjUUhx2A1CaKWk1hoRT6dje2iNsUvMh4eH+/v7t/e3tus455vN5nQ6O+dW3gfnnJDSGKOUHIZBKT2O43QVcrfbbTbPvx4fX19flVJVVS3tzHu21gohEDGE0Pe9kDLnfH0xttvter2+vb2t32shxNPTEwB8wSGEYRjOFwGIlMarbc/1ZbNZ/1zf/LipqkopdXd3xzj33n85n06n8/ncdi0ijuN4bbvEfn5+rqrtft9IKatqCyC+nGOMAGCt3TdN27Zd9zlPQYilWmsppYSQj7qmB9qfz5RSxhgArJbrEuP863jvSynee0QkhHDOKaEDY2Q/n+OXQohxHEdj7G+so4v4u8tMcAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png\"\n        srcset=\"/static/83339124bc869908ad6f92e63006f4a1/12f09/performance.png 148w,\n/static/83339124bc869908ad6f92e63006f4a1/e4a3f/performance.png 295w,\n/static/83339124bc869908ad6f92e63006f4a1/fcda8/performance.png 590w,\n/static/83339124bc869908ad6f92e63006f4a1/efc66/performance.png 885w,\n/static/83339124bc869908ad6f92e63006f4a1/22475/performance.png 1039w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>paper : <a href=\"https://arxiv.org/pdf/1911.04252.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<p>code : <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code</a></p>\n<h2 id=\"related-work\" style=\"position:relative;\"><a href=\"#related-work\" aria-label=\"related work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Work</h2>\n<p>EfficientNet : <a href=\"https://arxiv.org/pdf/1905.11946.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arXiv</a></p>\n<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h2>\n<p>이전 Image Classification 연구 동향들에서도 간단하게 설명은 다음에 다른 포스트로 다뤄볼께요. 양이 좀 많이 질 듯 해서 (<del>귀찮네요</del>)</p>\n<h2 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h2>\n<h3 id=\"self-training-w-noisy-student\" style=\"position:relative;\"><a href=\"#self-training-w-noisy-student\" aria-label=\"self training w noisy student permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Training w/ Noisy Student</h3>\n<p>학습은 다음과 같은 process 로 이뤄지는데,</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACpUlEQVR42oWUa4+kKhCG+///tsmebE9aBVtBuxG5KIJiK4Ju1Jk5Z3Y3Oc8XDNQtVW95AQC8vb1FUUQIuUVRlmV5tlNVVGvNOW/bVkrZNM00TVprpVQjZU1p26rLum5SiNvtBgDI8wwAkOX54/EghFQ7FGGkte4PhsEOw2B2+jWslxACrSoAkhTC9+sVQBjFcRLHQoonIZzzM0pd15wxwbkQglL6fD6995dt2+wwKKXatmWMcSGapmGce++3/+Py55Vzs1JqGOyyOGut1nocx2marLXbtq2ffHP+uhr6Pk1TSqkQYrB2ek1d17XtXtpp8C3z+h3vF2PMPM/n028G4eDDeZ5nxlj5KMuyuKf3NE0fj/JzXhmEkBCCMcIY13VNCKG0wrgI4bNsKUUKIcYIwrQoCkopBCDLMoxxnmdlWbrFhxD+kvmrtpNlWaZpmqdZGzMcWGunaQohzDvTh/0R6PJbt5bF9ca8xtGYfpdC34/j+Hq9Qgh70KMR/zZsnmcpZVkWdV1n9/vP63t2vwMA4yShtEII1zUty/L53AVTFLgsHwjlOUJCyj1zTSmEEOV5kiQ/fvwTx1ECYJIkURxfr9cUwuv1Pc8RhAAAcLvt+h/H0Tm3OyulkiRhrEYIRVEURxFCKLvfn6QqCoxQjjAuiiJJkte01/8lvt05eG+tnZ3bts1775zz3q/bdnbVOTcMg/fhfF2W5avtu/PrNeYIEVIJzpRS2phzBxmrh2GQQmCEqorUjGutu67b14PzpmkuR7yFc97vWxbWI9t/BdX3veDimNbHRM+Zh7BenHNKKcbYkawhhLT72XAhpJQVpU3TaK2lbGi9f3PGpBBcCK31ZV3XipA4ioSUfd9LKc0hD2OM1rptW3P8CbquU0oZY5Rqz+Kttb8A/XIre5/8pGsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png\"\n        srcset=\"/static/db781720920abfda90bbcb8c902c548d/12f09/teacher-student.png 148w,\n/static/db781720920abfda90bbcb8c902c548d/e4a3f/teacher-student.png 295w,\n/static/db781720920abfda90bbcb8c902c548d/fcda8/teacher-student.png 590w,\n/static/db781720920abfda90bbcb8c902c548d/68e9c/teacher-student.png 654w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li>Labelled dataset 인 ImageNet 으로 <strong>Teacher Model</strong> 을 학습</li>\n<li>un-labelled dataset 인 JFT-300M 를 <strong>Teacher Model</strong> 로 pseudo labelling 하기</li>\n<li><code class=\"language-text\">2</code> 에서 생성된 data + ImageNet 으로 <strong>Student Model</strong> 학습 w/ noise</li>\n<li>다시 <code class=\"language-text\">2</code> 으로 가서 반복 (iterative training)</li>\n</ol>\n<p>위 과정이 knowledge (self) distillation 과 비슷한 과정인데, 주로 요 목적은 compression 으로 사용되는데, 여기는\n해당 목적 (not compression) 으로 사용하지 않는다는게 차이점 입니다.</p>\n<h3 id=\"training\" style=\"position:relative;\"><a href=\"#training\" aria-label=\"training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training</h3>\n<p>데이터 셋이나 구체적인 training recipe 들이 있지만, 적용한 technique 가 있어서 이걸 설명 해 보면,</p>\n<h4 id=\"fix-train-test-resolution-discrepancy\" style=\"position:relative;\"><a href=\"#fix-train-test-resolution-discrepancy\" aria-label=\"fix train test resolution discrepancy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>fix train-test resolution discrepancy</h4>\n<p>처음 몇 epoch 은 low resolution image 로 훈련을 하고 후에 high resolution image 로 fine-tuning 하는 기법입니다.</p>\n<p>논문 실험에서는 처음 350 epochs 는 낮은 해상도 이미지로 훈련하고, 1.5 epochs 는 더 큰 해상도로 unlabelled image 에 대해서 훈련했다고 하네요.</p>\n<p>unlabelled image data 는 학습할 때 labelled image 보다 14 배 큰 batch size 를 사용했다고도 하네요.</p>\n<h4 id=\"iterative-training\" style=\"position:relative;\"><a href=\"#iterative-training\" aria-label=\"iterative training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Iterative Training</h4>\n<p>논문에서는 총 3 steps 의 iterative training 을 했다고 소개합니다.</p>\n<ol>\n<li><code class=\"language-text\">EfficientNet-b7</code> 을 ImageNet 으로 훈련 <code class=\"language-text\">(as Teacher)</code></li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 JFT-300M + ImageNet 으로 훈련 <code class=\"language-text\">(as Student)</code> (batch size 비율은 labelled : unlabelled = 1 : 14)</li>\n<li><code class=\"language-text\">EfficientNet-L2</code> 를 새롭게 훈련 <code class=\"language-text\">2</code> 에서 만든 모델을 <code class=\"language-text\">Teacher</code> 로 사용 <code class=\"language-text\">(as Student)</code></li>\n<li><code class=\"language-text\">3</code> 과 비슷한 scheme 으로 진행하는데, (batch size 비율은 labelled : unlabelled = 1 : 28) 로 훈련</li>\n</ol>\n<h4 id=\"noisy\" style=\"position:relative;\"><a href=\"#noisy\" aria-label=\"noisy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Noisy</h4>\n<p>Student Model 을 학습할 때 논문에서 <code class=\"language-text\">Noisy</code> 하게 훈련한다고 했는데, 이 때 <code class=\"language-text\">Noisy</code> 에 해당하는 부분은 크게 3 부분 입니다.</p>\n<ol>\n<li>Data Augmentation w/ RandAugment</li>\n<li>Dropout</li>\n<li>Stochastic Depth</li>\n<li>other techniques (data filtering, balancing)</li>\n</ol>\n<ul>\n<li>OOD (Out-Of-Distribution)</li>\n<li>unlabelled data 에 대해선 class 별 samples 수가 biased 돼있으니, 적은 sample 들 duplicate 하기</li>\n<li>pseudo label 시, soft or hard pseudo 한다고 했었는데, soft, hard 둘다 좋은 결과를 보였지만, soft 가 더 좋았다</li>\n</ul>\n<p>요런 기법들은 이전에 소개된 기법들이니 설명은 pass</p>\n<h2 id=\"experiment-result\" style=\"position:relative;\"><a href=\"#experiment-result\" aria-label=\"experiment result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiment Result</h2>\n<h3 id=\"imagenet-benchmark\" style=\"position:relative;\"><a href=\"#imagenet-benchmark\" aria-label=\"imagenet benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ImageNet Benchmark</h3>\n<p>역대 ImageNet architecture 들 정확도를 benchmark 한 table 인데, 다른 구조보다</p>\n<ol>\n<li>더 적은 params 수</li>\n<li>상대적으로 적은 extra data 수</li>\n<li>더 높은 성능</li>\n</ol>\n<p>을 달성했다는 점에서 의미가 있을 것 같네요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACDklEQVR42m2TW5OjIBCF/f9/bidbycR44aYIES+oCIKAWzE187Az56m7iq/61Gk64ZwhhNWpRS3ruvZ9BwDouk5rPYwjAIAx9qqHgRDStq1oW4TwOI4J5xyUELVotnP00XuvtUYIresaQtj33RjjnAshWGubphFCOOcQQtqYpKH0dr0VMCdzdZyy1oKyNGZ7tyGE7+LJmRDiOI6iLKzdkqZpLh+XkoPZzUd8PXLOFkVhjPkPjjFijBljx3E80nTbtuT55Ok9RQRXoprt/IYhhNv2y2RCiOi6GCMAwFqXcMbS7FF1tdTSB/+2jTH+hmOM33BD6TAM8YgQQmdt0vJnmqdiFf5rgnMWIfjTtveeMSaljDEihJxzCefs8ufywClX/CuwrciLXwMjBAvRhRDyPH/BlNKPvx+Ag9GM8UzMWvt4pEqpnzDGqG3bGGP6Dowxds/ulNHn8jR+e8NlWerTdoyvzcdTIQRKad/38TgwxtbahLHm8ciKPAcEsJb1qlvXFUK4LIs/tZ/y3m/bRintut5aSwjRWie0rq+3W5Hnt+sNVKCaamVUlmXLsiil5KlpmqdpMsZUhLTnJxFC7N6/9pxlGYQQE9xwKmYh1QTKclFK63UYhnE8cSm11nVdd32/73sJgDHbK7DPz9v9fi9LIEc5L/MoX8cgpez7HkGIECaENE2jlMqzrGFsnue3tX+KYxJfndkmBAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png\"\n        srcset=\"/static/4a5d59c7ec213c223b48d78af30834c5/12f09/overall-imagenet-performance.png 148w,\n/static/4a5d59c7ec213c223b48d78af30834c5/e4a3f/overall-imagenet-performance.png 295w,\n/static/4a5d59c7ec213c223b48d78af30834c5/fcda8/overall-imagenet-performance.png 590w,\n/static/4a5d59c7ec213c223b48d78af30834c5/efc66/overall-imagenet-performance.png 885w,\n/static/4a5d59c7ec213c223b48d78af30834c5/2bef9/overall-imagenet-performance.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" style=\"position:relative;\"><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\" aria-label=\"robustness results on imagenet a imagenetc and imagenet p permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</h3>\n<p>해당 dataset 은 이 모델이 정말로 noise 에 robust 한지 체크하는 task 입니다.</p>\n<p>예를 들어서, (일반적인 corruptions, perturbations)</p>\n<ol>\n<li>blur 섞인 이미지</li>\n<li>fogged 이미지</li>\n<li>rotated 이미지</li>\n<li>scaled 이미지</li>\n</ol>\n<p>등등이 데이터에 섞여 있어요.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.1891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACBklEQVR42m2SWY+bMBSF+f+/qqNI6VOnyTTDYmIbb4CBEBYvEMBUgEZqq35PV5aPjn3P8WSe3yEkhFSPR9M0z7+p65qQhHMuhJBSVlWVJIkQglHWtp1XVeXt8xPeYdd1xhpj9WCHYRyHHa01YxQixBjNpTTGgCgSaZokiTbGq6oqBoBQegcQSwQ1stqOO865YRxfr1eeZ1mWL84ty+LcOs+z2/GKorjdbgAAjBGhFPRAlIImhBCyLO61k6cpZcw5t66rc+4Qr+vqlWUZBAHGmCJ6FxAaaLQZX9uz13U9xELwLMv2+xvLshyDVxTy+nGNIgApZDnnmtOMEkw4Z/PuPE1TUUiRpv8Rl4WMogjGEGSAtHToBzUqPWmjrNFGaz3PsxB8+8WX+k/nwvf9IAwThBGElx+XMA2RRoDFAN/bpp2m7dnxHU7T9K9YShkEAYQwIZhxDiGklGRK8p7X6qmVqesH3dZHtyyNVkp1XaeUstZuC4uiCGMUgZhS+qgeyzK7LZYtjHEL3CIE71/OVVVdr5f6+dycszQ9nU4fH9dvb2/fz+fL5RqGgR0Ga+1RkrIsfr6//7r5Xdcty4IxOp/PIhXzPHta62bv4VHGun62bdt/0e0cU9+rpmm6bjts21Zr7R1x53nu+76UMsEoDMM4BhCiLM+2AeE8S8MwDKMIAMAZ63t1tOU3DWLP1elxXKsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png\"\n        srcset=\"/static/bd4580870392b73ee63cd51263ff1380/12f09/imagenet-a-benchmark.png 148w,\n/static/bd4580870392b73ee63cd51263ff1380/e4a3f/imagenet-a-benchmark.png 295w,\n/static/bd4580870392b73ee63cd51263ff1380/fcda8/imagenet-a-benchmark.png 590w,\n/static/bd4580870392b73ee63cd51263ff1380/0c69d/imagenet-a-benchmark.png 598w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>ImageNet-A dataset benchmark table 만 하나 보면, 확실히 <code class=\"language-text\">The Noise</code> 가 robustness 에 큰 도움을 주고 있네요.</p>\n<h3 id=\"adversarial-robustness\" style=\"position:relative;\"><a href=\"#adversarial-robustness\" aria-label=\"adversarial robustness permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adversarial Robustness</h3>\n<p>이번 실험은 adversarial attack 에도 robust 한지 확인하는 겁니다. 주로 FGSM Attack 을 해서 테스트를 하는데, 성능은 아래와 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 523px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.75675675675677%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABsElEQVR42n1T25KjIBT0/z8veUyqTMpIFBDC4abczhbqmtmdzHTxQAFNH5o+DSLmUpxSUcolxmWel38xOxfHcWJspJRzTghhjD2fT+dcg4gxJS1lbFv8hBSCvd262+18Pl8ul9Pp9Hg8OOfW2qaU0vc9FUL0PRpTELGUr+ScszTGew8AasU8zyEE730jhDDrngHIlL6ZpezjE97k6/WqtTbeR8aw674TylfkXF+6kelqw7Is3ntnLXqPfY+E1Im1aMxH8RhjJU/TRAix1qacQalNB0NAgEoeBhxHFAKnqQ4p63XWRkq9c42UknPunENEpVROCX9BjKj1m8w5Z4xZaxERANJB/tWwvWxCSNd1WmtE1FoLIQ6HPouv67thfd8zxpZlqXlIiXPuvf/J7f+Vtdb3+x0AdqsQpZSU0i0JP5Ww//MmuBl2HE0p1Z9zbhiG1+u1zZ1zKaXtQIxxnudKJoRIKQ/l8i2etT3WhjF/QSndGyOlBADTNG2GAYCUUgihlNrCSykFAMYYAAzDoLVu23Yv+7i+lBJCyDnHFd77tm3HcXTOxRi3smucUjLGlFL+AEFxoHRcQd4GAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"\"\n        src=\"/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png\"\n        srcset=\"/static/3a179011ae1230e5213e3dcc7fa3a416/12f09/fgsm-benchmark.png 148w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/e4a3f/fgsm-benchmark.png 295w,\n/static/3a179011ae1230e5213e3dcc7fa3a416/3e286/fgsm-benchmark.png 523w\"\n        sizes=\"(max-width: 523px) 100vw, 523px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>FGSM 보다 더 강력한 attack 인 FGD Attack 시에도 꽤괜 성능을 보였다고 캅니다.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>요즘 경향들은 이전처럼 deep 한 architecture 를 설계하거나 AutoML 을 이용한 NAS 를 만드는 것 보다는,\ntraining recipe (~ techniques) 에 집중을 하고 있는데, 이런 trend 에서 재미있는 approach 들이 많이 나오고 있는 것 같네요.</p>\n<p>또 현재 상태에서 CutMix 등등 여러 또 다른 기술들이 적용되면 최고 performance 가 어느 정도 될지도 궁금해 지네요.</p>\n<p>결론 : 굳</p>","excerpt":"TL;DR 이번 포스팅에서 리뷰할 논문은 EfficientNet 기반으로 새로운 techniques 를 적용해서 ImageNet dataset 에서 SOTA 를 찍은 논문입니다.\n나온지는 꽤 됐지만, 최근 TPU 에서 돌아가는 요 코드를 짜다가 생각…","tableOfContents":"<ul>\n<li>\n<p><a href=\"#tldr\">TL;DR</a></p>\n</li>\n<li>\n<p><a href=\"#related-work\">Related Work</a></p>\n</li>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#architecture\">Architecture</a></p>\n<ul>\n<li>\n<p><a href=\"#self-training-w-noisy-student\">Self-Training w/ Noisy Student</a></p>\n</li>\n<li>\n<p><a href=\"#training\">Training</a></p>\n<ul>\n<li><a href=\"#fix-train-test-resolution-discrepancy\">fix train-test resolution discrepancy</a></li>\n<li><a href=\"#iterative-training\">Iterative Training</a></li>\n<li><a href=\"#noisy\">Noisy</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#experiment-result\">Experiment Result</a></p>\n<ul>\n<li><a href=\"#imagenet-benchmark\">ImageNet Benchmark</a></li>\n<li><a href=\"#robustness-results-on-imagenet-a-imagenetc-and-imagenet-p\">Robustness Results on ImageNet-A, ImageNetC and ImageNet-P</a></li>\n<li><a href=\"#adversarial-robustness\">Adversarial Robustness</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#conclusion\">Conclusion</a></p>\n</li>\n</ul>","fields":{"slug":"/Noisy-Student/"},"frontmatter":{"title":"Self-training with Noisy Student improves ImageNet classification","date":"Apr 11, 2020","tags":["Deep-Learning"],"keywords":["EfficientNet","Noisy-Student"],"update":"Apr 11, 2020"},"timeToRead":2}},"pageContext":{"slug":"/Noisy-Student/","series":[],"lastmod":"2020-04-11"}},"staticQueryHashes":["2027115977","2744905544","694178885"],"slicesMap":{}}