---
layout: post
title: SAN - Second-order Attention Network for Single Image Super-Resolution
author: kozistr
categories: deep-learning
tags: GAN, DeepLearning, SAN, SuperResolution, SISR
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

이번 포스팅에서는 리뷰할 논문은 *SAN* (Second-order Attention Network) 이라는 Image Super Resolution task 에서 현재 여러 test set 에서 제일 높은 성능 (19년도 기준)을 보이고 있는 architecture 입니다.

이 때 까지도 여러 attention module 들을 붙여서 super resolution network 의 성능을 올리는 데 trend 였는데, 재밌는 (?) approach 를 해서 리뷰 해 보게 됐습니다.

paper : [CVPR19](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf)

official implementation : [code](https://github.com/daitao/SAN)

## Related Work

요건 다른 Super Resolution 들 paper list 입니다.

* DBPN : [arXiv](https://arxiv.org/pdf/1803.02735.pdf)
* ESRGAN : [arXiv](https://arxiv.org/pdf/1809.00219)
* RCAN : [arXiv](https://arxiv.org/pdf/1807.02758.pdf)

## Introduction

이전에 여러 SISR (Single Image Super Resolution) task 들의 network 들은 *더 넓고 깊은 구조*를 띄면서, 상대적으로 network 의 

1. *representation 능력*
2. *각 중간 layer 들의 feature correlation*

들을 덜 고려하는 경향을 보였다면서, 이번에 **SAN** (Second-order Attention Network) 를 제안하면서, 이런 문제들을 더 고려한 구조를 맨들어 봤다고 캅니다.

기존 *SE* (Squeeze and Excitation) Module 은 *Squeeze* stage 에서 *GAP* (Global Average Pooling) 을 하면서 first-order distribution feature 밖에 학습하지 못했는데,
여기 **SOCA Module** 이라 부르고 attention network 에선 second-order-tic 하게 연산을 위한 operation 이 들어가서 adaptive 하게 feature 를 rescaling 했다고 하는데, 궁금해지네요.

## Architecture

*SAN* 는 4 가지 부분으로 구성이 되어 있는데요,

1. shallow feature extraction
2. non-locally enhanced residual group (NLRG)
3. up-scale module
4. reconstruction part

입니다.

전반적인 **SAN** architecture 는 아래 사진과 같습니다.

![img](/assets/SAN/overall_architecture.png)

### 1. shallow feature extraction

논문에서 자기들은 shallow feature extraction 을 위해서 LR (Low Resolution) 이미지로 부터 오직 convolution layer 1 층만 쌓았다고 합니다.


###  SOCA (Second Order Channel Attention) Module



### NLRG (Non-Locally enhanced Residual Group) Module


### LSRAG (Local-Source Residual Attention Group) Module



## Experiment Result


## Conclusion


결론 : 마음에 드는 논문
