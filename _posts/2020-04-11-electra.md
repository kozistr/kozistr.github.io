---
layout: post
title: ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators
author: kozistr
categories: deep-learning
tags: DeepLearning, nlp, lm, electra, pre-trained
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

이번에 리뷰할 논문은 *ELECTRA* 란 google ai 에서 3월에 발표한 논문인데, 재밌는 approach 를 하고 있어서 가져와 봤습니다.

ELECTRA paper : [OpenReview](https://openreview.net/pdf?id=r1xMH1BtvB)

google ai blog : [blog](https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html)

## Related Work

이전 trend 들 5 개 정도만...

BERT : [paper](https://arxiv.org/pdf/1810.04805.pdf)

XLNET : [paper](https://arxiv.org/pdf/1906.08237.pdf)

RoBERTa : [paper](https://arxiv.org/pdf/1907.11692.pdf)

ALBERT : [paper](https://arxiv.org/pdf/1909.11942.pdf)

T5 : [paper](https://arxiv.org/pdf/1910.10683.pdf)

## Introduction

간단하게 이번 *ELECTRA* paper 에서 이전과 다른 점 3 가지를 정리하면

1. input 을 masking 하는게 아닌 generator 로 token 생성 (masking 효과)

2. token ID 를 예측하는 게 아닌 discriminator 로 각 token 이 generated 됐는지 예측

3. 기존 MLM 보다 더 좋음. (small MLM, ...)

## Architecture

### Previous Story

이전 LM 들을 보면 DAE 형태로 학습을 하고 (masked input 을 복원), *BERT* 같은 경우에는 masking 때문에 example 당 token 의 15% 밖에 학습이 안돼서
학습 비용이 꽤 컸어요.

그래서 위 문제를 해결하려고 *ELECTRA*에서 replaced token detection task 를 제안했는데, 
masking 하는 대신, 작은 MLM (masked language model ~ generator) 으로 생성된 output 으로 일부 교체 하고 discriminator 를 둬서 이게 replaced token or not 인지를 예측하게 학습했습니다.

장점 으로는
* MLM 자제가 작은걸 사용 -> 연산이 더 빨라짐
* masked 된 부분만이 아닌 전체 token 에 대해서 discriminate -> 학습 효율 증가

![img](/assets/ELECTRA/disc_gen_overview.png)

### Method

generator / discriminator 로 GAN 과 유사해 보이는데, 해당 network 구조만 그렇고
실제로 *adversarial* 하게 훈련하지는 않습니다.

각 network encoder 는 transformer 로 구성되어있고,

generator 는 각 token 에 대한 softmax 값을 output 로 주고

> $p_G(x_t\|x) = exp(e(x_t)^T h_G(x)_t) / \sigma_x` exp (e(x`)^T h_G(x)_t$

discriminator 는 각 token 에 대해 replaced / not replaced 를 예측합니다.

> $D(x, t) = sigmoid(w^T h_D(x)_t)$

### Model

#### Weight Sharing

* generator 하고 discriminator 크기가 같으면 weight sharing 
* 그런데 실험 결과로는 크기가 같지 않고 small generator 를 사용하는게 훨 좋았음
* 그래서 small generator 를 사용하는 경우엔 token embedding table 만 weight sharing 을 함

### Small MLM (Generator)

* generator / discriminator 크기가 같으면 기존 MLM 보다 2 배 커짐
* 주로 generator 가 discriminator 크기의 x0.25 ~ x0.5 일 때 괜춘함
* 

## Experiment Result

## Conclusion

결론 : 굳
