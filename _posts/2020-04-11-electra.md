---
layout: post
title: ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators
author: kozistr
categories: deep-learning
tags: DeepLearning, nlp, lm, electra, pre-trained
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

이번에 리뷰할 논문은 *ELECTRA* 란 google ai 에서 3월에 발표한 논문인데, 재밌는 approach 를 하고 있어서 가져와 봤습니다.

ELECTRA paper : [OpenReview](https://openreview.net/pdf?id=r1xMH1BtvB)

google ai blog : [blog](https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html)

## Related Work

이전 trend 들 5 개 정도만...

BERT : [paper](https://arxiv.org/pdf/1810.04805.pdf)

XLNET : [paper](https://arxiv.org/pdf/1906.08237.pdf)

RoBERTa : [paper](https://arxiv.org/pdf/1907.11692.pdf)

ALBERT : [paper](https://arxiv.org/pdf/1909.11942.pdf)

T5 : [paper](https://arxiv.org/pdf/1910.10683.pdf)

## Introduction

간단하게 이번 *ELECTRA* paper 에서 이전과 다른 점 3 가지를 정리하면

1. input 을 masking 하는게 아닌 generator 로 token 생성 (masking 효과)

2. token ID 를 예측하는 게 아닌 discriminator 로 각 token 이 generated 됐는지 예측

![img](/assets/ELECTRA/disc_gen_overview.png)

3. 기존 MLM 보다 더 좋음. (small MLM, ...)


## Architecture



## Experiment Result

## Conclusion

결론 : 굳
