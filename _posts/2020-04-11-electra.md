---
layout: post
title: ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators
author: kozistr
categories: deep-learning
tags: DeepLearning, nlp, lm, electra, pre-trained
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

이번에 리뷰할 논문은 *ELECTRA* 란 google ai 에서 3월에 발표한 논문인데, 재밌는 approach 를 하고 있어서 가져와 봤습니다.

ELECTRA paper : [OpenReview](https://openreview.net/pdf?id=r1xMH1BtvB)
google ai blog : [blog](https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html)

## Related Work

이전 trend 들 5 개 정도만...

BERT : [paper](https://arxiv.org/pdf/1810.04805.pdf)
XLNET : [paper](https://arxiv.org/pdf/1906.08237.pdf)
RoBERTa : [paper](https://arxiv.org/pdf/1907.11692.pdf)
ALBERT : [paper](https://arxiv.org/pdf/1909.11942.pdf)
T5 : [paper](https://arxiv.org/pdf/1910.10683.pdf)

## Introduction


## Architecture



## Experiment Result

## Conclusion

