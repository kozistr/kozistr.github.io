---
layout: post
title: NVAE: A Deep Hierarchical Variational Autoencoder
author: kozistr
categories: deep-learning
tags: DeepLearning, Hierarchical, VAE, Generative Models
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

최근(?)에 NVLabs 에서 VAE 관련 논문이 하나 나왔는데, 매주 월요일이 회사 짬데이라고 개인 or 팀 끼리 공부하고 공유하는 문화가 있어서, 마침 잘 돼서 논문 리뷰를 해 봅니다.

paper : [arXiv](https://arxiv.org/pdf/2007.03898.pdf)

code : [github](https://github.com/NVlabs/NVAE)

twitter : [twitter](https://twitter.com/ArashVahdat/status/1281036985981825024/photo/1)

## Related Work

VAE 관련 연구들이 엄청 많아서, 요 논문과 직접 연관이 있는 것들만 적어보면

1. IAF-VAEs (VAE w/ Invertible Autoregressive Flows) : [paper](https://arxiv.org/pdf/1606.04934.pdf)
2. VQ-VAE-2 (Vector Quantized Variational AutoEncoder v2) : [paper](https://arxiv.org/pdf/1906.00446.pdf)
3. BIVA (Bidirectional-Inference Variational Autoencoder) : [paper](https://arxiv.org/pdf/1902.02102.pdf)

## Difference

논문에서 previous works 와 this work 의 차이를 *related work* 에 적힌 3 개의 연구와 비교를 합니다.

요약하면 아래와 같습니다.

### VQ-VAE-2 vs NVAE

비슷한 점은 둘 다 high quality image 생성이 가능하다는 점

| diff \ work | VQ-VAE-2 | NVAE |
| :---: | :---: | :---: |
| objective | ~~VAE objective~~ | VAE objective |
| latent variable | up to 128x128 (big) | small | 

### IAF-VAEs vs NVAE

statistical models (hierarchical prior, approximate posterior) 컨셉을 IAF-VAEs 에서 가져온 것은 맞는데,

| diff \ work | IAF-VAEs | NVAE |
| :---: | :---: | :---: |
| statistical models | ~~neural network~~ | neural network |
| posterior | x | parameterized | 
| large-scale | x | o |

## Proposed Approach

![img](/assets/NVAE/architecture.png)

## Experiment Result

이미지 생성 결과.

![img](/assets/NVAE/generated_images.png)

## Conclusion

결론 : 굳굳굳
