---
layout: post
title: Cotatron Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data
author: kozistr
categories: deep-learning
tags: DeepLearning, nlp, chatbot, blender, fair
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

최근 mindslabd에서 VC (Voice Conversion)관련 논문이 나와서 오랜만에 요 쪽 domain 도 볼 겸 해서 논문을 읽게 됐습니당.

간단하게 요약하면, 유명한 google 의 TTS model 인 *tacotron2* 기반으로 given transcript 와 mel alignment 를 활용해서 speaker-independent linguistic representation 을 뽑는 concept(?) 입니다.

결론은 VCTK dataset 에서 최근 paper 인 *Blow* 보다 훨 높은 MOS, DMOS 를 달성했습니다. 아래 링크에 들어가면 모델이 생성한 sample 들을 들어볼 수 있어요.

paper : [arXiv](https://arxiv.org/pdf/2005.03295.pdf)
demo : [link](https://mindslab-ai.github.io/cotatron/)
code : 아직 official code / pre-trained model은 없는데, 곧 나올 예정인 듯 합니다

## Related Work

이전 SOTA 였던 paper

* Blow : [arXiv](https://arxiv.org/pdf/1906.00794.pdf)

## Introduction


## Architecture

![img](/assets/Cotatron/cotatron-architecture.png)

## Summary

   
## Experiment Result



## Conclusion

transcript 를 주지 않아도 성능이 준 것과 comparable 하다는 점도 재밌고, 
Cortatron encoder 를 다른 task 에 적용 해 봐도 재밌는 결과 볼 수 있을 것 같네용

결론 : 굳
