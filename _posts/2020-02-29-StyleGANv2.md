---
layout: post
title: StyleGAN-v2 - Analyzing and Improving the Image Quality of StyleGAN
author: kozistr
categories: deep-learning
tags: GAN, DeepLearning, StyleGANv2
use_math: true
---

posted by [kozistr](http://kozistr.tech)

## tl;dr

이번 포스팅에서는 리뷰할 논문은 지난 19년 11월에 나온 **StyleGAN v2**를 리부 해 보겠습니다

StyleGAN 에 이어서 2 번째 논문인데, 이번 버전에서는 어떤 문제점들을 어떻게 해결했는지를 한번 보려고 합니다!

아래는 StyleGAN v2 로 생성한 이미지들 입니다.

![img](/assets/StyleGANv2/stylegan2-teaser-1024x256.png)

paper : [arXiv](https://arxiv.org/pdf/1912.04958.pdf)

official implementation : [code](https://github.com/NVlabs/stylegan2)

## Related Work

요건 이전 버전 StyleGAN paper 입니다.

* StyleGAN : [arXiv](https://arxiv.org/pdf/1812.04948.pdf)

## Introduction

이번 논문에서는 이전에 발표한 StyleGAN 의 artifacts 들에 대해 지적하면서 시작합니다.

요약 해 보면 크게 3개의 문제점을 지적 / 개선 / 해결 했는데, 

1. blob-like artifacts
2. artifacts related to progressive growing
3. metrics for evaluating GAN performance

### blob-like artifacts

StyleGAN 에서 *Instance Normalization* 이 아래 이미지 처럼 water droplet 과 같은 artifacts 를 발생한다고 합니다. 

![img](/assets/StyleGANv2/droplet-llike-artifacts.png)

*generator* 의 activation map 을 보면 (오른쪽 사진들) 자국 같은 것 들이 보일텐데, 설계상 때문에 요게 문제가 됐다는 겁니다.

그래서 StyleGANv2 에서는 새로운 normalization method 를 사용해서 이 문제를 해결합니다.

### artifacts related to progressive growing

high-resolution GAN 에서 stable 한 훈련을 위해 low resolution 부터 훈련을 시작하는 progressive 한 훈련 방식을 해 왔었는데,
이전에는 각 resolution 마다 같은 network 구조를 사용했는데, 다른 resolution 을 훈련할 때는 다른 network topology 를 사용해야 한다는 말 입니다.

매 resolution 마다 같은 구조가 아닌 다른 구조로 학습을 하게 되면, 각 resolution 에 맞게 더 효율적으로 훈련할 수 있다는 논문피셜 입니다.
(당연한 이야기긴 하지만)

### metrics for evaluating GAN performance

GAN 생성 이미지 quality 를 측정하기 위해 여러 metric 들을 사용하는데 (e.g. FID, precision, recall),
이런 metric 들의 문제점을 제기하고 새로운 gan metric 을 사용해서 performance 를 측정했다고 합니다.

간단하게 설명 해 보면, 위에 소개된 이전 metric 들은 inception v3 같은 base network 에 기반해서, 전반적인 texture 보다 shape 같은 것에 집중을 하는데,
결론적으로 이미지 quality 전반적인 면을 capture 하지 못한다는 말 입니다.

그래서 이런 문제가 어느 정도 해결 한 perceptual path length (PPL) metric 을 사용했다고 합니다.
 
### etc

마지막으론 latent space $W$ 가 더 잘된다고 하네요
 
## Architecture


## Experiment Result
